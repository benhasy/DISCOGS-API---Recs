{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56101697",
   "metadata": {},
   "source": [
    "# Discogs API - Fetching data on all releases by style and storing in a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4687ce8",
   "metadata": {},
   "source": [
    "# Jungle Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badacdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"JungleScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_jungle.csv\"\n",
    "\n",
    "# Load existing IDs to avoid duplicates\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# The key fix: use style= parameter and split by year to avoid 10k limit\n",
    "for year in range(1991, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",  # Use style taxonomy, not query\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)  # Rate limit: 60/min\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "# Append to CSV\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba68f75",
   "metadata": {},
   "source": [
    "# Ghettotech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ad21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"GhettotechScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_ghettotech.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1995, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Ghettotech\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578e906",
   "metadata": {},
   "source": [
    "# Breakbeat Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1989, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90301f95",
   "metadata": {},
   "source": [
    "# Breakbeat + Hardcore Pull - with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_hardcore.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1989, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            # Only keep if it has BOTH Breakbeat and Hardcore\n",
    "            if 'Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43fd2a",
   "metadata": {},
   "source": [
    "# Drum and Bass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4622f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991: 1 new releases\n",
      "1992: 1 new releases\n",
      "1993: 344 new releases\n",
      "1994: 1479 new releases\n",
      "1995: 2579 new releases\n",
      "1996: 3461 new releases\n",
      "1997: 4657 new releases\n",
      "1998: 4248 new releases\n",
      "1999: 3550 new releases\n",
      "2000: 3208 new releases\n",
      "2001: 3193 new releases\n",
      "2002: 3375 new releases\n",
      "2003: 3466 new releases\n",
      "2004: 3570 new releases\n",
      "2005: 3159 new releases\n",
      "2006: 3463 new releases\n",
      "2007: 3446 new releases\n",
      "2008: 3836 new releases\n",
      "2009: 3971 new releases\n",
      "2010: 4366 new releases\n",
      "2011: 5039 new releases\n",
      "2012: 5404 new releases\n",
      "2013: 5512 new releases\n",
      "2014: 5363 new releases\n",
      "2015: 5090 new releases\n",
      "2016: 5193 new releases\n",
      "2017: 4963 new releases\n",
      "2018: 4845 new releases\n",
      "2019: 4747 new releases\n",
      "2020: 6064 new releases\n",
      "2021: 5379 new releases\n",
      "2022: 4346 new releases\n",
      "2023: 4318 new releases\n",
      "2024: 3801 new releases\n",
      "2025: 3121 new releases\n",
      "\n",
      "Added 132558 new releases to discogs_dnb.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DnBScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_dnb.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1991, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Drum n Bass\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fd151",
   "metadata": {},
   "source": [
    "# Breakbeat & House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatHouseScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_house.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'House' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb80416",
   "metadata": {},
   "source": [
    "# Gabber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"GabberScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_gabber.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1989, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Gabber\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738395",
   "metadata": {},
   "source": [
    "# Happy Hardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df31646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991: 1 new releases\n",
      "1992: 35 new releases\n",
      "1993: 216 new releases\n",
      "1994: 1246 new releases\n",
      "1995: 3527 new releases\n",
      "1996: 3355 new releases\n",
      "1997: 1798 new releases\n",
      "1998: 767 new releases\n",
      "1999: 625 new releases\n",
      "2000: 450 new releases\n",
      "2001: 408 new releases\n",
      "2002: 561 new releases\n",
      "2003: 514 new releases\n",
      "2004: 674 new releases\n",
      "2005: 832 new releases\n",
      "2006: 795 new releases\n",
      "2007: 889 new releases\n",
      "2008: 955 new releases\n",
      "2009: 930 new releases\n",
      "2010: 989 new releases\n",
      "2011: 1191 new releases\n",
      "2012: 1163 new releases\n",
      "2013: 1188 new releases\n",
      "2014: 1166 new releases\n",
      "2015: 1095 new releases\n",
      "2016: 1098 new releases\n",
      "2017: 992 new releases\n",
      "2018: 823 new releases\n",
      "2019: 1018 new releases\n",
      "2020: 1021 new releases\n",
      "2021: 1098 new releases\n",
      "2022: 1074 new releases\n",
      "2023: 949 new releases\n",
      "2024: 640 new releases\n",
      "2025: 422 new releases\n",
      "\n",
      "Added 34505 new releases to discogs_happy_hardcore.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"HappyHardcoreScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_happy_hardcore.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1991, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Happy Hardcore\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ec8be",
   "metadata": {},
   "source": [
    "# Techno & Future Jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1aaed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985: 1 new releases\n",
      "1991: 2 new releases\n",
      "1993: 4 new releases\n",
      "1994: 10 new releases\n",
      "1995: 12 new releases\n",
      "1996: 20 new releases\n",
      "1997: 27 new releases\n",
      "1998: 32 new releases\n",
      "1999: 30 new releases\n",
      "2000: 29 new releases\n",
      "2001: 48 new releases\n",
      "2002: 48 new releases\n",
      "2003: 47 new releases\n",
      "2004: 41 new releases\n",
      "2005: 22 new releases\n",
      "2006: 36 new releases\n",
      "2007: 9 new releases\n",
      "2008: 2 new releases\n",
      "2011: 2 new releases\n",
      "\n",
      "Added 422 new releases to discogs_techno_future_jazz.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"TechnoFutureJazzScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_techno_future_jazz.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Techno\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Future Jazz' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ad98d",
   "metadata": {},
   "source": [
    "# Ghetto House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaac3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989: 2 new releases\n",
      "1991: 4 new releases\n",
      "1992: 3 new releases\n",
      "1993: 14 new releases\n",
      "1994: 60 new releases\n",
      "1995: 166 new releases\n",
      "1996: 172 new releases\n",
      "1997: 175 new releases\n",
      "1998: 76 new releases\n",
      "1999: 61 new releases\n",
      "2000: 47 new releases\n",
      "2001: 31 new releases\n",
      "2002: 35 new releases\n",
      "2003: 24 new releases\n",
      "2004: 33 new releases\n",
      "2005: 27 new releases\n",
      "2006: 49 new releases\n",
      "2007: 42 new releases\n",
      "2008: 95 new releases\n",
      "2009: 96 new releases\n",
      "2010: 94 new releases\n",
      "2011: 122 new releases\n",
      "2012: 114 new releases\n",
      "2013: 172 new releases\n",
      "2014: 183 new releases\n",
      "2015: 173 new releases\n",
      "2016: 176 new releases\n",
      "2017: 130 new releases\n",
      "2018: 124 new releases\n",
      "2019: 118 new releases\n",
      "2020: 171 new releases\n",
      "2021: 132 new releases\n",
      "2022: 99 new releases\n",
      "2023: 87 new releases\n",
      "2024: 61 new releases\n",
      "2025: 35 new releases\n",
      "\n",
      "Added 3203 new releases to discogs_ghetto_house.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"GhettoHouseScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_ghetto_house.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Ghetto House\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d29f0",
   "metadata": {},
   "source": [
    "# Juke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b78231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996: 4 new releases\n",
      "1997: 3 new releases\n",
      "1998: 10 new releases\n",
      "1999: 6 new releases\n",
      "2000: 8 new releases\n",
      "2001: 5 new releases\n",
      "2002: 9 new releases\n",
      "2003: 6 new releases\n",
      "2004: 11 new releases\n",
      "2005: 6 new releases\n",
      "2006: 6 new releases\n",
      "2007: 25 new releases\n",
      "2008: 41 new releases\n",
      "2009: 24 new releases\n",
      "2010: 47 new releases\n",
      "2011: 121 new releases\n",
      "2012: 231 new releases\n",
      "2013: 454 new releases\n",
      "2014: 519 new releases\n",
      "2015: 485 new releases\n",
      "2016: 467 new releases\n",
      "2017: 383 new releases\n",
      "2018: 372 new releases\n",
      "2019: 299 new releases\n",
      "2020: 299 new releases\n",
      "2021: 219 new releases\n",
      "2022: 159 new releases\n",
      "2023: 165 new releases\n",
      "2024: 106 new releases\n",
      "2025: 83 new releases\n",
      "\n",
      "Added 4573 new releases to discogs_juke.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"JukeScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_juke.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1995, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Juke\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cb3ad",
   "metadata": {},
   "source": [
    "# Footwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28271a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007: 6 new releases\n",
      "2008: 8 new releases\n",
      "2009: 5 new releases\n",
      "2010: 42 new releases\n",
      "2011: 52 new releases\n",
      "2012: 85 new releases\n",
      "2013: 110 new releases\n",
      "2014: 132 new releases\n",
      "2015: 174 new releases\n",
      "2016: 175 new releases\n",
      "2017: 155 new releases\n",
      "2018: 200 new releases\n",
      "2019: 245 new releases\n",
      "2020: 419 new releases\n",
      "2021: 399 new releases\n",
      "2022: 313 new releases\n",
      "2023: 281 new releases\n",
      "2024: 240 new releases\n",
      "2025: 141 new releases\n",
      "\n",
      "Added 3182 new releases to discogs_footwork.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"FootworkScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_footwork.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(2005, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Footwork\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3c8cd",
   "metadata": {},
   "source": [
    "# Electro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7c2f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980: 554 new releases\n",
      "1981: 955 new releases\n",
      "1982: 1911 new releases\n",
      "1983: 3791 new releases\n",
      "1984: 4637 new releases\n",
      "1985: 4079 new releases\n",
      "1986: 3412 new releases\n",
      "1987: 2872 new releases\n",
      "1988: 2941 new releases\n",
      "1989: 2192 new releases\n",
      "1990: 1837 new releases\n",
      "1991: 1665 new releases\n",
      "1992: 1534 new releases\n",
      "1993: 1599 new releases\n",
      "1994: 1713 new releases\n",
      "1995: 2068 new releases\n",
      "1996: 2482 new releases\n",
      "1997: 3028 new releases\n",
      "1998: 3708 new releases\n",
      "1999: 3637 new releases\n",
      "2000: 3811 new releases\n",
      "2001: 4327 new releases\n",
      "2002: 5567 new releases\n",
      "2003: 6326 new releases\n",
      "2004: 7109 new releases\n",
      "2005: 7402 new releases\n",
      "2006: 7458 new releases\n",
      "2007: 7413 new releases\n",
      "2008: 7496 new releases\n",
      "2009: 7602 new releases\n",
      "2010: 7714 new releases\n",
      "2011: 7335 new releases\n",
      "2012: 7589 new releases\n",
      "2013: 7314 new releases\n",
      "2014: 7460 new releases\n",
      "2015: 7908 new releases\n",
      "2016: 7436 new releases\n",
      "2017: 7630 new releases\n",
      "2018: 7682 new releases\n",
      "2019: 7841 new releases\n",
      "2020: 7686 new releases\n",
      "2021: 7520 new releases\n",
      "2022: 7716 new releases\n",
      "2023: 7604 new releases\n",
      "2024: 6217 new releases\n",
      "2025: 4665 new releases\n",
      "\n",
      "Added 232443 new releases to discogs_electro.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"ElectroScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_electro.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1980, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Electro\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2084bba",
   "metadata": {},
   "source": [
    "# Breakbeat & Happy Hardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51590b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 9 new releases\n",
      "1993: 69 new releases\n",
      "1994: 364 new releases\n",
      "1995: 431 new releases\n",
      "1996: 268 new releases\n",
      "1997: 107 new releases\n",
      "1998: 59 new releases\n",
      "1999: 49 new releases\n",
      "2000: 38 new releases\n",
      "2001: 43 new releases\n",
      "2002: 70 new releases\n",
      "2003: 72 new releases\n",
      "2004: 75 new releases\n",
      "2005: 67 new releases\n",
      "2006: 52 new releases\n",
      "2007: 64 new releases\n",
      "2008: 51 new releases\n",
      "2009: 62 new releases\n",
      "2010: 63 new releases\n",
      "2011: 78 new releases\n",
      "2012: 77 new releases\n",
      "2013: 78 new releases\n",
      "2014: 58 new releases\n",
      "2015: 81 new releases\n",
      "2016: 80 new releases\n",
      "2017: 82 new releases\n",
      "2018: 53 new releases\n",
      "2019: 78 new releases\n",
      "2020: 92 new releases\n",
      "2021: 96 new releases\n",
      "2022: 107 new releases\n",
      "2023: 83 new releases\n",
      "2024: 79 new releases\n",
      "2025: 91 new releases\n",
      "\n",
      "Added 3226 new releases to discogs_breakbeat_happy_hardcore.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatHappyHardcoreScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_happy_hardcore.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1991, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Happy Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb775b",
   "metadata": {},
   "source": [
    "# Bleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "827d5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988: 5 new releases\n",
      "1989: 41 new releases\n",
      "1990: 452 new releases\n",
      "1991: 498 new releases\n",
      "1992: 139 new releases\n",
      "1993: 17 new releases\n",
      "1994: 5 new releases\n",
      "1995: 3 new releases\n",
      "1996: 7 new releases\n",
      "1997: 1 new releases\n",
      "1999: 15 new releases\n",
      "2000: 1 new releases\n",
      "2001: 3 new releases\n",
      "2002: 6 new releases\n",
      "2003: 3 new releases\n",
      "2004: 4 new releases\n",
      "2005: 4 new releases\n",
      "2006: 7 new releases\n",
      "2007: 5 new releases\n",
      "2008: 9 new releases\n",
      "2009: 8 new releases\n",
      "2010: 5 new releases\n",
      "2011: 13 new releases\n",
      "2012: 5 new releases\n",
      "2013: 5 new releases\n",
      "2014: 6 new releases\n",
      "2015: 12 new releases\n",
      "2016: 13 new releases\n",
      "2017: 26 new releases\n",
      "2018: 22 new releases\n",
      "2019: 46 new releases\n",
      "2020: 164 new releases\n",
      "2021: 158 new releases\n",
      "2022: 98 new releases\n",
      "2023: 111 new releases\n",
      "2024: 94 new releases\n",
      "2025: 77 new releases\n",
      "\n",
      "Added 2088 new releases to discogs_bleep.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BleepScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_bleep.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1988, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Bleep\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69975f",
   "metadata": {},
   "source": [
    "# Breakbeat, Hardcore, techno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbb3e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990: 45 new releases\n",
      "1991: 547 new releases\n",
      "1992: 1014 new releases\n",
      "1993: 363 new releases\n",
      "1994: 127 new releases\n",
      "1995: 85 new releases\n",
      "1996: 68 new releases\n",
      "1997: 82 new releases\n",
      "1998: 26 new releases\n",
      "1999: 28 new releases\n",
      "2000: 28 new releases\n",
      "2001: 33 new releases\n",
      "2002: 44 new releases\n",
      "2003: 19 new releases\n",
      "2004: 45 new releases\n",
      "2005: 36 new releases\n",
      "2006: 26 new releases\n",
      "2007: 20 new releases\n",
      "2008: 44 new releases\n",
      "2009: 30 new releases\n",
      "2010: 34 new releases\n",
      "2011: 38 new releases\n",
      "2012: 31 new releases\n",
      "2013: 28 new releases\n",
      "2014: 20 new releases\n",
      "2015: 34 new releases\n",
      "2016: 47 new releases\n",
      "2017: 54 new releases\n",
      "2018: 67 new releases\n",
      "2019: 87 new releases\n",
      "2020: 71 new releases\n",
      "2021: 81 new releases\n",
      "2022: 68 new releases\n",
      "2023: 63 new releases\n",
      "2024: 45 new releases\n",
      "2025: 46 new releases\n",
      "\n",
      "Added 3524 new releases to discogs_breakbeat_hardcore_techno.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatHardcoreTechnoScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_hardcore_techno.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1989, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Hardcore' in styles and 'Techno' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8b846",
   "metadata": {},
   "source": [
    "# Hardcore, jungle, techno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "063903b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 26 new releases\n",
      "1993: 70 new releases\n",
      "1994: 58 new releases\n",
      "1995: 34 new releases\n",
      "1996: 18 new releases\n",
      "1997: 29 new releases\n",
      "1998: 9 new releases\n",
      "1999: 6 new releases\n",
      "2000: 5 new releases\n",
      "2001: 8 new releases\n",
      "2002: 10 new releases\n",
      "2003: 11 new releases\n",
      "2004: 9 new releases\n",
      "2005: 16 new releases\n",
      "2006: 11 new releases\n",
      "2007: 12 new releases\n",
      "2008: 7 new releases\n",
      "2009: 9 new releases\n",
      "2010: 7 new releases\n",
      "2011: 15 new releases\n",
      "2012: 9 new releases\n",
      "2013: 19 new releases\n",
      "2014: 9 new releases\n",
      "2015: 12 new releases\n",
      "2016: 20 new releases\n",
      "2017: 19 new releases\n",
      "2018: 35 new releases\n",
      "2019: 54 new releases\n",
      "2020: 28 new releases\n",
      "2021: 42 new releases\n",
      "2022: 29 new releases\n",
      "2023: 35 new releases\n",
      "2024: 13 new releases\n",
      "2025: 17 new releases\n",
      "\n",
      "Added 711 new releases to discogs_hardcore_techno_jungle.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"HardcoreTechnoJungleScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_hardcore_techno_jungle.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Hardcore' in styles and 'Techno' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e1d22",
   "metadata": {},
   "source": [
    "# Jungle, techno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97cb8c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 38 new releases\n",
      "1993: 89 new releases\n",
      "1994: 92 new releases\n",
      "1995: 93 new releases\n",
      "1996: 92 new releases\n",
      "1997: 113 new releases\n",
      "1998: 66 new releases\n",
      "1999: 34 new releases\n",
      "2000: 33 new releases\n",
      "2001: 29 new releases\n",
      "2002: 32 new releases\n",
      "2003: 30 new releases\n",
      "2004: 26 new releases\n",
      "2005: 39 new releases\n",
      "2006: 38 new releases\n",
      "2007: 30 new releases\n",
      "2008: 27 new releases\n",
      "2009: 39 new releases\n",
      "2010: 32 new releases\n",
      "2011: 62 new releases\n",
      "2012: 59 new releases\n",
      "2013: 64 new releases\n",
      "2014: 80 new releases\n",
      "2015: 71 new releases\n",
      "2016: 90 new releases\n",
      "2017: 97 new releases\n",
      "2018: 133 new releases\n",
      "2019: 184 new releases\n",
      "2020: 234 new releases\n",
      "2021: 184 new releases\n",
      "2022: 149 new releases\n",
      "2023: 169 new releases\n",
      "2024: 82 new releases\n",
      "2025: 82 new releases\n",
      "\n",
      "Added 2712 new releases to discogs_jungle_techno.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"JungleTechnoScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_jungle_techno.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Techno' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c453ae",
   "metadata": {},
   "source": [
    "# Italo House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3b82c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986: 32 new releases\n",
      "1987: 117 new releases\n",
      "1988: 310 new releases\n",
      "1989: 752 new releases\n",
      "1990: 1202 new releases\n",
      "1991: 1262 new releases\n",
      "1992: 827 new releases\n",
      "1993: 432 new releases\n",
      "1994: 322 new releases\n",
      "1995: 204 new releases\n",
      "1996: 166 new releases\n",
      "1997: 151 new releases\n",
      "1998: 152 new releases\n",
      "1999: 166 new releases\n",
      "2000: 160 new releases\n",
      "2001: 112 new releases\n",
      "2002: 101 new releases\n",
      "2003: 111 new releases\n",
      "2004: 136 new releases\n",
      "2005: 113 new releases\n",
      "2006: 149 new releases\n",
      "2007: 165 new releases\n",
      "2008: 142 new releases\n",
      "2009: 258 new releases\n",
      "2010: 233 new releases\n",
      "2011: 154 new releases\n",
      "2012: 157 new releases\n",
      "2013: 152 new releases\n",
      "2014: 146 new releases\n",
      "2015: 161 new releases\n",
      "2016: 150 new releases\n",
      "2017: 159 new releases\n",
      "2018: 154 new releases\n",
      "2019: 191 new releases\n",
      "2020: 228 new releases\n",
      "2021: 182 new releases\n",
      "2022: 205 new releases\n",
      "2023: 198 new releases\n",
      "2024: 222 new releases\n",
      "2025: 171 new releases\n",
      "\n",
      "Added 10405 new releases to discogs_italo_house.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"ItaloHouseScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_italo_house.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1986, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Italo House\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957615",
   "metadata": {},
   "source": [
    "# Freetekno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa2a5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993: 6 new releases\n",
      "1994: 7 new releases\n",
      "1995: 41 new releases\n",
      "1996: 79 new releases\n",
      "1997: 139 new releases\n",
      "1998: 207 new releases\n",
      "1999: 256 new releases\n",
      "2000: 329 new releases\n",
      "2001: 230 new releases\n",
      "2002: 238 new releases\n",
      "2003: 283 new releases\n",
      "2004: 294 new releases\n",
      "2005: 331 new releases\n",
      "2006: 333 new releases\n",
      "2007: 349 new releases\n",
      "2008: 312 new releases\n",
      "2009: 308 new releases\n",
      "2010: 233 new releases\n",
      "2011: 274 new releases\n",
      "2012: 254 new releases\n",
      "2013: 237 new releases\n",
      "2014: 208 new releases\n",
      "2015: 191 new releases\n",
      "2016: 297 new releases\n",
      "2017: 262 new releases\n",
      "2018: 393 new releases\n",
      "2019: 365 new releases\n",
      "2020: 426 new releases\n",
      "2021: 483 new releases\n",
      "2022: 366 new releases\n",
      "2023: 413 new releases\n",
      "2024: 409 new releases\n",
      "2025: 358 new releases\n",
      "\n",
      "Added 8911 new releases to discogs_freetekno.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"FreeteknoScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_freetekno.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Freetekno\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d11f44",
   "metadata": {},
   "source": [
    "# Makina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbca1a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990: 1 new releases\n",
      "1991: 5 new releases\n",
      "1992: 35 new releases\n",
      "1993: 209 new releases\n",
      "1994: 528 new releases\n",
      "1995: 663 new releases\n",
      "1996: 782 new releases\n",
      "1997: 541 new releases\n",
      "1998: 348 new releases\n",
      "1999: 300 new releases\n",
      "2000: 343 new releases\n",
      "2001: 383 new releases\n",
      "2002: 270 new releases\n",
      "2003: 232 new releases\n",
      "2004: 226 new releases\n",
      "2005: 240 new releases\n",
      "2006: 199 new releases\n",
      "2007: 179 new releases\n",
      "2008: 194 new releases\n",
      "2009: 254 new releases\n",
      "2010: 244 new releases\n",
      "2011: 256 new releases\n",
      "2012: 257 new releases\n",
      "2013: 360 new releases\n",
      "2014: 325 new releases\n",
      "2015: 270 new releases\n",
      "2016: 216 new releases\n",
      "2017: 221 new releases\n",
      "2018: 274 new releases\n",
      "2019: 178 new releases\n",
      "2020: 219 new releases\n",
      "2021: 304 new releases\n",
      "2022: 248 new releases\n",
      "2023: 212 new releases\n",
      "2024: 153 new releases\n",
      "2025: 130 new releases\n",
      "\n",
      "Added 9799 new releases to discogs_makina.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"MakinaScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_makina.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Makina\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4555571",
   "metadata": {},
   "source": [
    "# Tribal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a274085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985: 97 new releases\n",
      "1986: 92 new releases\n",
      "1987: 132 new releases\n",
      "1988: 192 new releases\n",
      "1989: 270 new releases\n",
      "1990: 328 new releases\n",
      "1991: 383 new releases\n",
      "1992: 744 new releases\n",
      "1993: 883 new releases\n",
      "1994: 1336 new releases\n",
      "1995: 1292 new releases\n",
      "1996: 1330 new releases\n",
      "1997: 1288 new releases\n",
      "1998: 1296 new releases\n",
      "1999: 1348 new releases\n",
      "2000: 1604 new releases\n",
      "2001: 2108 new releases\n",
      "2002: 2283 new releases\n",
      "2003: 2476 new releases\n",
      "2004: 2019 new releases\n",
      "2005: 1684 new releases\n",
      "2006: 1494 new releases\n",
      "2007: 1375 new releases\n",
      "2008: 1306 new releases\n",
      "2009: 1351 new releases\n",
      "2010: 1299 new releases\n",
      "2011: 1030 new releases\n",
      "2012: 1126 new releases\n",
      "2013: 996 new releases\n",
      "2014: 971 new releases\n",
      "2015: 1056 new releases\n",
      "2016: 1241 new releases\n",
      "2017: 1261 new releases\n",
      "2018: 1412 new releases\n",
      "2019: 1443 new releases\n",
      "2020: 1582 new releases\n",
      "2021: 1331 new releases\n",
      "2022: 973 new releases\n",
      "2023: 1112 new releases\n",
      "2024: 937 new releases\n",
      "2025: 724 new releases\n",
      "\n",
      "Added 47205 new releases to discogs_tribal.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"TribalScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_tribal.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Tribal\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f282c",
   "metadata": {},
   "source": [
    "# Freetekno, tribal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20b821fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994: 2 new releases\n",
      "1995: 5 new releases\n",
      "1996: 22 new releases\n",
      "1997: 47 new releases\n",
      "1998: 60 new releases\n",
      "1999: 62 new releases\n",
      "2000: 98 new releases\n",
      "2001: 85 new releases\n",
      "2002: 78 new releases\n",
      "2003: 125 new releases\n",
      "2004: 90 new releases\n",
      "2005: 109 new releases\n",
      "2006: 90 new releases\n",
      "2007: 97 new releases\n",
      "2008: 168 new releases\n",
      "2009: 170 new releases\n",
      "2010: 165 new releases\n",
      "2011: 152 new releases\n",
      "2012: 154 new releases\n",
      "2013: 115 new releases\n",
      "2014: 100 new releases\n",
      "2015: 101 new releases\n",
      "2016: 153 new releases\n",
      "2017: 134 new releases\n",
      "2018: 272 new releases\n",
      "2019: 260 new releases\n",
      "2020: 209 new releases\n",
      "2021: 261 new releases\n",
      "2022: 102 new releases\n",
      "2023: 105 new releases\n",
      "2024: 76 new releases\n",
      "2025: 99 new releases\n",
      "\n",
      "Added 3766 new releases to discogs_tribal_freetekno.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"TribalFreeteknoScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_tribal_freetekno.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Tribal\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Freetekno' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80e344",
   "metadata": {},
   "source": [
    "# Hip Hop, Breakbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb6ba720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980: 1 new releases\n",
      "1983: 7 new releases\n",
      "1984: 11 new releases\n",
      "1985: 4 new releases\n",
      "1986: 10 new releases\n",
      "1987: 6 new releases\n",
      "1988: 21 new releases\n",
      "1989: 19 new releases\n",
      "1990: 73 new releases\n",
      "1991: 55 new releases\n",
      "1992: 38 new releases\n",
      "1993: 34 new releases\n",
      "1994: 31 new releases\n",
      "1995: 22 new releases\n",
      "1996: 51 new releases\n",
      "1997: 65 new releases\n",
      "1998: 96 new releases\n",
      "1999: 143 new releases\n",
      "2000: 117 new releases\n",
      "2001: 96 new releases\n",
      "2002: 155 new releases\n",
      "2003: 107 new releases\n",
      "2004: 143 new releases\n",
      "2005: 164 new releases\n",
      "2006: 151 new releases\n",
      "2007: 134 new releases\n",
      "2008: 107 new releases\n",
      "2009: 143 new releases\n",
      "2010: 121 new releases\n",
      "2011: 159 new releases\n",
      "2012: 153 new releases\n",
      "2013: 170 new releases\n",
      "2014: 127 new releases\n",
      "2015: 93 new releases\n",
      "2016: 89 new releases\n",
      "2017: 62 new releases\n",
      "2018: 80 new releases\n",
      "2019: 61 new releases\n",
      "2020: 89 new releases\n",
      "2021: 69 new releases\n",
      "2022: 50 new releases\n",
      "2023: 42 new releases\n",
      "2024: 32 new releases\n",
      "2025: 32 new releases\n",
      "\n",
      "Added 3433 new releases to discogs_hiphop_breakbeat.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"HiphopBreakbeatScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_hiphop_breakbeat.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1980, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Hip Hop' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca15ec",
   "metadata": {},
   "source": [
    "# Uk Funky\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9059967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006: 8 new releases\n",
      "2007: 19 new releases\n",
      "2008: 73 new releases\n",
      "2009: 185 new releases\n",
      "2010: 97 new releases\n",
      "2011: 47 new releases\n",
      "2012: 46 new releases\n",
      "2013: 16 new releases\n",
      "2014: 11 new releases\n",
      "2015: 15 new releases\n",
      "2016: 23 new releases\n",
      "2017: 34 new releases\n",
      "2018: 110 new releases\n",
      "2019: 100 new releases\n",
      "2020: 112 new releases\n",
      "2021: 85 new releases\n",
      "2022: 79 new releases\n",
      "2023: 63 new releases\n",
      "2024: 45 new releases\n",
      "2025: 25 new releases\n",
      "\n",
      "Added 1193 new releases to discogs_uk_funky.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"UKFunkyScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_uk_funky.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(2006, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"UK Funky\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35055c4",
   "metadata": {},
   "source": [
    "# Uk Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f38cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994: 17 new releases\n",
      "1995: 110 new releases\n",
      "1996: 258 new releases\n",
      "1997: 1206 new releases\n",
      "1998: 2068 new releases\n",
      "1999: 1763 new releases\n",
      "2000: 2963 new releases\n",
      "2001: 2858 new releases\n",
      "2002: 1997 new releases\n",
      "2003: 1323 new releases\n",
      "2004: 908 new releases\n",
      "2005: 734 new releases\n",
      "2006: 569 new releases\n",
      "2007: 585 new releases\n",
      "2008: 658 new releases\n",
      "2009: 579 new releases\n",
      "2010: 833 new releases\n",
      "2011: 1006 new releases\n",
      "2012: 1117 new releases\n",
      "2013: 996 new releases\n",
      "2014: 825 new releases\n",
      "2015: 709 new releases\n",
      "2016: 648 new releases\n",
      "2017: 721 new releases\n",
      "2018: 903 new releases\n",
      "2019: 923 new releases\n",
      "2020: 1093 new releases\n",
      "2021: 1283 new releases\n",
      "2022: 1052 new releases\n",
      "2023: 908 new releases\n",
      "2024: 888 new releases\n",
      "2025: 845 new releases\n",
      "\n",
      "Added 33346 new releases to discogs_uk_garage.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"UKGarageScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_uk_garage.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1994, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"UK Garage\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e3030",
   "metadata": {},
   "source": [
    "# Bassline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e364804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5613 existing releases\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     28\u001b[39m     response = requests.get(BASE_URL, headers=HEADERS, params={\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBassline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrelease\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mper_page\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m100\u001b[39m\n\u001b[32m     34\u001b[39m     })\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BasslineScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_bassline.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(2002, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Bassline\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42f369",
   "metadata": {},
   "source": [
    "# Speed Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a1a68b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996: 60 new releases\n",
      "1997: 1158 new releases\n",
      "1998: 1391 new releases\n",
      "1999: 257 new releases\n",
      "2000: 139 new releases\n",
      "2001: 113 new releases\n",
      "2002: 111 new releases\n",
      "2003: 106 new releases\n",
      "2004: 211 new releases\n",
      "2005: 291 new releases\n",
      "2006: 236 new releases\n",
      "2007: 189 new releases\n",
      "2008: 234 new releases\n",
      "2009: 149 new releases\n",
      "2010: 95 new releases\n",
      "2011: 75 new releases\n",
      "2012: 48 new releases\n",
      "2013: 45 new releases\n",
      "2014: 81 new releases\n",
      "2015: 64 new releases\n",
      "2016: 78 new releases\n",
      "2017: 41 new releases\n",
      "2018: 54 new releases\n",
      "2019: 71 new releases\n",
      "2020: 87 new releases\n",
      "2021: 142 new releases\n",
      "2022: 208 new releases\n",
      "2023: 171 new releases\n",
      "2024: 204 new releases\n",
      "2025: 183 new releases\n",
      "\n",
      "Added 6292 new releases to discogs_speed_garage.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"SpeedGarageScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_speed_garage.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1996, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Speed Garage\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a03793",
   "metadata": {},
   "source": [
    "# Ghetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a76caa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985: 1 new releases\n",
      "1987: 6 new releases\n",
      "1988: 2 new releases\n",
      "1989: 6 new releases\n",
      "1990: 10 new releases\n",
      "1991: 7 new releases\n",
      "1992: 19 new releases\n",
      "1993: 28 new releases\n",
      "1994: 84 new releases\n",
      "1995: 168 new releases\n",
      "1996: 187 new releases\n",
      "1997: 204 new releases\n",
      "1998: 103 new releases\n",
      "1999: 90 new releases\n",
      "2000: 69 new releases\n",
      "2001: 61 new releases\n",
      "2002: 67 new releases\n",
      "2003: 59 new releases\n",
      "2004: 86 new releases\n",
      "2005: 93 new releases\n",
      "2006: 161 new releases\n",
      "2007: 147 new releases\n",
      "2008: 228 new releases\n",
      "2009: 212 new releases\n",
      "2010: 183 new releases\n",
      "2011: 266 new releases\n",
      "2012: 362 new releases\n",
      "2013: 345 new releases\n",
      "2014: 258 new releases\n",
      "2015: 251 new releases\n",
      "2016: 244 new releases\n",
      "2017: 182 new releases\n",
      "2018: 197 new releases\n",
      "2019: 190 new releases\n",
      "2020: 260 new releases\n",
      "2021: 216 new releases\n",
      "2022: 158 new releases\n",
      "2023: 122 new releases\n",
      "2024: 87 new releases\n",
      "2025: 53 new releases\n",
      "\n",
      "Added 5472 new releases to discogs_ghetto.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"GhettoScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_ghetto.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Ghetto\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fbf36",
   "metadata": {},
   "source": [
    "# Miami Bass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0edd9266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984: 2 new releases\n",
      "1985: 4 new releases\n",
      "1986: 37 new releases\n",
      "1987: 94 new releases\n",
      "1988: 112 new releases\n",
      "1989: 132 new releases\n",
      "1990: 153 new releases\n",
      "1991: 147 new releases\n",
      "1992: 87 new releases\n",
      "1993: 99 new releases\n",
      "1994: 96 new releases\n",
      "1995: 77 new releases\n",
      "1996: 128 new releases\n",
      "1997: 101 new releases\n",
      "1998: 97 new releases\n",
      "1999: 98 new releases\n",
      "2000: 68 new releases\n",
      "2001: 30 new releases\n",
      "2002: 32 new releases\n",
      "2003: 28 new releases\n",
      "2004: 10 new releases\n",
      "2005: 17 new releases\n",
      "2006: 13 new releases\n",
      "2007: 17 new releases\n",
      "2008: 24 new releases\n",
      "2009: 26 new releases\n",
      "2010: 9 new releases\n",
      "2011: 9 new releases\n",
      "2012: 9 new releases\n",
      "2013: 25 new releases\n",
      "2014: 38 new releases\n",
      "2015: 24 new releases\n",
      "2016: 31 new releases\n",
      "2017: 40 new releases\n",
      "2018: 29 new releases\n",
      "2019: 27 new releases\n",
      "2020: 40 new releases\n",
      "2021: 28 new releases\n",
      "2022: 45 new releases\n",
      "2023: 39 new releases\n",
      "2024: 31 new releases\n",
      "2025: 16 new releases\n",
      "\n",
      "Added 2169 new releases to discogs_miami_bass.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"MiamiBassScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_miami_bass.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1984, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Miami Bass\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea96e3a",
   "metadata": {},
   "source": [
    "# Baltimore Club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23eb99d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991: 5 new releases\n",
      "1992: 9 new releases\n",
      "1993: 15 new releases\n",
      "1994: 15 new releases\n",
      "1995: 30 new releases\n",
      "1996: 36 new releases\n",
      "1997: 29 new releases\n",
      "1998: 50 new releases\n",
      "1999: 32 new releases\n",
      "2000: 26 new releases\n",
      "2001: 28 new releases\n",
      "2002: 32 new releases\n",
      "2003: 15 new releases\n",
      "2004: 3 new releases\n",
      "2005: 21 new releases\n",
      "2006: 37 new releases\n",
      "2007: 35 new releases\n",
      "2008: 51 new releases\n",
      "2009: 25 new releases\n",
      "2010: 9 new releases\n",
      "2011: 17 new releases\n",
      "2012: 28 new releases\n",
      "2013: 25 new releases\n",
      "2014: 37 new releases\n",
      "2015: 42 new releases\n",
      "2016: 44 new releases\n",
      "2017: 34 new releases\n",
      "2018: 36 new releases\n",
      "2019: 33 new releases\n",
      "2020: 41 new releases\n",
      "2021: 43 new releases\n",
      "2022: 35 new releases\n",
      "2023: 43 new releases\n",
      "2024: 17 new releases\n",
      "2025: 25 new releases\n",
      "\n",
      "Added 1003 new releases to discogs_baltimore_club.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BaltimoreClubScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_baltimore_club.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Baltimore Club\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dca77e",
   "metadata": {},
   "source": [
    "# Donk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d08af080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000: 68 new releases\n",
      "2001: 72 new releases\n",
      "2002: 65 new releases\n",
      "2003: 110 new releases\n",
      "2004: 206 new releases\n",
      "2005: 325 new releases\n",
      "2006: 355 new releases\n",
      "2007: 322 new releases\n",
      "2008: 297 new releases\n",
      "2009: 195 new releases\n",
      "2010: 71 new releases\n",
      "2011: 83 new releases\n",
      "2012: 124 new releases\n",
      "2013: 169 new releases\n",
      "2014: 118 new releases\n",
      "2015: 179 new releases\n",
      "2016: 198 new releases\n",
      "2017: 229 new releases\n",
      "2018: 274 new releases\n",
      "2019: 325 new releases\n",
      "2020: 402 new releases\n",
      "2021: 392 new releases\n",
      "2022: 380 new releases\n",
      "2023: 187 new releases\n",
      "2024: 168 new releases\n",
      "2025: 105 new releases\n",
      "\n",
      "Added 5419 new releases to discogs_donk.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DonkScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_donk.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(2000, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Donk\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4cf8fb",
   "metadata": {},
   "source": [
    "# Techno, Deep Techno, Acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0929fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991: 1 new releases\n",
      "1992: 2 new releases\n",
      "1993: 1 new releases\n",
      "1994: 5 new releases\n",
      "1995: 1 new releases\n",
      "1996: 3 new releases\n",
      "1997: 4 new releases\n",
      "1998: 1 new releases\n",
      "2000: 1 new releases\n",
      "2003: 1 new releases\n",
      "2004: 1 new releases\n",
      "2006: 3 new releases\n",
      "2010: 1 new releases\n",
      "2012: 1 new releases\n",
      "2013: 3 new releases\n",
      "2015: 3 new releases\n",
      "2016: 16 new releases\n",
      "2019: 10 new releases\n",
      "2021: 15 new releases\n",
      "2022: 11 new releases\n",
      "2023: 7 new releases\n",
      "2024: 8 new releases\n",
      "2025: 12 new releases\n",
      "\n",
      "Added 111 new releases to discogs_techno_deep_acid.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"TechnoDeepAcidScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_techno_deep_acid.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Techno\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Deep Techno' in styles and 'Acid' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20507c72",
   "metadata": {},
   "source": [
    "# Breakbeat, Acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6ab62de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988: 1 new releases\n",
      "1989: 9 new releases\n",
      "1990: 30 new releases\n",
      "1991: 58 new releases\n",
      "1992: 180 new releases\n",
      "1993: 169 new releases\n",
      "1994: 126 new releases\n",
      "1995: 175 new releases\n",
      "1996: 229 new releases\n",
      "1997: 187 new releases\n",
      "1998: 145 new releases\n",
      "1999: 91 new releases\n",
      "2000: 61 new releases\n",
      "2001: 31 new releases\n",
      "2002: 47 new releases\n",
      "2003: 44 new releases\n",
      "2004: 53 new releases\n",
      "2005: 41 new releases\n",
      "2006: 36 new releases\n",
      "2007: 47 new releases\n",
      "2008: 63 new releases\n",
      "2009: 59 new releases\n",
      "2010: 57 new releases\n",
      "2011: 76 new releases\n",
      "2012: 94 new releases\n",
      "2013: 68 new releases\n",
      "2014: 52 new releases\n",
      "2015: 91 new releases\n",
      "2016: 103 new releases\n",
      "2017: 108 new releases\n",
      "2018: 203 new releases\n",
      "2019: 291 new releases\n",
      "2020: 405 new releases\n",
      "2021: 346 new releases\n",
      "2022: 242 new releases\n",
      "2023: 223 new releases\n",
      "2024: 191 new releases\n",
      "2025: 83 new releases\n",
      "\n",
      "Added 4515 new releases to discogs_breakbeat_acid.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatAcidScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_acid.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1987, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Breakbeat\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Acid' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8e1e7",
   "metadata": {},
   "source": [
    "# Hardcore, Acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e328fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988: 3 new releases\n",
      "1990: 7 new releases\n",
      "1991: 64 new releases\n",
      "1992: 226 new releases\n",
      "1993: 461 new releases\n",
      "1994: 401 new releases\n",
      "1995: 214 new releases\n",
      "1996: 127 new releases\n",
      "1997: 155 new releases\n",
      "1998: 109 new releases\n",
      "1999: 65 new releases\n",
      "2000: 27 new releases\n",
      "2001: 45 new releases\n",
      "2002: 43 new releases\n",
      "2003: 32 new releases\n",
      "2004: 32 new releases\n",
      "2005: 38 new releases\n",
      "2006: 37 new releases\n",
      "2007: 30 new releases\n",
      "2008: 23 new releases\n",
      "2009: 17 new releases\n",
      "2010: 29 new releases\n",
      "2011: 14 new releases\n",
      "2012: 18 new releases\n",
      "2013: 11 new releases\n",
      "2014: 10 new releases\n",
      "2015: 21 new releases\n",
      "2016: 11 new releases\n",
      "2017: 33 new releases\n",
      "2018: 21 new releases\n",
      "2019: 19 new releases\n",
      "2020: 11 new releases\n",
      "2021: 13 new releases\n",
      "2022: 13 new releases\n",
      "2023: 5 new releases\n",
      "2024: 8 new releases\n",
      "2025: 30 new releases\n",
      "\n",
      "Added 2423 new releases to discogs_hardcore_acid.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"HardcoreAcidScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_hardcore_acid.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1988, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Hardcore\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Acid' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index_ids=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988d7fe",
   "metadata": {},
   "source": [
    "# Acid House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fff73ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985: 3 new releases\n",
      "1986: 34 new releases\n",
      "1987: 223 new releases\n",
      "1988: 1775 new releases\n",
      "1989: 2157 new releases\n",
      "1990: 1225 new releases\n",
      "1991: 893 new releases\n",
      "1992: 957 new releases\n",
      "1993: 888 new releases\n",
      "1994: 985 new releases\n",
      "1995: 1368 new releases\n",
      "1996: 1454 new releases\n",
      "1997: 1004 new releases\n",
      "1998: 656 new releases\n",
      "1999: 574 new releases\n",
      "2000: 474 new releases\n",
      "2001: 556 new releases\n",
      "2002: 575 new releases\n",
      "2003: 570 new releases\n",
      "2004: 693 new releases\n",
      "2005: 826 new releases\n",
      "2006: 728 new releases\n",
      "2007: 636 new releases\n",
      "2008: 549 new releases\n",
      "2009: 619 new releases\n",
      "2010: 744 new releases\n",
      "2011: 891 new releases\n",
      "2012: 925 new releases\n",
      "2013: 1008 new releases\n",
      "2014: 1144 new releases\n",
      "2015: 1200 new releases\n",
      "2016: 1210 new releases\n",
      "2017: 1315 new releases\n",
      "2018: 1179 new releases\n",
      "2019: 1260 new releases\n",
      "2020: 1409 new releases\n",
      "2021: 1186 new releases\n",
      "2022: 1134 new releases\n",
      "2023: 1031 new releases\n",
      "2024: 917 new releases\n",
      "2025: 886 new releases\n",
      "\n",
      "Added 37861 new releases to discogs_acid_house.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"AcidHouseScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_acid_house.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1985, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Acid House\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47d554",
   "metadata": {},
   "source": [
    "# Breakbeat, Hardcore, Jungle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e71c35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 321 new releases\n",
      "1993: 1085 new releases\n",
      "1994: 366 new releases\n",
      "1995: 94 new releases\n",
      "1996: 38 new releases\n",
      "1997: 42 new releases\n",
      "1998: 17 new releases\n",
      "1999: 16 new releases\n",
      "2000: 30 new releases\n",
      "2001: 32 new releases\n",
      "2002: 27 new releases\n",
      "2003: 21 new releases\n",
      "2004: 24 new releases\n",
      "2005: 37 new releases\n",
      "2006: 18 new releases\n",
      "2007: 21 new releases\n",
      "2008: 36 new releases\n",
      "2009: 42 new releases\n",
      "2010: 38 new releases\n",
      "2011: 57 new releases\n",
      "2012: 33 new releases\n",
      "2013: 64 new releases\n",
      "2014: 49 new releases\n",
      "2015: 51 new releases\n",
      "2016: 84 new releases\n",
      "2017: 94 new releases\n",
      "2018: 107 new releases\n",
      "2019: 132 new releases\n",
      "2020: 171 new releases\n",
      "2021: 220 new releases\n",
      "2022: 247 new releases\n",
      "2023: 170 new releases\n",
      "2024: 172 new releases\n",
      "2025: 191 new releases\n",
      "\n",
      "Added 4147 new releases to discogs_breakbeat_hardcore_jungle.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatHardcoreJungleScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_hardcore_jungle.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Breakbeat' in styles and 'Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a4d07",
   "metadata": {},
   "source": [
    "# Hardcore, Jungle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d3d3aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 411 new releases\n",
      "1993: 1443 new releases\n",
      "1994: 699 new releases\n",
      "1995: 188 new releases\n",
      "1996: 79 new releases\n",
      "1997: 86 new releases\n",
      "1998: 41 new releases\n",
      "1999: 30 new releases\n",
      "2000: 46 new releases\n",
      "2001: 62 new releases\n",
      "2002: 61 new releases\n",
      "2003: 56 new releases\n",
      "2004: 52 new releases\n",
      "2005: 71 new releases\n",
      "2006: 51 new releases\n",
      "2007: 64 new releases\n",
      "2008: 72 new releases\n",
      "2009: 85 new releases\n",
      "2010: 76 new releases\n",
      "2011: 110 new releases\n",
      "2012: 84 new releases\n",
      "2013: 114 new releases\n",
      "2014: 90 new releases\n",
      "2015: 119 new releases\n",
      "2016: 147 new releases\n",
      "2017: 150 new releases\n",
      "2018: 205 new releases\n",
      "2019: 258 new releases\n",
      "2020: 305 new releases\n",
      "2021: 352 new releases\n",
      "2022: 384 new releases\n",
      "2023: 301 new releases\n",
      "2024: 310 new releases\n",
      "2025: 319 new releases\n",
      "\n",
      "Added 6921 new releases to discogs_hardcore_jungle.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"HardcoreJungleScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_hardcore_jungle.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd90e85",
   "metadata": {},
   "source": [
    "# Dub, Jungle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e726cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 10 new releases\n",
      "1993: 24 new releases\n",
      "1994: 25 new releases\n",
      "1995: 69 new releases\n",
      "1996: 45 new releases\n",
      "1997: 41 new releases\n",
      "1998: 48 new releases\n",
      "1999: 37 new releases\n",
      "2000: 46 new releases\n",
      "2001: 15 new releases\n",
      "2002: 28 new releases\n",
      "2003: 28 new releases\n",
      "2004: 23 new releases\n",
      "2005: 27 new releases\n",
      "2006: 20 new releases\n",
      "2007: 23 new releases\n",
      "2008: 30 new releases\n",
      "2009: 35 new releases\n",
      "2010: 24 new releases\n",
      "2011: 27 new releases\n",
      "2012: 40 new releases\n",
      "2013: 47 new releases\n",
      "2014: 45 new releases\n",
      "2015: 44 new releases\n",
      "2016: 41 new releases\n",
      "2017: 41 new releases\n",
      "2018: 45 new releases\n",
      "2019: 44 new releases\n",
      "2020: 45 new releases\n",
      "2021: 41 new releases\n",
      "2022: 32 new releases\n",
      "2023: 16 new releases\n",
      "2024: 25 new releases\n",
      "2025: 15 new releases\n",
      "\n",
      "Added 1146 new releases to discogs_dub_jungle.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DubJungleScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_dub_jungle.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1990, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Jungle\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Dub' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b47f4",
   "metadata": {},
   "source": [
    "# Dubstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5b86610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000: 2 new releases\n",
      "2001: 25 new releases\n",
      "2002: 38 new releases\n",
      "2003: 73 new releases\n",
      "2004: 105 new releases\n",
      "2005: 146 new releases\n",
      "2006: 447 new releases\n",
      "2007: 952 new releases\n",
      "2008: 1760 new releases\n",
      "2009: 3423 new releases\n",
      "2010: 4176 new releases\n",
      "2011: 5442 new releases\n",
      "2012: 5602 new releases\n",
      "2013: 4248 new releases\n",
      "2014: 3275 new releases\n",
      "2015: 2889 new releases\n",
      "2016: 2802 new releases\n",
      "2017: 2458 new releases\n",
      "2018: 2639 new releases\n",
      "2019: 2437 new releases\n",
      "2020: 2296 new releases\n",
      "2021: 1882 new releases\n",
      "2022: 1446 new releases\n",
      "2023: 1316 new releases\n",
      "2024: 1011 new releases\n",
      "2025: 780 new releases\n",
      "\n",
      "Added 51670 new releases to discogs_dubstep.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DubstepScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_dubstep.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(2000, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Dubstep\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751dd757",
   "metadata": {},
   "source": [
    "# Britcore, Breakbeat, Hardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65b99fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990: 1 new releases\n",
      "1991: 1 new releases\n",
      "1992: 2 new releases\n",
      "1994: 3 new releases\n",
      "2013: 1 new releases\n",
      "\n",
      "Added 8 new releases to discogs_britcore_breakbeat_hardcore.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BritcoreBreakbeatHardcoreScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_britcore_breakbeat_hardcore.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1988, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Britcore\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Breakbeat' in styles and 'Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73a093",
   "metadata": {},
   "source": [
    "# drum n bass, samba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8c3bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996: 1 new releases\n",
      "1998: 4 new releases\n",
      "2000: 5 new releases\n",
      "2001: 4 new releases\n",
      "2002: 7 new releases\n",
      "2003: 13 new releases\n",
      "2004: 11 new releases\n",
      "2005: 5 new releases\n",
      "2006: 8 new releases\n",
      "2007: 10 new releases\n",
      "2008: 5 new releases\n",
      "2009: 1 new releases\n",
      "2010: 1 new releases\n",
      "2011: 1 new releases\n",
      "2015: 1 new releases\n",
      "2017: 2 new releases\n",
      "2018: 3 new releases\n",
      "2019: 2 new releases\n",
      "2024: 2 new releases\n",
      "\n",
      "Added 86 new releases to discogs_dnb_samba.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DnBSambaScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_dnb_samba.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1995, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Drum n Bass\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Samba' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8ded7",
   "metadata": {},
   "source": [
    "# Electro, Funk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eac092bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980: 20 new releases\n",
      "1981: 126 new releases\n",
      "1982: 333 new releases\n",
      "1983: 617 new releases\n",
      "1984: 685 new releases\n",
      "1985: 600 new releases\n",
      "1986: 442 new releases\n",
      "1987: 331 new releases\n",
      "1988: 301 new releases\n",
      "1989: 192 new releases\n",
      "1990: 103 new releases\n",
      "1991: 66 new releases\n",
      "1992: 53 new releases\n",
      "1993: 78 new releases\n",
      "1994: 75 new releases\n",
      "1995: 53 new releases\n",
      "1996: 67 new releases\n",
      "1997: 58 new releases\n",
      "1998: 65 new releases\n",
      "1999: 49 new releases\n",
      "2000: 68 new releases\n",
      "2001: 55 new releases\n",
      "2002: 60 new releases\n",
      "2003: 78 new releases\n",
      "2004: 102 new releases\n",
      "2005: 37 new releases\n",
      "2006: 29 new releases\n",
      "2007: 20 new releases\n",
      "2008: 23 new releases\n",
      "2009: 39 new releases\n",
      "2010: 40 new releases\n",
      "2011: 30 new releases\n",
      "2012: 42 new releases\n",
      "2013: 39 new releases\n",
      "2014: 38 new releases\n",
      "2015: 57 new releases\n",
      "2016: 34 new releases\n",
      "2017: 50 new releases\n",
      "2018: 45 new releases\n",
      "2019: 58 new releases\n",
      "2020: 30 new releases\n",
      "2021: 39 new releases\n",
      "2022: 74 new releases\n",
      "2023: 86 new releases\n",
      "2024: 3 new releases\n",
      "2025: 39 new releases\n",
      "\n",
      "Added 5529 new releases to discogs_electro_funk.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"ElectroFunkScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_electro_funk.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1980, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Electro\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Funk' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6980c",
   "metadata": {},
   "source": [
    "# breakbeat, hardcore, happy hardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6477e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992: 6 new releases\n",
      "1993: 43 new releases\n",
      "1994: 148 new releases\n",
      "1995: 140 new releases\n",
      "1996: 92 new releases\n",
      "1997: 62 new releases\n",
      "1998: 31 new releases\n",
      "1999: 19 new releases\n",
      "2000: 15 new releases\n",
      "2001: 12 new releases\n",
      "2002: 37 new releases\n",
      "2003: 52 new releases\n",
      "2004: 51 new releases\n",
      "2005: 38 new releases\n",
      "2006: 41 new releases\n",
      "2007: 43 new releases\n",
      "2008: 33 new releases\n",
      "2009: 40 new releases\n",
      "2010: 51 new releases\n",
      "2011: 68 new releases\n",
      "2012: 57 new releases\n",
      "2013: 70 new releases\n",
      "2014: 40 new releases\n",
      "2015: 50 new releases\n",
      "2016: 48 new releases\n",
      "2017: 49 new releases\n",
      "2018: 35 new releases\n",
      "2019: 62 new releases\n",
      "2020: 63 new releases\n",
      "2021: 48 new releases\n",
      "2022: 75 new releases\n",
      "2023: 65 new releases\n",
      "2024: 38 new releases\n",
      "2025: 62 new releases\n",
      "\n",
      "Added 1784 new releases to discogs_breakbeat_hardcore_happy_hardcore.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"BreakbeatHardcoreHappyHardcoreScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_breakbeat_hardcore_happy_hardcore.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1991, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"Happy Hardcore\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            styles = r.get('style', [])\n",
    "            if 'Breakbeat' in styles and 'Hardcore' in styles and str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(styles),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2baf29",
   "metadata": {},
   "source": [
    "# Dj Battle Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97ecb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980: 1 new releases\n",
      "1981: 3 new releases\n",
      "1983: 9 new releases\n",
      "1984: 7 new releases\n",
      "1985: 5 new releases\n",
      "1986: 2 new releases\n",
      "1987: 7 new releases\n",
      "1988: 12 new releases\n",
      "1989: 31 new releases\n",
      "1990: 39 new releases\n",
      "1991: 49 new releases\n",
      "1992: 56 new releases\n",
      "1993: 78 new releases\n",
      "1994: 39 new releases\n",
      "1995: 53 new releases\n",
      "1996: 59 new releases\n",
      "1997: 81 new releases\n",
      "1998: 107 new releases\n",
      "1999: 141 new releases\n",
      "2000: 190 new releases\n",
      "2001: 173 new releases\n",
      "2002: 144 new releases\n",
      "2003: 144 new releases\n",
      "2004: 143 new releases\n",
      "2005: 137 new releases\n",
      "2006: 113 new releases\n",
      "2007: 93 new releases\n",
      "2008: 71 new releases\n",
      "2009: 69 new releases\n",
      "2010: 54 new releases\n",
      "2011: 74 new releases\n",
      "2012: 43 new releases\n",
      "2013: 38 new releases\n",
      "2014: 38 new releases\n",
      "2015: 55 new releases\n",
      "2016: 101 new releases\n",
      "2017: 146 new releases\n",
      "2018: 94 new releases\n",
      "2019: 99 new releases\n",
      "2020: 77 new releases\n",
      "2021: 68 new releases\n",
      "2022: 40 new releases\n",
      "2023: 59 new releases\n",
      "2024: 34 new releases\n",
      "2025: 17 new releases\n",
      "\n",
      "Added 3093 new releases to discogs_dj_battle_tool.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"LLfStsfCkzWZJsJelXKKfztgmLTApuTiFZlaRcVV\"\n",
    "BASE_URL = \"https://api.discogs.com/database/search\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Discogs token={API_TOKEN}\",\n",
    "    \"User-Agent\": \"DJBattleToolScraper/1.0\"\n",
    "}\n",
    "\n",
    "CSV_FILE = \"discogs_dj_battle_tool.csv\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df_existing = pd.read_csv(CSV_FILE)\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist())\n",
    "    print(f\"Loaded {len(existing_ids)} existing releases\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for year in range(1980, 2026):\n",
    "    page = 1\n",
    "    year_count = 0\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params={\n",
    "            \"style\": \"DJ Battle Tool\",\n",
    "            \"type\": \"release\",\n",
    "            \"year\": year,\n",
    "            \"page\": page,\n",
    "            \"per_page\": 100\n",
    "        })\n",
    "        time.sleep(1.1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        for r in results:\n",
    "            if str(r['id']) not in existing_ids:\n",
    "                all_results.append({\n",
    "                    'id': r.get('id'),\n",
    "                    'title': r.get('title'),\n",
    "                    'year': r.get('year'),\n",
    "                    'label': ', '.join(r.get('label', [])),\n",
    "                    'catno': r.get('catno'),\n",
    "                    'format': ', '.join(r.get('format', [])),\n",
    "                    'style': ', '.join(r.get('style', [])),\n",
    "                    'country': r.get('country'),\n",
    "                    'resource_url': r.get('resource_url'),\n",
    "                })\n",
    "                existing_ids.add(str(r['id']))\n",
    "                year_count += 1\n",
    "        \n",
    "        if page >= data.get('pagination', {}).get('pages', 1):\n",
    "            break\n",
    "        page += 1\n",
    "    \n",
    "    if year_count > 0:\n",
    "        print(f\"{year}: {year_count} new releases\")\n",
    "\n",
    "if all_results:\n",
    "    df_new = pd.DataFrame(all_results)\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_old = pd.read_csv(CSV_FILE)\n",
    "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(CSV_FILE, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\nAdded {len(all_results)} new releases to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46864dfe",
   "metadata": {},
   "source": [
    "# Cleaning the CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8521fdb4",
   "metadata": {},
   "source": [
    "I analysed the datasets and noticed that a lot of duplicates where appearing and some information wasn't in an ideal format. For this step i then decided to start by correcting the weblink as it was cuurently showing the api endpoint instead of taking you to the discogs release page. This was an easy fix as the API link had the same release identification ID contained within it as the web link for users. i ran a simple script to fix this issue and update every file within my dataset before further processing and splitting. \n",
    "\n",
    "The next step needed for cleaning was to remove duplicate releases. These appear often as many releases will have multiple versions such as \"white label\" early pressings, different pressings for releases in different countries, different label pressings, future represses and more. I wanted to keep the data of the other releases rather than just remove them as this may be useful for future reference or for allowing users to view the various versions of the same release. I created four new columns to contain the data for duplicates: alt_versions_link, alt_versions_catno, alt_versions_year, alt_versions_id. These feature columns will contain the data on duplicates at the end of the file and will help reduce the overall row count used for recommendations. \n",
    "\n",
    "I also realised that splitting the data on duplicates would need to be done separately for different media formats. This is because a CD for a release often contains different and more tracks than the vinyl release and so keeping both is wise. I wanted these to be stored in separate CSV files for each release as they often cater to different types of users (Some people often only collect CDs or Vinyl) whilst also keeping a main CSV with all the releases. I also kept the base files and backed them up incase any issues occured due to the amount of time and resources it took to pull the data in the previous steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d91de",
   "metadata": {},
   "source": [
    "### Correcting the web link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6e5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed discogs_breakbeat_hardcore_happy_hardcore.csv\n",
      "Fixed discogs_italo_house.csv\n",
      "Fixed discogs_breakbeat_hardcore_techno.csv\n",
      "Fixed discogs_dubstep.csv\n",
      "Fixed discogs_jungle.csv\n",
      "Fixed discogs_hiphop_breakbeat.csv\n",
      "Fixed discogs_bassline.csv\n",
      "Fixed discogs_speed_garage.csv\n",
      "Fixed discogs_hardcore_acid.csv\n",
      "Fixed discogs_tribal_freetekno.csv\n",
      "Fixed discogs_dj_battle_tool.csv\n",
      "Fixed discogs_breakbeat_happy_hardcore.csv\n",
      "Fixed discogs_ghettotech.csv\n",
      "Fixed discogs_acid_house.csv\n",
      "Fixed discogs_electro_funk.csv\n",
      "Fixed discogs_happy_hardcore.csv\n",
      "Fixed discogs_dub_jungle.csv\n",
      "Fixed discogs_hardcore_jungle.csv\n",
      "Fixed discogs_uk_funky.csv\n",
      "Fixed discogs_uk_garage.csv\n",
      "Fixed discogs_ghetto.csv\n",
      "Fixed discogs_dnb.csv\n",
      "Fixed discogs_makina.csv\n",
      "Fixed discogs_miami_bass.csv\n",
      "Fixed discogs_bleep.csv\n",
      "Fixed discogs_donk.csv\n",
      "Fixed discogs_dnb_samba.csv\n",
      "Fixed discogs_breakbeat_acid.csv\n",
      "Fixed discogs_jungle_techno.csv\n",
      "Fixed discogs_footwork.csv\n",
      "Fixed discogs_juke.csv\n",
      "Fixed discogs_gabber.csv\n",
      "Fixed discogs_breakbeat_house.csv\n",
      "Fixed discogs_freetekno.csv\n",
      "Fixed discogs_tribal.csv\n",
      "Fixed discogs_baltimore_club.csv\n",
      "Fixed discogs_ghetto_house.csv\n",
      "Fixed discogs_techno_deep_acid.csv\n",
      "Fixed discogs_techno_future_jazz.csv\n",
      "Fixed discogs_hardcore_techno_jungle.csv\n",
      "Fixed discogs_breakbeat_hardcore.csv\n",
      "Fixed discogs_breakbeat_hardcore_jungle.csv\n",
      "Fixed discogs_breakbeat.csv\n",
      "Fixed discogs_britcore_breakbeat_hardcore.csv\n",
      "Fixed discogs_electro.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Fix resource_url in all CSV files - replace 'api.discogs.com/releases/' with 'www.discogs.com/release/'\n",
    "# This is to correct the URLs so that they point to the releases web page instead of the API endpoint\n",
    "for csv_file in glob.glob(\"discogs_*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if 'resource_url' in df.columns:\n",
    "        df['resource_url'] = df['resource_url'].str.replace('api.discogs.com/releases/', 'www.discogs.com/release/')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"Fixed {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18444990",
   "metadata": {},
   "source": [
    "### Removing duplicates & splitting by media format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a870bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 CSV files to process:\n",
      "  - discogs_breakbeat_hardcore_happy_hardcore.csv\n",
      "  - discogs_italo_house.csv\n",
      "  - discogs_breakbeat_hardcore_techno.csv\n",
      "  - discogs_dubstep.csv\n",
      "  - discogs_jungle.csv\n",
      "  - discogs_hiphop_breakbeat.csv\n",
      "  - discogs_bassline.csv\n",
      "  - discogs_speed_garage.csv\n",
      "  - discogs_hardcore_acid.csv\n",
      "  - discogs_tribal_freetekno.csv\n",
      "  - discogs_dj_battle_tool.csv\n",
      "  - discogs_breakbeat_happy_hardcore.csv\n",
      "  - discogs_ghettotech.csv\n",
      "  - discogs_acid_house.csv\n",
      "  - discogs_electro_funk.csv\n",
      "  - discogs_happy_hardcore.csv\n",
      "  - discogs_dub_jungle.csv\n",
      "  - discogs_hardcore_jungle.csv\n",
      "  - discogs_uk_funky.csv\n",
      "  - discogs_uk_garage.csv\n",
      "  - discogs_ghetto.csv\n",
      "  - discogs_dnb.csv\n",
      "  - discogs_makina.csv\n",
      "  - discogs_miami_bass.csv\n",
      "  - discogs_bleep.csv\n",
      "  - discogs_donk.csv\n",
      "  - discogs_dnb_samba.csv\n",
      "  - discogs_breakbeat_acid.csv\n",
      "  - discogs_jungle_techno.csv\n",
      "  - discogs_footwork.csv\n",
      "  - discogs_juke.csv\n",
      "  - discogs_gabber.csv\n",
      "  - discogs_breakbeat_house.csv\n",
      "  - discogs_freetekno.csv\n",
      "  - discogs_tribal.csv\n",
      "  - discogs_baltimore_club.csv\n",
      "  - discogs_ghetto_house.csv\n",
      "  - discogs_techno_deep_acid.csv\n",
      "  - discogs_techno_future_jazz.csv\n",
      "  - discogs_hardcore_techno_jungle.csv\n",
      "  - discogs_breakbeat_hardcore.csv\n",
      "  - discogs_breakbeat_hardcore_jungle.csv\n",
      "  - discogs_breakbeat.csv\n",
      "  - discogs_britcore_breakbeat_hardcore.csv\n",
      "  - discogs_electro.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_hardcore_happy_hardcore\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore\n",
      "============================================================\n",
      "Loaded 1784 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 177 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 1784\n",
      "After deduplication: 1607\n",
      "Duplicates removed: 177\n",
      "Compilations flagged: 422\n",
      "Releases with alt versions stored: 144\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     480\n",
      "CD          452\n",
      "Vinyl       356\n",
      "Cassette    258\n",
      "Other        28\n",
      "VHS          23\n",
      "Dubplate     10\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore...\n",
      "  cleaned_full.csv: 1607 releases\n",
      "  vinyl.csv: 356 releases\n",
      "  dubplate.csv: 10 releases\n",
      "  cd.csv: 452 releases\n",
      "  cassette.csv: 258 releases\n",
      "  digital.csv: 480 releases\n",
      "  vhs.csv: 23 releases\n",
      "  other.csv: 28 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_hardcore_happy_hardcore.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_italo_house\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house\n",
      "============================================================\n",
      "Loaded 10405 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 2038 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 10405\n",
      "After deduplication: 8367\n",
      "Duplicates removed: 2038\n",
      "Compilations flagged: 1267\n",
      "Releases with alt versions stored: 1190\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       3881\n",
      "CD          2570\n",
      "Digital     1326\n",
      "Cassette     529\n",
      "Other         42\n",
      "VHS           10\n",
      "Dubplate       9\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house...\n",
      "  cleaned_full.csv: 8367 releases\n",
      "  vinyl.csv: 3881 releases\n",
      "  dubplate.csv: 9 releases\n",
      "  cd.csv: 2570 releases\n",
      "  cassette.csv: 529 releases\n",
      "  digital.csv: 1326 releases\n",
      "  vhs.csv: 10 releases\n",
      "  other.csv: 42 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_italo_house.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_hardcore_techno\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno\n",
      "============================================================\n",
      "Loaded 3524 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 725 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3524\n",
      "After deduplication: 2799\n",
      "Duplicates removed: 725\n",
      "Compilations flagged: 640\n",
      "Releases with alt versions stored: 483\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       1295\n",
      "CD           662\n",
      "Cassette     402\n",
      "Digital      377\n",
      "VHS           34\n",
      "Other         20\n",
      "Dubplate       9\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno...\n",
      "  cleaned_full.csv: 2799 releases\n",
      "  vinyl.csv: 1295 releases\n",
      "  dubplate.csv: 9 releases\n",
      "  cd.csv: 662 releases\n",
      "  cassette.csv: 402 releases\n",
      "  digital.csv: 377 releases\n",
      "  vhs.csv: 34 releases\n",
      "  other.csv: 20 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_hardcore_techno.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_dubstep\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep\n",
      "============================================================\n",
      "Loaded 51670 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 5624 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 51670\n",
      "After deduplication: 46046\n",
      "Duplicates removed: 5624\n",
      "Compilations flagged: 4611\n",
      "Releases with alt versions stored: 4035\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     31106\n",
      "Vinyl        7853\n",
      "CD           6275\n",
      "Other         480\n",
      "Cassette      308\n",
      "Dubplate       23\n",
      "VHS             1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep...\n",
      "  cleaned_full.csv: 46046 releases\n",
      "  vinyl.csv: 7853 releases\n",
      "  dubplate.csv: 23 releases\n",
      "  cd.csv: 6275 releases\n",
      "  cassette.csv: 308 releases\n",
      "  digital.csv: 31106 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 480 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_dubstep.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_jungle\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle\n",
      "============================================================\n",
      "Loaded 37390 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 6293 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 37390\n",
      "After deduplication: 31097\n",
      "Duplicates removed: 6293\n",
      "Compilations flagged: 2784\n",
      "Releases with alt versions stored: 4662\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       11786\n",
      "Digital     10427\n",
      "CD           4358\n",
      "Cassette     3273\n",
      "Dubplate      749\n",
      "Other         447\n",
      "VHS            57\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle...\n",
      "  cleaned_full.csv: 31097 releases\n",
      "  vinyl.csv: 11786 releases\n",
      "  dubplate.csv: 749 releases\n",
      "  cd.csv: 4358 releases\n",
      "  cassette.csv: 3273 releases\n",
      "  digital.csv: 10427 releases\n",
      "  vhs.csv: 57 releases\n",
      "  other.csv: 447 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_jungle.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_hiphop_breakbeat\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat\n",
      "============================================================\n",
      "Loaded 3433 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 636 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3433\n",
      "After deduplication: 2797\n",
      "Duplicates removed: 636\n",
      "Compilations flagged: 1138\n",
      "Releases with alt versions stored: 288\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "CD          1110\n",
      "Digital      896\n",
      "Vinyl        590\n",
      "Cassette     152\n",
      "Other         34\n",
      "VHS           13\n",
      "Dubplate       2\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat...\n",
      "  cleaned_full.csv: 2797 releases\n",
      "  vinyl.csv: 590 releases\n",
      "  dubplate.csv: 2 releases\n",
      "  cd.csv: 1110 releases\n",
      "  cassette.csv: 152 releases\n",
      "  digital.csv: 896 releases\n",
      "  vhs.csv: 13 releases\n",
      "  other.csv: 34 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_hiphop_breakbeat.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_bassline\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline\n",
      "============================================================\n",
      "Loaded 5613 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 322 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 5613\n",
      "After deduplication: 5291\n",
      "Duplicates removed: 322\n",
      "Compilations flagged: 725\n",
      "Releases with alt versions stored: 241\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     3471\n",
      "Vinyl        904\n",
      "CD           697\n",
      "Cassette     116\n",
      "Other        101\n",
      "Dubplate       1\n",
      "VHS            1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline...\n",
      "  cleaned_full.csv: 5291 releases\n",
      "  vinyl.csv: 904 releases\n",
      "  dubplate.csv: 1 releases\n",
      "  cd.csv: 697 releases\n",
      "  cassette.csv: 116 releases\n",
      "  digital.csv: 3471 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 101 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_bassline.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_speed_garage\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage\n",
      "============================================================\n",
      "Loaded 6292 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 1089 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 6292\n",
      "After deduplication: 5203\n",
      "Duplicates removed: 1089\n",
      "Compilations flagged: 332\n",
      "Releases with alt versions stored: 775\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       2648\n",
      "CD          1200\n",
      "Digital      974\n",
      "Cassette     191\n",
      "Other        145\n",
      "Dubplate      42\n",
      "VHS            3\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage...\n",
      "  cleaned_full.csv: 5203 releases\n",
      "  vinyl.csv: 2648 releases\n",
      "  dubplate.csv: 42 releases\n",
      "  cd.csv: 1200 releases\n",
      "  cassette.csv: 191 releases\n",
      "  digital.csv: 974 releases\n",
      "  vhs.csv: 3 releases\n",
      "  other.csv: 145 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_speed_garage.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_hardcore_acid\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid\n",
      "============================================================\n",
      "Loaded 2423 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 436 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 2423\n",
      "After deduplication: 1987\n",
      "Duplicates removed: 436\n",
      "Compilations flagged: 179\n",
      "Releases with alt versions stored: 340\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       1062\n",
      "CD           483\n",
      "Digital      214\n",
      "Cassette     212\n",
      "Dubplate       6\n",
      "VHS            6\n",
      "Other          4\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid...\n",
      "  cleaned_full.csv: 1987 releases\n",
      "  vinyl.csv: 1062 releases\n",
      "  dubplate.csv: 6 releases\n",
      "  cd.csv: 483 releases\n",
      "  cassette.csv: 212 releases\n",
      "  digital.csv: 214 releases\n",
      "  vhs.csv: 6 releases\n",
      "  other.csv: 4 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_hardcore_acid.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_tribal_freetekno\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno\n",
      "============================================================\n",
      "Loaded 3766 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 371 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3766\n",
      "After deduplication: 3395\n",
      "Duplicates removed: 371\n",
      "Compilations flagged: 53\n",
      "Releases with alt versions stored: 291\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       2275\n",
      "Digital      895\n",
      "Cassette     141\n",
      "CD            80\n",
      "Dubplate       3\n",
      "Other          1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno...\n",
      "  cleaned_full.csv: 3395 releases\n",
      "  vinyl.csv: 2275 releases\n",
      "  dubplate.csv: 3 releases\n",
      "  cd.csv: 80 releases\n",
      "  cassette.csv: 141 releases\n",
      "  digital.csv: 895 releases\n",
      "  other.csv: 1 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_tribal_freetekno.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_dj_battle_tool\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool\n",
      "============================================================\n",
      "Loaded 3093 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 536 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3093\n",
      "After deduplication: 2557\n",
      "Duplicates removed: 536\n",
      "Compilations flagged: 122\n",
      "Releases with alt versions stored: 338\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       2102\n",
      "CD           237\n",
      "Digital      130\n",
      "Cassette      36\n",
      "Other         34\n",
      "VHS           16\n",
      "Dubplate       2\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool...\n",
      "  cleaned_full.csv: 2557 releases\n",
      "  vinyl.csv: 2102 releases\n",
      "  dubplate.csv: 2 releases\n",
      "  cd.csv: 237 releases\n",
      "  cassette.csv: 36 releases\n",
      "  digital.csv: 130 releases\n",
      "  vhs.csv: 16 releases\n",
      "  other.csv: 34 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_dj_battle_tool.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_happy_hardcore\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore\n",
      "============================================================\n",
      "Loaded 3226 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 432 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3226\n",
      "After deduplication: 2794\n",
      "Duplicates removed: 432\n",
      "Compilations flagged: 609\n",
      "Releases with alt versions stored: 345\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       881\n",
      "CD          802\n",
      "Digital     685\n",
      "Cassette    344\n",
      "Other        32\n",
      "Dubplate     31\n",
      "VHS          19\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore...\n",
      "  cleaned_full.csv: 2794 releases\n",
      "  vinyl.csv: 881 releases\n",
      "  dubplate.csv: 31 releases\n",
      "  cd.csv: 802 releases\n",
      "  cassette.csv: 344 releases\n",
      "  digital.csv: 685 releases\n",
      "  vhs.csv: 19 releases\n",
      "  other.csv: 32 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_happy_hardcore.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_ghettotech\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech\n",
      "============================================================\n",
      "Loaded 1512 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 101 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 1512\n",
      "After deduplication: 1411\n",
      "Duplicates removed: 101\n",
      "Compilations flagged: 178\n",
      "Releases with alt versions stored: 88\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     761\n",
      "Vinyl       365\n",
      "CD          161\n",
      "Cassette    111\n",
      "Other        12\n",
      "VHS           1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech...\n",
      "  cleaned_full.csv: 1411 releases\n",
      "  vinyl.csv: 365 releases\n",
      "  cd.csv: 161 releases\n",
      "  cassette.csv: 111 releases\n",
      "  digital.csv: 761 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 12 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_ghettotech.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_acid_house\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house\n",
      "============================================================\n",
      "Loaded 37861 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 7866 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 37861\n",
      "After deduplication: 29995\n",
      "Duplicates removed: 7866\n",
      "Compilations flagged: 4685\n",
      "Releases with alt versions stored: 4725\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       12601\n",
      "Digital      8220\n",
      "CD           6701\n",
      "Cassette     2079\n",
      "Other         253\n",
      "VHS            98\n",
      "Dubplate       43\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house...\n",
      "  cleaned_full.csv: 29995 releases\n",
      "  vinyl.csv: 12601 releases\n",
      "  dubplate.csv: 43 releases\n",
      "  cd.csv: 6701 releases\n",
      "  cassette.csv: 2079 releases\n",
      "  digital.csv: 8220 releases\n",
      "  vhs.csv: 98 releases\n",
      "  other.csv: 253 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_acid_house.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_electro_funk\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk\n",
      "============================================================\n",
      "Loaded 5529 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 1927 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 5529\n",
      "After deduplication: 3602\n",
      "Duplicates removed: 1927\n",
      "Compilations flagged: 532\n",
      "Releases with alt versions stored: 825\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       2097\n",
      "CD           887\n",
      "Cassette     350\n",
      "Digital      185\n",
      "Dubplate      40\n",
      "Other         28\n",
      "VHS           15\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk...\n",
      "  cleaned_full.csv: 3602 releases\n",
      "  vinyl.csv: 2097 releases\n",
      "  dubplate.csv: 40 releases\n",
      "  cd.csv: 887 releases\n",
      "  cassette.csv: 350 releases\n",
      "  digital.csv: 185 releases\n",
      "  vhs.csv: 15 releases\n",
      "  other.csv: 28 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_electro_funk.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_happy_hardcore\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore\n",
      "============================================================\n",
      "Loaded 34505 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 3864 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 34505\n",
      "After deduplication: 30641\n",
      "Duplicates removed: 3864\n",
      "Compilations flagged: 3306\n",
      "Releases with alt versions stored: 2835\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     13201\n",
      "CD           9173\n",
      "Vinyl        5212\n",
      "Cassette     2665\n",
      "Dubplate      176\n",
      "Other         143\n",
      "VHS            71\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore...\n",
      "  cleaned_full.csv: 30641 releases\n",
      "  vinyl.csv: 5212 releases\n",
      "  dubplate.csv: 176 releases\n",
      "  cd.csv: 9173 releases\n",
      "  cassette.csv: 2665 releases\n",
      "  digital.csv: 13201 releases\n",
      "  vhs.csv: 71 releases\n",
      "  other.csv: 143 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_happy_hardcore.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_dub_jungle\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle\n",
      "============================================================\n",
      "Loaded 1146 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 140 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 1146\n",
      "After deduplication: 1006\n",
      "Duplicates removed: 140\n",
      "Compilations flagged: 335\n",
      "Releases with alt versions stored: 96\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "CD          336\n",
      "Digital     300\n",
      "Vinyl       284\n",
      "Cassette     79\n",
      "Other         7\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle...\n",
      "  cleaned_full.csv: 1006 releases\n",
      "  vinyl.csv: 284 releases\n",
      "  cd.csv: 336 releases\n",
      "  cassette.csv: 79 releases\n",
      "  digital.csv: 300 releases\n",
      "  other.csv: 7 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_dub_jungle.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_hardcore_jungle\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle\n",
      "============================================================\n",
      "Loaded 6921 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 1360 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 6921\n",
      "After deduplication: 5561\n",
      "Duplicates removed: 1360\n",
      "Compilations flagged: 770\n",
      "Releases with alt versions stored: 1001\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       2281\n",
      "Digital     1381\n",
      "Cassette    1009\n",
      "CD           645\n",
      "Dubplate     121\n",
      "Other         94\n",
      "VHS           30\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle...\n",
      "  cleaned_full.csv: 5561 releases\n",
      "  vinyl.csv: 2281 releases\n",
      "  dubplate.csv: 121 releases\n",
      "  cd.csv: 645 releases\n",
      "  cassette.csv: 1009 releases\n",
      "  digital.csv: 1381 releases\n",
      "  vhs.csv: 30 releases\n",
      "  other.csv: 94 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_hardcore_jungle.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_uk_funky\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky\n",
      "============================================================\n",
      "Loaded 1193 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 73 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 1193\n",
      "After deduplication: 1120\n",
      "Duplicates removed: 73\n",
      "Compilations flagged: 116\n",
      "Releases with alt versions stored: 62\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     558\n",
      "Vinyl       409\n",
      "CD          119\n",
      "Cassette     19\n",
      "Other        15\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky...\n",
      "  cleaned_full.csv: 1120 releases\n",
      "  vinyl.csv: 409 releases\n",
      "  cd.csv: 119 releases\n",
      "  cassette.csv: 19 releases\n",
      "  digital.csv: 558 releases\n",
      "  other.csv: 15 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_uk_funky.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_uk_garage\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage\n",
      "============================================================\n",
      "Loaded 33346 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 6464 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 33346\n",
      "After deduplication: 26882\n",
      "Duplicates removed: 6464\n",
      "Compilations flagged: 2266\n",
      "Releases with alt versions stored: 4008\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       12019\n",
      "Digital      8415\n",
      "CD           4851\n",
      "Other         807\n",
      "Cassette      611\n",
      "Dubplate      157\n",
      "VHS            22\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage...\n",
      "  cleaned_full.csv: 26882 releases\n",
      "  vinyl.csv: 12019 releases\n",
      "  dubplate.csv: 157 releases\n",
      "  cd.csv: 4851 releases\n",
      "  cassette.csv: 611 releases\n",
      "  digital.csv: 8415 releases\n",
      "  vhs.csv: 22 releases\n",
      "  other.csv: 807 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_uk_garage.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_ghetto\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto\n",
      "============================================================\n",
      "Loaded 5472 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 609 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 5472\n",
      "After deduplication: 4863\n",
      "Duplicates removed: 609\n",
      "Compilations flagged: 661\n",
      "Releases with alt versions stored: 477\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     2274\n",
      "Vinyl       1443\n",
      "CD           746\n",
      "Cassette     348\n",
      "Other         46\n",
      "VHS            4\n",
      "Dubplate       2\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto...\n",
      "  cleaned_full.csv: 4863 releases\n",
      "  vinyl.csv: 1443 releases\n",
      "  dubplate.csv: 2 releases\n",
      "  cd.csv: 746 releases\n",
      "  cassette.csv: 348 releases\n",
      "  digital.csv: 2274 releases\n",
      "  vhs.csv: 4 releases\n",
      "  other.csv: 46 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_ghetto.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_dnb\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb\n",
      "============================================================\n",
      "Loaded 132558 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 20930 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 132558\n",
      "After deduplication: 111628\n",
      "Duplicates removed: 20930\n",
      "Compilations flagged: 8804\n",
      "Releases with alt versions stored: 15720\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     56570\n",
      "Vinyl       30624\n",
      "CD          18498\n",
      "Cassette     3675\n",
      "Dubplate     1478\n",
      "Other         715\n",
      "VHS            68\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb...\n",
      "  cleaned_full.csv: 111628 releases\n",
      "  vinyl.csv: 30624 releases\n",
      "  dubplate.csv: 1478 releases\n",
      "  cd.csv: 18498 releases\n",
      "  cassette.csv: 3675 releases\n",
      "  digital.csv: 56570 releases\n",
      "  vhs.csv: 68 releases\n",
      "  other.csv: 715 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_dnb.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_makina\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina\n",
      "============================================================\n",
      "Loaded 9799 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 748 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 9799\n",
      "After deduplication: 9051\n",
      "Duplicates removed: 748\n",
      "Compilations flagged: 500\n",
      "Releases with alt versions stored: 622\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       3585\n",
      "Digital     2827\n",
      "CD          2051\n",
      "Cassette     553\n",
      "Other         22\n",
      "VHS           13\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina...\n",
      "  cleaned_full.csv: 9051 releases\n",
      "  vinyl.csv: 3585 releases\n",
      "  cd.csv: 2051 releases\n",
      "  cassette.csv: 553 releases\n",
      "  digital.csv: 2827 releases\n",
      "  vhs.csv: 13 releases\n",
      "  other.csv: 22 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_makina.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_miami_bass\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass\n",
      "============================================================\n",
      "Loaded 2169 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 485 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 2169\n",
      "After deduplication: 1684\n",
      "Duplicates removed: 485\n",
      "Compilations flagged: 114\n",
      "Releases with alt versions stored: 289\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "CD          649\n",
      "Vinyl       579\n",
      "Cassette    218\n",
      "Digital     215\n",
      "Other        20\n",
      "Dubplate      2\n",
      "VHS           1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass...\n",
      "  cleaned_full.csv: 1684 releases\n",
      "  vinyl.csv: 579 releases\n",
      "  dubplate.csv: 2 releases\n",
      "  cd.csv: 649 releases\n",
      "  cassette.csv: 218 releases\n",
      "  digital.csv: 215 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 20 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_miami_bass.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_bleep\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep\n",
      "============================================================\n",
      "Loaded 2088 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 544 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 2088\n",
      "After deduplication: 1544\n",
      "Duplicates removed: 544\n",
      "Compilations flagged: 178\n",
      "Releases with alt versions stored: 344\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       948\n",
      "Digital     398\n",
      "CD          122\n",
      "Cassette     64\n",
      "Other         8\n",
      "Dubplate      3\n",
      "VHS           1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep...\n",
      "  cleaned_full.csv: 1544 releases\n",
      "  vinyl.csv: 948 releases\n",
      "  dubplate.csv: 3 releases\n",
      "  cd.csv: 122 releases\n",
      "  cassette.csv: 64 releases\n",
      "  digital.csv: 398 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 8 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_bleep.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_donk\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk\n",
      "============================================================\n",
      "Loaded 5419 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 257 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 5419\n",
      "After deduplication: 5162\n",
      "Duplicates removed: 257\n",
      "Compilations flagged: 233\n",
      "Releases with alt versions stored: 222\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     3274\n",
      "Vinyl       1204\n",
      "CD           619\n",
      "Cassette      46\n",
      "Other         19\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk...\n",
      "  cleaned_full.csv: 5162 releases\n",
      "  vinyl.csv: 1204 releases\n",
      "  cd.csv: 619 releases\n",
      "  cassette.csv: 46 releases\n",
      "  digital.csv: 3274 releases\n",
      "  other.csv: 19 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_donk.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_dnb_samba\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba\n",
      "============================================================\n",
      "Loaded 86 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 14 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 86\n",
      "After deduplication: 72\n",
      "Duplicates removed: 14\n",
      "Compilations flagged: 36\n",
      "Releases with alt versions stored: 11\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "CD          45\n",
      "Digital     13\n",
      "Vinyl       12\n",
      "Cassette     1\n",
      "Other        1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba...\n",
      "  cleaned_full.csv: 72 releases\n",
      "  vinyl.csv: 12 releases\n",
      "  cd.csv: 45 releases\n",
      "  cassette.csv: 1 releases\n",
      "  digital.csv: 13 releases\n",
      "  other.csv: 1 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_dnb_samba.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_acid\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid\n",
      "============================================================\n",
      "Loaded 4515 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 781 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 4515\n",
      "After deduplication: 3734\n",
      "Duplicates removed: 781\n",
      "Compilations flagged: 1130\n",
      "Releases with alt versions stored: 514\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       1438\n",
      "Digital     1260\n",
      "CD           715\n",
      "Cassette     274\n",
      "Other         40\n",
      "Dubplate       5\n",
      "VHS            2\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid...\n",
      "  cleaned_full.csv: 3734 releases\n",
      "  vinyl.csv: 1438 releases\n",
      "  dubplate.csv: 5 releases\n",
      "  cd.csv: 715 releases\n",
      "  cassette.csv: 274 releases\n",
      "  digital.csv: 1260 releases\n",
      "  vhs.csv: 2 releases\n",
      "  other.csv: 40 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_acid.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_jungle_techno\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno\n",
      "============================================================\n",
      "Loaded 2712 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 350 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 2712\n",
      "After deduplication: 2362\n",
      "Duplicates removed: 350\n",
      "Compilations flagged: 919\n",
      "Releases with alt versions stored: 237\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     943\n",
      "CD          513\n",
      "Vinyl       503\n",
      "Cassette    350\n",
      "Other        31\n",
      "VHS          22\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno...\n",
      "  cleaned_full.csv: 2362 releases\n",
      "  vinyl.csv: 503 releases\n",
      "  cd.csv: 513 releases\n",
      "  cassette.csv: 350 releases\n",
      "  digital.csv: 943 releases\n",
      "  vhs.csv: 22 releases\n",
      "  other.csv: 31 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_jungle_techno.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_footwork\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork\n",
      "============================================================\n",
      "Loaded 3182 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 161 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3182\n",
      "After deduplication: 3021\n",
      "Duplicates removed: 161\n",
      "Compilations flagged: 501\n",
      "Releases with alt versions stored: 136\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     2253\n",
      "Vinyl        331\n",
      "CD           207\n",
      "Cassette     207\n",
      "Other         23\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork...\n",
      "  cleaned_full.csv: 3021 releases\n",
      "  vinyl.csv: 331 releases\n",
      "  cd.csv: 207 releases\n",
      "  cassette.csv: 207 releases\n",
      "  digital.csv: 2253 releases\n",
      "  other.csv: 23 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_footwork.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_juke\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke\n",
      "============================================================\n",
      "Loaded 4573 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 399 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 4573\n",
      "After deduplication: 4174\n",
      "Duplicates removed: 399\n",
      "Compilations flagged: 645\n",
      "Releases with alt versions stored: 298\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     2859\n",
      "Vinyl        562\n",
      "CD           434\n",
      "Cassette     286\n",
      "Other         32\n",
      "VHS            1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke...\n",
      "  cleaned_full.csv: 4174 releases\n",
      "  vinyl.csv: 562 releases\n",
      "  cd.csv: 434 releases\n",
      "  cassette.csv: 286 releases\n",
      "  digital.csv: 2859 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 32 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_juke.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_gabber\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber\n",
      "============================================================\n",
      "Loaded 24308 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 3603 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 24308\n",
      "After deduplication: 20705\n",
      "Duplicates removed: 3603\n",
      "Compilations flagged: 2482\n",
      "Releases with alt versions stored: 2638\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     8349\n",
      "CD          5475\n",
      "Vinyl       4782\n",
      "Cassette    1805\n",
      "Other        172\n",
      "VHS          115\n",
      "Dubplate       7\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber...\n",
      "  cleaned_full.csv: 20705 releases\n",
      "  vinyl.csv: 4782 releases\n",
      "  dubplate.csv: 7 releases\n",
      "  cd.csv: 5475 releases\n",
      "  cassette.csv: 1805 releases\n",
      "  digital.csv: 8349 releases\n",
      "  vhs.csv: 115 releases\n",
      "  other.csv: 172 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_gabber.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_house\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house\n",
      "============================================================\n",
      "Loaded 13957 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 2541 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 13957\n",
      "After deduplication: 11416\n",
      "Duplicates removed: 2541\n",
      "Compilations flagged: 3172\n",
      "Releases with alt versions stored: 1542\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       4430\n",
      "CD          3857\n",
      "Digital     2174\n",
      "Cassette     696\n",
      "Other        202\n",
      "VHS           36\n",
      "Dubplate      21\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house...\n",
      "  cleaned_full.csv: 11416 releases\n",
      "  vinyl.csv: 4430 releases\n",
      "  dubplate.csv: 21 releases\n",
      "  cd.csv: 3857 releases\n",
      "  cassette.csv: 696 releases\n",
      "  digital.csv: 2174 releases\n",
      "  vhs.csv: 36 releases\n",
      "  other.csv: 202 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_house.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_freetekno\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno\n",
      "============================================================\n",
      "Loaded 8911 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 996 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 8911\n",
      "After deduplication: 7915\n",
      "Duplicates removed: 996\n",
      "Compilations flagged: 133\n",
      "Releases with alt versions stored: 721\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       5287\n",
      "Digital     1895\n",
      "Cassette     379\n",
      "CD           344\n",
      "Other          7\n",
      "Dubplate       3\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno...\n",
      "  cleaned_full.csv: 7915 releases\n",
      "  vinyl.csv: 5287 releases\n",
      "  dubplate.csv: 3 releases\n",
      "  cd.csv: 344 releases\n",
      "  cassette.csv: 379 releases\n",
      "  digital.csv: 1895 releases\n",
      "  other.csv: 7 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_freetekno.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_tribal\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal\n",
      "============================================================\n",
      "Loaded 47205 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 8686 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 47205\n",
      "After deduplication: 38519\n",
      "Duplicates removed: 8686\n",
      "Compilations flagged: 3951\n",
      "Releases with alt versions stored: 5608\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       13434\n",
      "CD          12088\n",
      "Digital     10603\n",
      "Cassette     2083\n",
      "Other         232\n",
      "Dubplate       46\n",
      "VHS            33\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal...\n",
      "  cleaned_full.csv: 38519 releases\n",
      "  vinyl.csv: 13434 releases\n",
      "  dubplate.csv: 46 releases\n",
      "  cd.csv: 12088 releases\n",
      "  cassette.csv: 2083 releases\n",
      "  digital.csv: 10603 releases\n",
      "  vhs.csv: 33 releases\n",
      "  other.csv: 232 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_tribal.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_baltimore_club\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club\n",
      "============================================================\n",
      "Loaded 1003 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 67 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 1003\n",
      "After deduplication: 936\n",
      "Duplicates removed: 67\n",
      "Compilations flagged: 115\n",
      "Releases with alt versions stored: 55\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       425\n",
      "Digital     347\n",
      "CD          114\n",
      "Cassette     42\n",
      "Other         6\n",
      "Dubplate      1\n",
      "VHS           1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club...\n",
      "  cleaned_full.csv: 936 releases\n",
      "  vinyl.csv: 425 releases\n",
      "  dubplate.csv: 1 releases\n",
      "  cd.csv: 114 releases\n",
      "  cassette.csv: 42 releases\n",
      "  digital.csv: 347 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 6 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_baltimore_club.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_ghetto_house\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house\n",
      "============================================================\n",
      "Loaded 3203 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 392 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 3203\n",
      "After deduplication: 2811\n",
      "Duplicates removed: 392\n",
      "Compilations flagged: 364\n",
      "Releases with alt versions stored: 312\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     1289\n",
      "Vinyl        952\n",
      "CD           358\n",
      "Cassette     193\n",
      "Other         17\n",
      "Dubplate       1\n",
      "VHS            1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house...\n",
      "  cleaned_full.csv: 2811 releases\n",
      "  vinyl.csv: 952 releases\n",
      "  dubplate.csv: 1 releases\n",
      "  cd.csv: 358 releases\n",
      "  cassette.csv: 193 releases\n",
      "  digital.csv: 1289 releases\n",
      "  vhs.csv: 1 releases\n",
      "  other.csv: 17 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_ghetto_house.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_techno_deep_acid\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid\n",
      "============================================================\n",
      "Loaded 111 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 4 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 111\n",
      "After deduplication: 107\n",
      "Duplicates removed: 4\n",
      "Compilations flagged: 9\n",
      "Releases with alt versions stored: 4\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     54\n",
      "Vinyl       35\n",
      "CD          12\n",
      "Cassette     5\n",
      "Other        1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid...\n",
      "  cleaned_full.csv: 107 releases\n",
      "  vinyl.csv: 35 releases\n",
      "  cd.csv: 12 releases\n",
      "  cassette.csv: 5 releases\n",
      "  digital.csv: 54 releases\n",
      "  other.csv: 1 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_techno_deep_acid.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_techno_future_jazz\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz\n",
      "============================================================\n",
      "Loaded 422 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 60 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 422\n",
      "After deduplication: 362\n",
      "Duplicates removed: 60\n",
      "Compilations flagged: 110\n",
      "Releases with alt versions stored: 50\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "CD          185\n",
      "Vinyl       146\n",
      "Other        15\n",
      "Cassette      8\n",
      "Digital       8\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz...\n",
      "  cleaned_full.csv: 362 releases\n",
      "  vinyl.csv: 146 releases\n",
      "  cd.csv: 185 releases\n",
      "  cassette.csv: 8 releases\n",
      "  digital.csv: 8 releases\n",
      "  other.csv: 15 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_techno_future_jazz.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_hardcore_techno_jungle\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle\n",
      "============================================================\n",
      "Loaded 711 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 88 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 711\n",
      "After deduplication: 623\n",
      "Duplicates removed: 88\n",
      "Compilations flagged: 285\n",
      "Releases with alt versions stored: 58\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     187\n",
      "Cassette    138\n",
      "Vinyl       134\n",
      "CD          130\n",
      "VHS          21\n",
      "Other        13\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle...\n",
      "  cleaned_full.csv: 623 releases\n",
      "  vinyl.csv: 134 releases\n",
      "  cd.csv: 130 releases\n",
      "  cassette.csv: 138 releases\n",
      "  digital.csv: 187 releases\n",
      "  vhs.csv: 21 releases\n",
      "  other.csv: 13 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_hardcore_techno_jungle.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_hardcore\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore\n",
      "============================================================\n",
      "Loaded 17387 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 3997 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 17387\n",
      "After deduplication: 13390\n",
      "Duplicates removed: 3997\n",
      "Compilations flagged: 1305\n",
      "Releases with alt versions stored: 2690\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       6673\n",
      "Digital     2888\n",
      "CD          1839\n",
      "Cassette    1602\n",
      "Dubplate     191\n",
      "Other        142\n",
      "VHS           55\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore...\n",
      "  cleaned_full.csv: 13390 releases\n",
      "  vinyl.csv: 6673 releases\n",
      "  dubplate.csv: 191 releases\n",
      "  cd.csv: 1839 releases\n",
      "  cassette.csv: 1602 releases\n",
      "  digital.csv: 2888 releases\n",
      "  vhs.csv: 55 releases\n",
      "  other.csv: 142 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_hardcore.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat_hardcore_jungle\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle\n",
      "============================================================\n",
      "Loaded 4147 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 911 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 4147\n",
      "After deduplication: 3236\n",
      "Duplicates removed: 911\n",
      "Compilations flagged: 328\n",
      "Releases with alt versions stored: 669\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       1634\n",
      "Digital      606\n",
      "Cassette     491\n",
      "CD           334\n",
      "Dubplate      94\n",
      "Other         58\n",
      "VHS           19\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle...\n",
      "  cleaned_full.csv: 3236 releases\n",
      "  vinyl.csv: 1634 releases\n",
      "  dubplate.csv: 94 releases\n",
      "  cd.csv: 334 releases\n",
      "  cassette.csv: 491 releases\n",
      "  digital.csv: 606 releases\n",
      "  vhs.csv: 19 releases\n",
      "  other.csv: 58 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat_hardcore_jungle.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_breakbeat\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat\n",
      "============================================================\n",
      "Loaded 88909 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 18045 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 88909\n",
      "After deduplication: 70864\n",
      "Duplicates removed: 18045\n",
      "Compilations flagged: 9292\n",
      "Releases with alt versions stored: 10597\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl       27081\n",
      "Digital     20699\n",
      "CD          17267\n",
      "Cassette     4495\n",
      "Other         807\n",
      "Dubplate      354\n",
      "VHS           161\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat...\n",
      "  cleaned_full.csv: 70864 releases\n",
      "  vinyl.csv: 27081 releases\n",
      "  dubplate.csv: 354 releases\n",
      "  cd.csv: 17267 releases\n",
      "  cassette.csv: 4495 releases\n",
      "  digital.csv: 20699 releases\n",
      "  vhs.csv: 161 releases\n",
      "  other.csv: 807 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_breakbeat.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_britcore_breakbeat_hardcore\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_britcore_breakbeat_hardcore\n",
      "============================================================\n",
      "Loaded 8 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 3 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 8\n",
      "After deduplication: 5\n",
      "Duplicates removed: 3\n",
      "Compilations flagged: 1\n",
      "Releases with alt versions stored: 2\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Vinyl      4\n",
      "Digital    1\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_britcore_breakbeat_hardcore...\n",
      "  cleaned_full.csv: 5 releases\n",
      "  vinyl.csv: 4 releases\n",
      "  digital.csv: 1 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_britcore_breakbeat_hardcore.csv\n",
      "\n",
      "============================================================\n",
      "Processing: discogs_electro\n",
      "Output to: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro\n",
      "============================================================\n",
      "Loaded 232443 releases\n",
      "\n",
      "Detecting duplicates (within same media type only)...\n",
      "  Found 45593 duplicates (same title + same media type)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Original releases: 232443\n",
      "After deduplication: 186850\n",
      "Duplicates removed: 45593\n",
      "Compilations flagged: 6778\n",
      "Releases with alt versions stored: 24931\n",
      "\n",
      "Format breakdown:\n",
      "primary_format\n",
      "Digital     77261\n",
      "Vinyl       49650\n",
      "CD          49571\n",
      "Cassette     7919\n",
      "Other        1852\n",
      "VHS           346\n",
      "Dubplate      251\n",
      "\n",
      "Saving files to /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro...\n",
      "  cleaned_full.csv: 186850 releases\n",
      "  vinyl.csv: 49650 releases\n",
      "  dubplate.csv: 251 releases\n",
      "  cd.csv: 49571 releases\n",
      "  cassette.csv: 7919 releases\n",
      "  digital.csv: 77261 releases\n",
      "  vhs.csv: 346 releases\n",
      "  other.csv: 1852 releases\n",
      "\n",
      "Done! Original file unchanged: /Users/benhasy/Documents/UNI/Foundations of AI/api/discogs_electro.csv\n",
      "\n",
      "============================================================\n",
      "ALL FILES PROCESSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DISCOGS DATA CLEANING PIPELINE v3 - JUPYTER NOTEBOOK VERSION\n",
    "Cleans and splits Discogs CSV exports according to project specifications.\n",
    "\n",
    "DUPLICATE RULES:\n",
    "- Same title + same media type = duplicate (keep best, store alt info)\n",
    "- Same title + different media type = NOT duplicate (keep both)\n",
    "- Vinyl 12\" and Vinyl 7\" are same media type\n",
    "- CD and Cassette are different media types\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === CONFIGURABLE PARAMETERS ===\n",
    "COMPILATION_STYLE_THRESHOLD = 5   # Styles > this = compilation\n",
    "EMPTY_CATNO_PLACEHOLDER = 'N0N3 - 000'\n",
    "\n",
    "\n",
    "def get_primary_format(format_str):\n",
    "    \"\"\"\n",
    "    Extract primary format from format string.\n",
    "    \"\"\"\n",
    "    if pd.isna(format_str):\n",
    "        return 'Other'\n",
    "    \n",
    "    format_str = str(format_str)\n",
    "    \n",
    "    if 'Acetate' in format_str:\n",
    "        return 'Dubplate'\n",
    "    elif 'Vinyl' in format_str:\n",
    "        return 'Vinyl'\n",
    "    elif 'CD' in format_str:\n",
    "        return 'CD'\n",
    "    elif 'Cassette' in format_str:\n",
    "        return 'Cassette'\n",
    "    elif 'VHS' in format_str:\n",
    "        return 'VHS'\n",
    "    elif 'File' in format_str:\n",
    "        return 'Digital'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "def parse_artist_from_title(title):\n",
    "    \"\"\"\n",
    "    Parse artist from title string.\n",
    "    Format is typically \"Artist - Track Name\"\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return '', title\n",
    "    \n",
    "    title = str(title)\n",
    "    \n",
    "    if ' - ' in title:\n",
    "        parts = title.split(' - ', 1)\n",
    "        artist = parts[0].strip()\n",
    "        track_name = parts[1].strip() if len(parts) > 1 else ''\n",
    "        return artist, track_name\n",
    "    else:\n",
    "        return '', title\n",
    "\n",
    "\n",
    "def clean_catno(catno):\n",
    "    \"\"\"\n",
    "    Clean catalogue number - replace empty/none with placeholder.\n",
    "    \"\"\"\n",
    "    if pd.isna(catno) or str(catno).lower().strip() in ['none', '', 'nan']:\n",
    "        return EMPTY_CATNO_PLACEHOLDER\n",
    "    return str(catno).strip()\n",
    "\n",
    "\n",
    "def is_compilation(style_str):\n",
    "    \"\"\"\n",
    "    Check if release is a compilation based on number of styles.\n",
    "    \"\"\"\n",
    "    if pd.isna(style_str):\n",
    "        return False\n",
    "    \n",
    "    styles = [s.strip() for s in str(style_str).split(',')]\n",
    "    return len(styles) > COMPILATION_STYLE_THRESHOLD\n",
    "\n",
    "\n",
    "def is_white_label(format_str):\n",
    "    \"\"\"\n",
    "    Check if release is a white label.\n",
    "    \"\"\"\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'white label' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def is_promo(format_str):\n",
    "    \"\"\"\n",
    "    Check if release is a promo.\n",
    "    \"\"\"\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'promo' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def is_test_pressing(format_str):\n",
    "    \"\"\"\n",
    "    Check if release is a test pressing.\n",
    "    \"\"\"\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'test pressing' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def get_format_priority(row):\n",
    "    \"\"\"\n",
    "    Return priority score for determining primary release.\n",
    "    Higher = better candidate for primary.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Prefer non-white-label\n",
    "    if not is_white_label(row['format']):\n",
    "        score += 100\n",
    "    \n",
    "    # Prefer non-promo\n",
    "    if not is_promo(row['format']):\n",
    "        score += 50\n",
    "    \n",
    "    # Prefer non-test-pressing\n",
    "    if not is_test_pressing(row['format']):\n",
    "        score += 50\n",
    "    \n",
    "    # Has real label bonus\n",
    "    if pd.notna(row['label']) and 'not on label' not in str(row['label']).lower():\n",
    "        score += 80\n",
    "    \n",
    "    # Earlier year bonus (original pressing)\n",
    "    if pd.notna(row['year']) and row['year'] > 0:\n",
    "        score += (2030 - row['year'])\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def clean_discogs_csv(input_path, output_base_folder):\n",
    "    \"\"\"\n",
    "    Clean a single Discogs CSV file and split by format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get filename without extension for naming\n",
    "    filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    \n",
    "    # Create output folder: split_<filename>\n",
    "    output_folder = os.path.join(output_base_folder, f\"split_{filename}\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"Output to: {output_folder}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(input_path)\n",
    "    original_count = len(df)\n",
    "    print(f\"Loaded {original_count} releases\")\n",
    "    \n",
    "    # Store original title\n",
    "    df['original_title'] = df['title']\n",
    "    \n",
    "    # Parse artist from title\n",
    "    df['parsed_artist'], df['track_name'] = zip(*df['title'].apply(parse_artist_from_title))\n",
    "    \n",
    "    # Extract primary format\n",
    "    df['primary_format'] = df['format'].apply(get_primary_format)\n",
    "    \n",
    "    # Clean catalogue numbers\n",
    "    df['catno_cleaned'] = df['catno'].apply(clean_catno)\n",
    "    \n",
    "    # Flag compilations\n",
    "    df['is_compilation'] = df['style'].apply(is_compilation)\n",
    "    \n",
    "    # Add source file column\n",
    "    df['source_file'] = filename\n",
    "    \n",
    "    # === HANDLE DUPLICATES ===\n",
    "    print(\"\\nDetecting duplicates (within same media type only)...\")\n",
    "    \n",
    "    # Create matching keys\n",
    "    df['title_lower'] = df['title'].str.lower().str.strip()\n",
    "    \n",
    "    # Calculate priority score for each release\n",
    "    df['priority_score'] = df.apply(get_format_priority, axis=1)\n",
    "    \n",
    "    # Initialize alt_versions columns\n",
    "    df['alt_versions_link'] = ''\n",
    "    df['alt_versions_catno'] = ''\n",
    "    df['alt_versions_year'] = ''\n",
    "    df['alt_versions_id'] = ''\n",
    "    \n",
    "    # Track which rows to keep\n",
    "    df['keep'] = True\n",
    "    \n",
    "    # === DUPLICATE DETECTION BY MEDIA TYPE ===\n",
    "    duplicates_removed = 0\n",
    "    \n",
    "    # Group by title (case-insensitive) AND primary_format\n",
    "    grouped = df.groupby(['title_lower', 'primary_format'])\n",
    "    \n",
    "    for (title_lower, media_type), group in grouped:\n",
    "        if len(group) > 1:\n",
    "            indices = group.index.tolist()\n",
    "            \n",
    "            # Find best version (highest priority score)\n",
    "            best_idx = group['priority_score'].idxmax()\n",
    "            \n",
    "            # Collect alt versions info\n",
    "            alt_links = []\n",
    "            alt_catnos = []\n",
    "            alt_years = []\n",
    "            alt_ids = []\n",
    "            \n",
    "            for idx in indices:\n",
    "                if idx != best_idx:\n",
    "                    alt_links.append(str(df.loc[idx, 'resource_url']))\n",
    "                    alt_catnos.append(str(df.loc[idx, 'catno_cleaned']))\n",
    "                    alt_years.append(str(df.loc[idx, 'year']))\n",
    "                    alt_ids.append(str(df.loc[idx, 'id']))\n",
    "                    \n",
    "                    df.loc[idx, 'keep'] = False\n",
    "                    duplicates_removed += 1\n",
    "            \n",
    "            # Store alt info on the kept release\n",
    "            if alt_links:\n",
    "                df.loc[best_idx, 'alt_versions_link'] = ','.join(alt_links)\n",
    "                df.loc[best_idx, 'alt_versions_catno'] = ','.join(alt_catnos)\n",
    "                df.loc[best_idx, 'alt_versions_year'] = ','.join(alt_years)\n",
    "                df.loc[best_idx, 'alt_versions_id'] = ','.join(alt_ids)\n",
    "    \n",
    "    print(f\"  Found {duplicates_removed} duplicates (same title + same media type)\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_cleaned = df[df['keep'] == True].copy()\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    df_cleaned = df_cleaned.drop(columns=['title_lower', 'priority_score', 'keep'])\n",
    "    \n",
    "    # Reorder columns to put alt_versions at the end\n",
    "    cols = [c for c in df_cleaned.columns if not c.startswith('alt_versions_')]\n",
    "    alt_cols = ['alt_versions_link', 'alt_versions_catno', 'alt_versions_year', 'alt_versions_id']\n",
    "    df_cleaned = df_cleaned[cols + alt_cols]\n",
    "    \n",
    "    # === STATS ===\n",
    "    print(f\"\\n--- SUMMARY ---\")\n",
    "    print(f\"Original releases: {original_count}\")\n",
    "    print(f\"After deduplication: {len(df_cleaned)}\")\n",
    "    print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "    print(f\"Compilations flagged: {df_cleaned['is_compilation'].sum()}\")\n",
    "    \n",
    "    has_alt = (df_cleaned['alt_versions_id'] != '').sum()\n",
    "    print(f\"Releases with alt versions stored: {has_alt}\")\n",
    "    \n",
    "    # === FORMAT BREAKDOWN ===\n",
    "    print(\"\\nFormat breakdown:\")\n",
    "    print(df_cleaned['primary_format'].value_counts().to_string())\n",
    "    \n",
    "    # === SAVE FILES ===\n",
    "    print(f\"\\nSaving files to {output_folder}...\")\n",
    "    \n",
    "    # Save full cleaned version\n",
    "    full_path = os.path.join(output_folder, \"cleaned_full.csv\")\n",
    "    df_cleaned.to_csv(full_path, index=False)\n",
    "    print(f\"  cleaned_full.csv: {len(df_cleaned)} releases\")\n",
    "    \n",
    "    # Split by format\n",
    "    formats = ['Vinyl', 'Dubplate', 'CD', 'Cassette', 'Digital', 'VHS', 'Other']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        fmt_df = df_cleaned[df_cleaned['primary_format'] == fmt]\n",
    "        if len(fmt_df) > 0:\n",
    "            fmt_path = os.path.join(output_folder, f\"{fmt.lower()}.csv\")\n",
    "            fmt_df.to_csv(fmt_path, index=False)\n",
    "            print(f\"  {fmt.lower()}.csv: {len(fmt_df)} releases\")\n",
    "    \n",
    "    print(f\"\\nDone! Original file unchanged: {input_path}\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def process_all_csvs(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv') and f.startswith('discogs')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files starting with 'discogs' found in {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        clean_discogs_csv(input_path, output_folder)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ALL FILES PROCESSED\")\n",
    "    print('='*60)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# RUN THIS CELL - EDIT PATHS BELOW\n",
    "# =====================================================\n",
    "\n",
    "INPUT_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api\"\n",
    "OUTPUT_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS\"\n",
    "\n",
    "process_all_csvs(INPUT_FOLDER, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86da56",
   "metadata": {},
   "source": [
    "I then realised that the csvs created had one issue. The alt_versions columns where combining the links without breaks which would allow for the files to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a31d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_italo_house/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_acid/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_footwork/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle_techno/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_freetekno/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_techno/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_happy_hardcore/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto_house/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bassline/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_bleep/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_acid/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_makina/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_funky/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_house/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_speed_garage/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb_samba/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_jungle/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_uk_garage/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_hardcore_jungle/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghettotech/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_juke/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_electro_funk/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_gabber/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_britcore_breakbeat_hardcore/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_britcore_breakbeat_hardcore/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_britcore_breakbeat_hardcore/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dub_jungle/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_jungle/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hiphop_breakbeat/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat_happy_hardcore/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_happy_hardcore/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dubstep/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_miami_bass/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_ghetto/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_deep_acid/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_acid_house/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_hardcore_techno_jungle/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_breakbeat/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_tribal_freetekno/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dnb/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_dj_battle_tool/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/vhs.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_baltimore_club/dubplate.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_donk/digital.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/vinyl.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/cd.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/cleaned_full.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/other.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/cassette.csv\n",
      "Fixed: /Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS/split_discogs_techno_future_jazz/digital.csv\n",
      "\n",
      "Done! Fixed 334 files\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick fix: Change comma separator to ' | ' in alt_versions columns\n",
    "Run this on your CLEANED_CSVS folder\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === EDIT THIS PATH ===\n",
    "CLEANED_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS\"\n",
    "\n",
    "def fix_separators(folder_path):\n",
    "    \"\"\"\n",
    "    Find all CSVs in folder and subfolders, change ',' to ' | ' in alt_versions columns\n",
    "    \"\"\"\n",
    "    \n",
    "    alt_columns = ['alt_versions_link', 'alt_versions_catno', 'alt_versions_year', 'alt_versions_id']\n",
    "    files_fixed = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    \n",
    "                    # Check if alt_versions columns exist\n",
    "                    if 'alt_versions_link' in df.columns:\n",
    "                        # Replace ',' with ' | ' in each alt column\n",
    "                        for col in alt_columns:\n",
    "                            if col in df.columns:\n",
    "                                df[col] = df[col].apply(lambda x: str(x).replace(',', ' | ') if pd.notna(x) and x != '' else x)\n",
    "                        \n",
    "                        # Save back\n",
    "                        df.to_csv(filepath, index=False)\n",
    "                        print(f\"Fixed: {filepath}\")\n",
    "                        files_fixed += 1\n",
    "                    else:\n",
    "                        print(f\"Skipped (no alt columns): {filepath}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {filepath}: {e}\")\n",
    "    \n",
    "    print(f\"\\nDone! Fixed {files_fixed} files\")\n",
    "\n",
    "# Run it\n",
    "fix_separators(CLEANED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: italo_house_vinyl\n",
      "============================================================\n",
      "Loaded 3881 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 6199 nodes, 73387 edges\n",
      "  Artists: 2742, Labels: 3457\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_vinyl/italo_house_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_vinyl/italo_house_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_vinyl/italo_house_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_vinyl/italo_house_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_cd\n",
      "============================================================\n",
      "Loaded 2570 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 5402 nodes, 13066 edges\n",
      "  Artists: 848, Labels: 4554\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cd/italo_house_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cd/italo_house_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cd/italo_house_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cd/italo_house_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_vhs\n",
      "============================================================\n",
      "Loaded 10 releases\n",
      "Skipping italo_house_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_other\n",
      "============================================================\n",
      "Loaded 42 releases\n",
      "Skipping italo_house_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_cassette\n",
      "============================================================\n",
      "Loaded 529 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1085 nodes, 1199 edges\n",
      "  Artists: 140, Labels: 945\n",
      "  Building 52 clusters...\n",
      "  Assigned 52 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cassette/italo_house_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cassette/italo_house_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cassette/italo_house_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_cassette/italo_house_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_digital\n",
      "============================================================\n",
      "Loaded 1326 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1817 nodes, 5535 edges\n",
      "  Artists: 1022, Labels: 795\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_digital/italo_house_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_digital/italo_house_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_digital/italo_house_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_italo_house_digital/italo_house_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: italo_house_dubplate\n",
      "============================================================\n",
      "Loaded 9 releases\n",
      "Skipping italo_house_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_vinyl\n",
      "============================================================\n",
      "Loaded 1438 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2971 nodes, 10104 edges\n",
      "  Artists: 1114, Labels: 1857\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_vinyl/breakbeat_acid_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_vinyl/breakbeat_acid_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_vinyl/breakbeat_acid_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_vinyl/breakbeat_acid_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_cd\n",
      "============================================================\n",
      "Loaded 715 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1993 nodes, 2579 edges\n",
      "  Artists: 391, Labels: 1602\n",
      "  Building 71 clusters...\n",
      "  Assigned 71 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cd/breakbeat_acid_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cd/breakbeat_acid_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cd/breakbeat_acid_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cd/breakbeat_acid_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_vhs\n",
      "============================================================\n",
      "Loaded 2 releases\n",
      "Skipping breakbeat_acid_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_other\n",
      "============================================================\n",
      "Loaded 40 releases\n",
      "Skipping breakbeat_acid_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_cassette\n",
      "============================================================\n",
      "Loaded 274 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 567 nodes, 487 edges\n",
      "  Artists: 200, Labels: 367\n",
      "  Building 27 clusters...\n",
      "  Assigned 27 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cassette/breakbeat_acid_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cassette/breakbeat_acid_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cassette/breakbeat_acid_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_cassette/breakbeat_acid_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_digital\n",
      "============================================================\n",
      "Loaded 1260 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1664 nodes, 1615 edges\n",
      "  Artists: 813, Labels: 851\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_digital/breakbeat_acid_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_digital/breakbeat_acid_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_digital/breakbeat_acid_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_acid_digital/breakbeat_acid_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_acid_dubplate\n",
      "============================================================\n",
      "Loaded 5 releases\n",
      "Skipping breakbeat_acid_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: footwork_vinyl\n",
      "============================================================\n",
      "Loaded 331 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 581 nodes, 1440 edges\n",
      "  Artists: 250, Labels: 331\n",
      "  Building 33 clusters...\n",
      "  Assigned 33 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_vinyl/footwork_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_vinyl/footwork_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_vinyl/footwork_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_vinyl/footwork_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: footwork_cd\n",
      "============================================================\n",
      "Loaded 207 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 335 nodes, 404 edges\n",
      "  Artists: 146, Labels: 189\n",
      "  Building 20 clusters...\n",
      "  Assigned 20 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cd/footwork_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cd/footwork_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cd/footwork_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cd/footwork_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: footwork_other\n",
      "============================================================\n",
      "Loaded 23 releases\n",
      "Skipping footwork_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: footwork_cassette\n",
      "============================================================\n",
      "Loaded 207 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 304 nodes, 338 edges\n",
      "  Artists: 169, Labels: 135\n",
      "  Building 20 clusters...\n",
      "  Assigned 20 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cassette/footwork_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cassette/footwork_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cassette/footwork_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_cassette/footwork_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: footwork_digital\n",
      "============================================================\n",
      "Loaded 2253 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1700 nodes, 6402 edges\n",
      "  Artists: 1034, Labels: 666\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_digital/footwork_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_digital/footwork_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_digital/footwork_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_footwork_digital/footwork_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_vinyl\n",
      "============================================================\n",
      "Loaded 6673 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 7051 nodes, 312724 edges\n",
      "  Artists: 3661, Labels: 3390\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_vinyl/breakbeat_hardcore_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_vinyl/breakbeat_hardcore_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_vinyl/breakbeat_hardcore_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_vinyl/breakbeat_hardcore_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_cd\n",
      "============================================================\n",
      "Loaded 1839 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3488 nodes, 7433 edges\n",
      "  Artists: 753, Labels: 2735\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cd/breakbeat_hardcore_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cd/breakbeat_hardcore_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cd/breakbeat_hardcore_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cd/breakbeat_hardcore_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_vhs\n",
      "============================================================\n",
      "Loaded 55 releases\n",
      "Skipping breakbeat_hardcore_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_other\n",
      "============================================================\n",
      "Loaded 142 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 219 nodes, 372 edges\n",
      "  Artists: 89, Labels: 130\n",
      "  Building 14 clusters...\n",
      "  Assigned 14 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_other/breakbeat_hardcore_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_other/breakbeat_hardcore_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_other/breakbeat_hardcore_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_other/breakbeat_hardcore_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_cassette\n",
      "============================================================\n",
      "Loaded 1602 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1456 nodes, 8429 edges\n",
      "  Artists: 693, Labels: 763\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cassette/breakbeat_hardcore_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cassette/breakbeat_hardcore_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cassette/breakbeat_hardcore_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_cassette/breakbeat_hardcore_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_digital\n",
      "============================================================\n",
      "Loaded 2888 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2588 nodes, 15655 edges\n",
      "  Artists: 1573, Labels: 1015\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_digital/breakbeat_hardcore_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_digital/breakbeat_hardcore_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_digital/breakbeat_hardcore_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_digital/breakbeat_hardcore_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_dubplate\n",
      "============================================================\n",
      "Loaded 191 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 263 nodes, 2198 edges\n",
      "  Artists: 207, Labels: 56\n",
      "  Building 19 clusters...\n",
      "  Assigned 19 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_dubplate/breakbeat_hardcore_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_dubplate/breakbeat_hardcore_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_dubplate/breakbeat_hardcore_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_dubplate/breakbeat_hardcore_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_vinyl\n",
      "============================================================\n",
      "Loaded 503 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1075 nodes, 2745 edges\n",
      "  Artists: 380, Labels: 695\n",
      "  Building 50 clusters...\n",
      "  Assigned 50 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_vinyl/jungle_techno_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_vinyl/jungle_techno_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_vinyl/jungle_techno_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_vinyl/jungle_techno_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_cd\n",
      "============================================================\n",
      "Loaded 513 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1295 nodes, 1559 edges\n",
      "  Artists: 251, Labels: 1044\n",
      "  Building 51 clusters...\n",
      "  Assigned 51 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cd/jungle_techno_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cd/jungle_techno_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cd/jungle_techno_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cd/jungle_techno_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_vhs\n",
      "============================================================\n",
      "Loaded 22 releases\n",
      "Skipping jungle_techno_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_other\n",
      "============================================================\n",
      "Loaded 31 releases\n",
      "Skipping jungle_techno_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_cassette\n",
      "============================================================\n",
      "Loaded 350 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 561 nodes, 932 edges\n",
      "  Artists: 239, Labels: 322\n",
      "  Building 35 clusters...\n",
      "  Assigned 35 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cassette/jungle_techno_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cassette/jungle_techno_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cassette/jungle_techno_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_cassette/jungle_techno_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_techno_digital\n",
      "============================================================\n",
      "Loaded 943 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1163 nodes, 1366 edges\n",
      "  Artists: 584, Labels: 579\n",
      "  Building 94 clusters...\n",
      "  Assigned 94 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_digital/jungle_techno_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_digital/jungle_techno_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_digital/jungle_techno_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_techno_digital/jungle_techno_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_vinyl\n",
      "============================================================\n",
      "Loaded 5287 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3661 nodes, 96022 edges\n",
      "  Artists: 2151, Labels: 1510\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_vinyl/freetekno_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_vinyl/freetekno_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_vinyl/freetekno_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_vinyl/freetekno_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_cd\n",
      "============================================================\n",
      "Loaded 344 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 449 nodes, 709 edges\n",
      "  Artists: 190, Labels: 259\n",
      "  Building 34 clusters...\n",
      "  Assigned 34 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cd/freetekno_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cd/freetekno_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cd/freetekno_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cd/freetekno_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_other\n",
      "============================================================\n",
      "Loaded 7 releases\n",
      "Skipping freetekno_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_cassette\n",
      "============================================================\n",
      "Loaded 379 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 336 nodes, 528 edges\n",
      "  Artists: 224, Labels: 112\n",
      "  Building 37 clusters...\n",
      "  Assigned 37 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cassette/freetekno_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cassette/freetekno_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cassette/freetekno_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_cassette/freetekno_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_digital\n",
      "============================================================\n",
      "Loaded 1895 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1116 nodes, 5295 edges\n",
      "  Artists: 660, Labels: 456\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_digital/freetekno_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_digital/freetekno_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_digital/freetekno_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_freetekno_digital/freetekno_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: freetekno_dubplate\n",
      "============================================================\n",
      "Loaded 3 releases\n",
      "Skipping freetekno_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_vinyl\n",
      "============================================================\n",
      "Loaded 1295 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2230 nodes, 17561 edges\n",
      "  Artists: 863, Labels: 1367\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_vinyl/breakbeat_hardcore_techno_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_vinyl/breakbeat_hardcore_techno_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_vinyl/breakbeat_hardcore_techno_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_vinyl/breakbeat_hardcore_techno_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_cd\n",
      "============================================================\n",
      "Loaded 662 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1682 nodes, 2513 edges\n",
      "  Artists: 234, Labels: 1448\n",
      "  Building 66 clusters...\n",
      "  Assigned 66 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cd/breakbeat_hardcore_techno_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cd/breakbeat_hardcore_techno_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cd/breakbeat_hardcore_techno_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cd/breakbeat_hardcore_techno_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_vhs\n",
      "============================================================\n",
      "Loaded 34 releases\n",
      "Skipping breakbeat_hardcore_techno_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_other\n",
      "============================================================\n",
      "Loaded 20 releases\n",
      "Skipping breakbeat_hardcore_techno_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_cassette\n",
      "============================================================\n",
      "Loaded 402 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 565 nodes, 1155 edges\n",
      "  Artists: 201, Labels: 364\n",
      "  Building 40 clusters...\n",
      "  Assigned 40 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cassette/breakbeat_hardcore_techno_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cassette/breakbeat_hardcore_techno_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cassette/breakbeat_hardcore_techno_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_cassette/breakbeat_hardcore_techno_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_digital\n",
      "============================================================\n",
      "Loaded 377 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 499 nodes, 479 edges\n",
      "  Artists: 237, Labels: 262\n",
      "  Building 37 clusters...\n",
      "  Assigned 37 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_digital/breakbeat_hardcore_techno_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_digital/breakbeat_hardcore_techno_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_digital/breakbeat_hardcore_techno_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_techno_digital/breakbeat_hardcore_techno_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_techno_dubplate\n",
      "============================================================\n",
      "Loaded 9 releases\n",
      "Skipping breakbeat_hardcore_techno_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_vinyl\n",
      "============================================================\n",
      "Loaded 356 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 601 nodes, 3468 edges\n",
      "  Artists: 309, Labels: 292\n",
      "  Building 35 clusters...\n",
      "  Assigned 35 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_vinyl/breakbeat_hardcore_happy_hardcore_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_vinyl/breakbeat_hardcore_happy_hardcore_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_vinyl/breakbeat_hardcore_happy_hardcore_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_vinyl/breakbeat_hardcore_happy_hardcore_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_cd\n",
      "============================================================\n",
      "Loaded 452 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 872 nodes, 1249 edges\n",
      "  Artists: 155, Labels: 717\n",
      "  Building 45 clusters...\n",
      "  Assigned 45 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cd/breakbeat_hardcore_happy_hardcore_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cd/breakbeat_hardcore_happy_hardcore_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cd/breakbeat_hardcore_happy_hardcore_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cd/breakbeat_hardcore_happy_hardcore_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_vhs\n",
      "============================================================\n",
      "Loaded 23 releases\n",
      "Skipping breakbeat_hardcore_happy_hardcore_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_other\n",
      "============================================================\n",
      "Loaded 28 releases\n",
      "Skipping breakbeat_hardcore_happy_hardcore_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_cassette\n",
      "============================================================\n",
      "Loaded 258 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 220 nodes, 551 edges\n",
      "  Artists: 100, Labels: 120\n",
      "  Building 25 clusters...\n",
      "  Assigned 25 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cassette/breakbeat_hardcore_happy_hardcore_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cassette/breakbeat_hardcore_happy_hardcore_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cassette/breakbeat_hardcore_happy_hardcore_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_cassette/breakbeat_hardcore_happy_hardcore_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_digital\n",
      "============================================================\n",
      "Loaded 480 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 496 nodes, 1606 edges\n",
      "  Artists: 281, Labels: 215\n",
      "  Building 48 clusters...\n",
      "  Assigned 48 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_digital/breakbeat_hardcore_happy_hardcore_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_digital/breakbeat_hardcore_happy_hardcore_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_digital/breakbeat_hardcore_happy_hardcore_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_happy_hardcore_digital/breakbeat_hardcore_happy_hardcore_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_happy_hardcore_dubplate\n",
      "============================================================\n",
      "Loaded 10 releases\n",
      "Skipping breakbeat_hardcore_happy_hardcore_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_vinyl\n",
      "============================================================\n",
      "Loaded 952 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1433 nodes, 12952 edges\n",
      "  Artists: 639, Labels: 794\n",
      "  Building 95 clusters...\n",
      "  Assigned 95 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_vinyl/ghetto_house_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_vinyl/ghetto_house_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_vinyl/ghetto_house_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_vinyl/ghetto_house_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_cd\n",
      "============================================================\n",
      "Loaded 358 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1005 nodes, 1621 edges\n",
      "  Artists: 249, Labels: 756\n",
      "  Building 35 clusters...\n",
      "  Assigned 35 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cd/ghetto_house_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cd/ghetto_house_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cd/ghetto_house_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cd/ghetto_house_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping ghetto_house_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_other\n",
      "============================================================\n",
      "Loaded 17 releases\n",
      "Skipping ghetto_house_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_cassette\n",
      "============================================================\n",
      "Loaded 193 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 308 nodes, 310 edges\n",
      "  Artists: 157, Labels: 151\n",
      "  Building 19 clusters...\n",
      "  Assigned 19 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cassette/ghetto_house_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cassette/ghetto_house_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cassette/ghetto_house_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_cassette/ghetto_house_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_digital\n",
      "============================================================\n",
      "Loaded 1289 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1631 nodes, 3263 edges\n",
      "  Artists: 901, Labels: 730\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_digital/ghetto_house_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_digital/ghetto_house_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_digital/ghetto_house_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_house_digital/ghetto_house_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_house_dubplate\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping ghetto_house_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_vinyl\n",
      "============================================================\n",
      "Loaded 904 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1624 nodes, 3544 edges\n",
      "  Artists: 817, Labels: 807\n",
      "  Building 90 clusters...\n",
      "  Assigned 90 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_vinyl/bassline_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_vinyl/bassline_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_vinyl/bassline_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_vinyl/bassline_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_cd\n",
      "============================================================\n",
      "Loaded 697 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1236 nodes, 3638 edges\n",
      "  Artists: 506, Labels: 730\n",
      "  Building 69 clusters...\n",
      "  Assigned 69 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cd/bassline_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cd/bassline_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cd/bassline_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cd/bassline_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping bassline_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_other\n",
      "============================================================\n",
      "Loaded 101 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 156 nodes, 142 edges\n",
      "  Artists: 87, Labels: 69\n",
      "  Building 10 clusters...\n",
      "  Assigned 10 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_other/bassline_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_other/bassline_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_other/bassline_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_other/bassline_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_cassette\n",
      "============================================================\n",
      "Loaded 116 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 216 nodes, 181 edges\n",
      "  Artists: 102, Labels: 114\n",
      "  Building 11 clusters...\n",
      "  Assigned 11 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cassette/bassline_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cassette/bassline_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cassette/bassline_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_cassette/bassline_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_digital\n",
      "============================================================\n",
      "Loaded 3471 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3988 nodes, 20154 edges\n",
      "  Artists: 2553, Labels: 1435\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_digital/bassline_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_digital/bassline_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_digital/bassline_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bassline_digital/bassline_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bassline_dubplate\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping bassline_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_vinyl\n",
      "============================================================\n",
      "Loaded 948 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1887 nodes, 10017 edges\n",
      "  Artists: 671, Labels: 1216\n",
      "  Building 94 clusters...\n",
      "  Assigned 94 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_vinyl/bleep_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_vinyl/bleep_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_vinyl/bleep_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_vinyl/bleep_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_cd\n",
      "============================================================\n",
      "Loaded 122 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 506 nodes, 707 edges\n",
      "  Artists: 84, Labels: 422\n",
      "  Building 12 clusters...\n",
      "  Assigned 12 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_cd/bleep_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_cd/bleep_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_cd/bleep_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_cd/bleep_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping bleep_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_other\n",
      "============================================================\n",
      "Loaded 8 releases\n",
      "Skipping bleep_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_cassette\n",
      "============================================================\n",
      "Loaded 64 releases\n",
      "Skipping bleep_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_digital\n",
      "============================================================\n",
      "Loaded 398 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 534 nodes, 635 edges\n",
      "  Artists: 302, Labels: 232\n",
      "  Building 39 clusters...\n",
      "  Assigned 39 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_digital/bleep_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_digital/bleep_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_digital/bleep_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_bleep_digital/bleep_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: bleep_dubplate\n",
      "============================================================\n",
      "Loaded 3 releases\n",
      "Skipping bleep_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_vinyl\n",
      "============================================================\n",
      "Loaded 1062 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1958 nodes, 7367 edges\n",
      "  Artists: 791, Labels: 1167\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_vinyl/hardcore_acid_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_vinyl/hardcore_acid_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_vinyl/hardcore_acid_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_vinyl/hardcore_acid_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_cd\n",
      "============================================================\n",
      "Loaded 483 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1429 nodes, 2184 edges\n",
      "  Artists: 192, Labels: 1237\n",
      "  Building 48 clusters...\n",
      "  Assigned 48 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cd/hardcore_acid_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cd/hardcore_acid_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cd/hardcore_acid_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cd/hardcore_acid_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_vhs\n",
      "============================================================\n",
      "Loaded 6 releases\n",
      "Skipping hardcore_acid_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_other\n",
      "============================================================\n",
      "Loaded 4 releases\n",
      "Skipping hardcore_acid_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_cassette\n",
      "============================================================\n",
      "Loaded 212 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 368 nodes, 354 edges\n",
      "  Artists: 104, Labels: 264\n",
      "  Building 21 clusters...\n",
      "  Assigned 21 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cassette/hardcore_acid_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cassette/hardcore_acid_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cassette/hardcore_acid_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_cassette/hardcore_acid_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_digital\n",
      "============================================================\n",
      "Loaded 214 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 288 nodes, 313 edges\n",
      "  Artists: 157, Labels: 131\n",
      "  Building 21 clusters...\n",
      "  Assigned 21 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_digital/hardcore_acid_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_digital/hardcore_acid_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_digital/hardcore_acid_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_acid_digital/hardcore_acid_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_acid_dubplate\n",
      "============================================================\n",
      "Loaded 6 releases\n",
      "Skipping hardcore_acid_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: makina_vinyl\n",
      "============================================================\n",
      "Loaded 3585 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3698 nodes, 479480 edges\n",
      "  Artists: 2670, Labels: 1028\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_vinyl/makina_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_vinyl/makina_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_vinyl/makina_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_vinyl/makina_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: makina_cd\n",
      "============================================================\n",
      "Loaded 2051 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1814 nodes, 5796 edges\n",
      "  Artists: 454, Labels: 1360\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cd/makina_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cd/makina_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cd/makina_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cd/makina_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: makina_vhs\n",
      "============================================================\n",
      "Loaded 13 releases\n",
      "Skipping makina_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: makina_other\n",
      "============================================================\n",
      "Loaded 22 releases\n",
      "Skipping makina_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: makina_cassette\n",
      "============================================================\n",
      "Loaded 553 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 534 nodes, 1736 edges\n",
      "  Artists: 111, Labels: 423\n",
      "  Building 55 clusters...\n",
      "  Assigned 55 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cassette/makina_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cassette/makina_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cassette/makina_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_cassette/makina_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: makina_digital\n",
      "============================================================\n",
      "Loaded 2827 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1846 nodes, 70080 edges\n",
      "  Artists: 1412, Labels: 434\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_digital/makina_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_digital/makina_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_digital/makina_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_makina_digital/makina_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_funky_vinyl\n",
      "============================================================\n",
      "Loaded 409 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 658 nodes, 1077 edges\n",
      "  Artists: 315, Labels: 343\n",
      "  Building 40 clusters...\n",
      "  Assigned 40 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_vinyl/uk_funky_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_vinyl/uk_funky_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_vinyl/uk_funky_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_vinyl/uk_funky_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_funky_cd\n",
      "============================================================\n",
      "Loaded 119 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 301 nodes, 349 edges\n",
      "  Artists: 100, Labels: 201\n",
      "  Building 11 clusters...\n",
      "  Assigned 11 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_cd/uk_funky_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_cd/uk_funky_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_cd/uk_funky_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_cd/uk_funky_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_funky_other\n",
      "============================================================\n",
      "Loaded 15 releases\n",
      "Skipping uk_funky_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: uk_funky_cassette\n",
      "============================================================\n",
      "Loaded 19 releases\n",
      "Skipping uk_funky_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: uk_funky_digital\n",
      "============================================================\n",
      "Loaded 558 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 672 nodes, 924 edges\n",
      "  Artists: 388, Labels: 284\n",
      "  Building 55 clusters...\n",
      "  Assigned 55 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_digital/uk_funky_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_digital/uk_funky_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_digital/uk_funky_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_funky_digital/uk_funky_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_vinyl\n",
      "============================================================\n",
      "Loaded 4430 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8694 nodes, 57821 edges\n",
      "  Artists: 3391, Labels: 5303\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_vinyl/breakbeat_house_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_vinyl/breakbeat_house_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_vinyl/breakbeat_house_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_vinyl/breakbeat_house_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_cd\n",
      "============================================================\n",
      "Loaded 3857 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 9835 nodes, 28442 edges\n",
      "  Artists: 1583, Labels: 8252\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cd/breakbeat_house_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cd/breakbeat_house_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cd/breakbeat_house_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cd/breakbeat_house_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_vhs\n",
      "============================================================\n",
      "Loaded 36 releases\n",
      "Skipping breakbeat_house_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_other\n",
      "============================================================\n",
      "Loaded 202 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 304 nodes, 346 edges\n",
      "  Artists: 71, Labels: 233\n",
      "  Building 20 clusters...\n",
      "  Assigned 20 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_other/breakbeat_house_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_other/breakbeat_house_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_other/breakbeat_house_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_other/breakbeat_house_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_cassette\n",
      "============================================================\n",
      "Loaded 696 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1997 nodes, 2225 edges\n",
      "  Artists: 349, Labels: 1648\n",
      "  Building 69 clusters...\n",
      "  Assigned 69 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cassette/breakbeat_house_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cassette/breakbeat_house_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cassette/breakbeat_house_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_cassette/breakbeat_house_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_digital\n",
      "============================================================\n",
      "Loaded 2174 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3207 nodes, 3783 edges\n",
      "  Artists: 1643, Labels: 1564\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_digital/breakbeat_house_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_digital/breakbeat_house_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_digital/breakbeat_house_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_house_digital/breakbeat_house_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_house_dubplate\n",
      "============================================================\n",
      "Loaded 21 releases\n",
      "Skipping breakbeat_house_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_vinyl\n",
      "============================================================\n",
      "Loaded 2648 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3936 nodes, 23457 edges\n",
      "  Artists: 1944, Labels: 1992\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_vinyl/speed_garage_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_vinyl/speed_garage_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_vinyl/speed_garage_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_vinyl/speed_garage_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_cd\n",
      "============================================================\n",
      "Loaded 1200 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 4322 nodes, 12371 edges\n",
      "  Artists: 663, Labels: 3659\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cd/speed_garage_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cd/speed_garage_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cd/speed_garage_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cd/speed_garage_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_vhs\n",
      "============================================================\n",
      "Loaded 3 releases\n",
      "Skipping speed_garage_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_other\n",
      "============================================================\n",
      "Loaded 145 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 168 nodes, 927 edges\n",
      "  Artists: 117, Labels: 51\n",
      "  Building 14 clusters...\n",
      "  Assigned 14 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_other/speed_garage_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_other/speed_garage_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_other/speed_garage_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_other/speed_garage_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_cassette\n",
      "============================================================\n",
      "Loaded 191 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 810 nodes, 1156 edges\n",
      "  Artists: 113, Labels: 697\n",
      "  Building 19 clusters...\n",
      "  Assigned 19 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cassette/speed_garage_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cassette/speed_garage_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cassette/speed_garage_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_cassette/speed_garage_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_digital\n",
      "============================================================\n",
      "Loaded 974 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1157 nodes, 3397 edges\n",
      "  Artists: 667, Labels: 490\n",
      "  Building 97 clusters...\n",
      "  Assigned 97 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_digital/speed_garage_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_digital/speed_garage_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_digital/speed_garage_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_speed_garage_digital/speed_garage_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: speed_garage_dubplate\n",
      "============================================================\n",
      "Loaded 42 releases\n",
      "Skipping speed_garage_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_samba_vinyl\n",
      "============================================================\n",
      "Loaded 12 releases\n",
      "Skipping dnb_samba_vinyl - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_samba_cd\n",
      "============================================================\n",
      "Loaded 45 releases\n",
      "Skipping dnb_samba_cd - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_samba_other\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping dnb_samba_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_samba_cassette\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping dnb_samba_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_samba_digital\n",
      "============================================================\n",
      "Loaded 13 releases\n",
      "Skipping dnb_samba_digital - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_vinyl\n",
      "============================================================\n",
      "Loaded 11786 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 12191 nodes, 503484 edges\n",
      "  Artists: 6566, Labels: 5625\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_vinyl/jungle_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_vinyl/jungle_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_vinyl/jungle_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_vinyl/jungle_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_cd\n",
      "============================================================\n",
      "Loaded 4358 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8072 nodes, 23927 edges\n",
      "  Artists: 2254, Labels: 5818\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cd/jungle_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cd/jungle_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cd/jungle_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cd/jungle_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_vhs\n",
      "============================================================\n",
      "Loaded 57 releases\n",
      "Skipping jungle_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_other\n",
      "============================================================\n",
      "Loaded 447 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 613 nodes, 1191 edges\n",
      "  Artists: 333, Labels: 280\n",
      "  Building 44 clusters...\n",
      "  Assigned 44 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_other/jungle_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_other/jungle_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_other/jungle_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_other/jungle_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_cassette\n",
      "============================================================\n",
      "Loaded 3273 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3038 nodes, 12082 edges\n",
      "  Artists: 1467, Labels: 1571\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cassette/jungle_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cassette/jungle_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cassette/jungle_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_cassette/jungle_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_digital\n",
      "============================================================\n",
      "Loaded 10427 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8856 nodes, 40247 edges\n",
      "  Artists: 5425, Labels: 3431\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_digital/jungle_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_digital/jungle_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_digital/jungle_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_digital/jungle_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: jungle_dubplate\n",
      "============================================================\n",
      "Loaded 749 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 826 nodes, 125261 edges\n",
      "  Artists: 750, Labels: 76\n",
      "  Building 74 clusters...\n",
      "  Assigned 74 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_dubplate/jungle_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_dubplate/jungle_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_dubplate/jungle_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_jungle_dubplate/jungle_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_vinyl\n",
      "============================================================\n",
      "Loaded 49650 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 66284 nodes, 3915388 edges\n",
      "  Artists: 30298, Labels: 35986\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vinyl/electro_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vinyl/electro_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vinyl/electro_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vinyl/electro_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_cd\n",
      "============================================================\n",
      "Loaded 49571 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 67335 nodes, 2190033 edges\n",
      "  Artists: 25082, Labels: 42253\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cd/electro_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cd/electro_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cd/electro_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cd/electro_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_vhs\n",
      "============================================================\n",
      "Loaded 346 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 614 nodes, 789 edges\n",
      "  Artists: 144, Labels: 470\n",
      "  Building 34 clusters...\n",
      "  Assigned 34 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vhs/electro_vhs_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vhs/electro_vhs_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vhs/electro_vhs_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_vhs/electro_vhs_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_other\n",
      "============================================================\n",
      "Loaded 1852 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3076 nodes, 6071 edges\n",
      "  Artists: 1226, Labels: 1850\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_other/electro_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_other/electro_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_other/electro_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_other/electro_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_cassette\n",
      "============================================================\n",
      "Loaded 7919 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 12602 nodes, 53561 edges\n",
      "  Artists: 4743, Labels: 7859\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cassette/electro_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cassette/electro_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cassette/electro_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_cassette/electro_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_digital\n",
      "============================================================\n",
      "Loaded 77261 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 62459 nodes, 3654532 edges\n",
      "  Artists: 43098, Labels: 19361\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_digital/electro_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_digital/electro_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_digital/electro_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_digital/electro_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_dubplate\n",
      "============================================================\n",
      "Loaded 251 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 449 nodes, 1277 edges\n",
      "  Artists: 236, Labels: 213\n",
      "  Building 25 clusters...\n",
      "  Assigned 25 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_dubplate/electro_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_dubplate/electro_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_dubplate/electro_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_dubplate/electro_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_vinyl\n",
      "============================================================\n",
      "Loaded 12019 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 14145 nodes, 499223 edges\n",
      "  Artists: 7893, Labels: 6252\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_vinyl/uk_garage_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_vinyl/uk_garage_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_vinyl/uk_garage_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_vinyl/uk_garage_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_cd\n",
      "============================================================\n",
      "Loaded 4851 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 9550 nodes, 75432 edges\n",
      "  Artists: 2321, Labels: 7229\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cd/uk_garage_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cd/uk_garage_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cd/uk_garage_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cd/uk_garage_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_vhs\n",
      "============================================================\n",
      "Loaded 22 releases\n",
      "Skipping uk_garage_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_other\n",
      "============================================================\n",
      "Loaded 807 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1022 nodes, 5896 edges\n",
      "  Artists: 458, Labels: 564\n",
      "  Building 80 clusters...\n",
      "  Assigned 80 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_other/uk_garage_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_other/uk_garage_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_other/uk_garage_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_other/uk_garage_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_cassette\n",
      "============================================================\n",
      "Loaded 611 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1461 nodes, 2602 edges\n",
      "  Artists: 378, Labels: 1083\n",
      "  Building 61 clusters...\n",
      "  Assigned 61 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cassette/uk_garage_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cassette/uk_garage_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cassette/uk_garage_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_cassette/uk_garage_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_digital\n",
      "============================================================\n",
      "Loaded 8415 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8056 nodes, 43468 edges\n",
      "  Artists: 5083, Labels: 2973\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_digital/uk_garage_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_digital/uk_garage_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_digital/uk_garage_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_digital/uk_garage_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: uk_garage_dubplate\n",
      "============================================================\n",
      "Loaded 157 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 242 nodes, 1370 edges\n",
      "  Artists: 163, Labels: 79\n",
      "  Building 15 clusters...\n",
      "  Assigned 15 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_dubplate/uk_garage_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_dubplate/uk_garage_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_dubplate/uk_garage_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_uk_garage_dubplate/uk_garage_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_vinyl\n",
      "============================================================\n",
      "Loaded 13434 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 19890 nodes, 366200 edges\n",
      "  Artists: 9248, Labels: 10642\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_vinyl/tribal_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_vinyl/tribal_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_vinyl/tribal_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_vinyl/tribal_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_cd\n",
      "============================================================\n",
      "Loaded 12088 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 25699 nodes, 257298 edges\n",
      "  Artists: 6302, Labels: 19397\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cd/tribal_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cd/tribal_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cd/tribal_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cd/tribal_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_vhs\n",
      "============================================================\n",
      "Loaded 33 releases\n",
      "Skipping tribal_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_other\n",
      "============================================================\n",
      "Loaded 232 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 522 nodes, 649 edges\n",
      "  Artists: 166, Labels: 356\n",
      "  Building 23 clusters...\n",
      "  Assigned 23 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_other/tribal_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_other/tribal_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_other/tribal_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_other/tribal_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_cassette\n",
      "============================================================\n",
      "Loaded 2083 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 4207 nodes, 6126 edges\n",
      "  Artists: 1423, Labels: 2784\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cassette/tribal_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cassette/tribal_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cassette/tribal_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_cassette/tribal_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_digital\n",
      "============================================================\n",
      "Loaded 10603 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 12030 nodes, 57133 edges\n",
      "  Artists: 7103, Labels: 4927\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_digital/tribal_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_digital/tribal_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_digital/tribal_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_digital/tribal_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_dubplate\n",
      "============================================================\n",
      "Loaded 46 releases\n",
      "Skipping tribal_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_vinyl\n",
      "============================================================\n",
      "Loaded 1634 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1923 nodes, 40411 edges\n",
      "  Artists: 1010, Labels: 913\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_vinyl/breakbeat_hardcore_jungle_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_vinyl/breakbeat_hardcore_jungle_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_vinyl/breakbeat_hardcore_jungle_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_vinyl/breakbeat_hardcore_jungle_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_cd\n",
      "============================================================\n",
      "Loaded 334 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 686 nodes, 891 edges\n",
      "  Artists: 134, Labels: 552\n",
      "  Building 33 clusters...\n",
      "  Assigned 33 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cd/breakbeat_hardcore_jungle_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cd/breakbeat_hardcore_jungle_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cd/breakbeat_hardcore_jungle_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cd/breakbeat_hardcore_jungle_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_vhs\n",
      "============================================================\n",
      "Loaded 19 releases\n",
      "Skipping breakbeat_hardcore_jungle_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_other\n",
      "============================================================\n",
      "Loaded 58 releases\n",
      "Skipping breakbeat_hardcore_jungle_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_cassette\n",
      "============================================================\n",
      "Loaded 491 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 455 nodes, 1774 edges\n",
      "  Artists: 236, Labels: 219\n",
      "  Building 49 clusters...\n",
      "  Assigned 49 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cassette/breakbeat_hardcore_jungle_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cassette/breakbeat_hardcore_jungle_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cassette/breakbeat_hardcore_jungle_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_cassette/breakbeat_hardcore_jungle_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_digital\n",
      "============================================================\n",
      "Loaded 606 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 710 nodes, 1810 edges\n",
      "  Artists: 381, Labels: 329\n",
      "  Building 60 clusters...\n",
      "  Assigned 60 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_digital/breakbeat_hardcore_jungle_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_digital/breakbeat_hardcore_jungle_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_digital/breakbeat_hardcore_jungle_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_hardcore_jungle_digital/breakbeat_hardcore_jungle_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_hardcore_jungle_dubplate\n",
      "============================================================\n",
      "Loaded 94 releases\n",
      "Skipping breakbeat_hardcore_jungle_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_vinyl\n",
      "============================================================\n",
      "Loaded 365 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 570 nodes, 1332 edges\n",
      "  Artists: 227, Labels: 343\n",
      "  Building 36 clusters...\n",
      "  Assigned 36 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_vinyl/ghettotech_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_vinyl/ghettotech_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_vinyl/ghettotech_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_vinyl/ghettotech_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_cd\n",
      "============================================================\n",
      "Loaded 161 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 338 nodes, 338 edges\n",
      "  Artists: 109, Labels: 229\n",
      "  Building 16 clusters...\n",
      "  Assigned 16 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cd/ghettotech_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cd/ghettotech_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cd/ghettotech_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cd/ghettotech_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping ghettotech_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_other\n",
      "============================================================\n",
      "Loaded 12 releases\n",
      "Skipping ghettotech_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_cassette\n",
      "============================================================\n",
      "Loaded 111 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 177 nodes, 153 edges\n",
      "  Artists: 104, Labels: 73\n",
      "  Building 11 clusters...\n",
      "  Assigned 11 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cassette/ghettotech_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cassette/ghettotech_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cassette/ghettotech_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_cassette/ghettotech_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghettotech_digital\n",
      "============================================================\n",
      "Loaded 761 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 860 nodes, 1285 edges\n",
      "  Artists: 471, Labels: 389\n",
      "  Building 76 clusters...\n",
      "  Assigned 76 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_digital/ghettotech_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_digital/ghettotech_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_digital/ghettotech_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghettotech_digital/ghettotech_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: juke_vinyl\n",
      "============================================================\n",
      "Loaded 562 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 847 nodes, 3067 edges\n",
      "  Artists: 381, Labels: 466\n",
      "  Building 56 clusters...\n",
      "  Assigned 56 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_vinyl/juke_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_vinyl/juke_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_vinyl/juke_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_vinyl/juke_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: juke_cd\n",
      "============================================================\n",
      "Loaded 434 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 692 nodes, 1018 edges\n",
      "  Artists: 286, Labels: 406\n",
      "  Building 43 clusters...\n",
      "  Assigned 43 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cd/juke_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cd/juke_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cd/juke_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cd/juke_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: juke_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping juke_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: juke_other\n",
      "============================================================\n",
      "Loaded 32 releases\n",
      "Skipping juke_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: juke_cassette\n",
      "============================================================\n",
      "Loaded 286 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 396 nodes, 506 edges\n",
      "  Artists: 226, Labels: 170\n",
      "  Building 28 clusters...\n",
      "  Assigned 28 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cassette/juke_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cassette/juke_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cassette/juke_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_cassette/juke_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: juke_digital\n",
      "============================================================\n",
      "Loaded 2859 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2449 nodes, 10329 edges\n",
      "  Artists: 1488, Labels: 961\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_digital/juke_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_digital/juke_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_digital/juke_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_juke_digital/juke_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_vinyl\n",
      "============================================================\n",
      "Loaded 2097 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 5483 nodes, 51687 edges\n",
      "  Artists: 1454, Labels: 4029\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_vinyl/electro_funk_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_vinyl/electro_funk_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_vinyl/electro_funk_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_vinyl/electro_funk_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_cd\n",
      "============================================================\n",
      "Loaded 887 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2813 nodes, 5461 edges\n",
      "  Artists: 414, Labels: 2399\n",
      "  Building 88 clusters...\n",
      "  Assigned 88 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cd/electro_funk_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cd/electro_funk_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cd/electro_funk_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cd/electro_funk_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_vhs\n",
      "============================================================\n",
      "Loaded 15 releases\n",
      "Skipping electro_funk_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_other\n",
      "============================================================\n",
      "Loaded 28 releases\n",
      "Skipping electro_funk_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_cassette\n",
      "============================================================\n",
      "Loaded 350 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 926 nodes, 1791 edges\n",
      "  Artists: 195, Labels: 731\n",
      "  Building 35 clusters...\n",
      "  Assigned 35 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cassette/electro_funk_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cassette/electro_funk_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cassette/electro_funk_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_cassette/electro_funk_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_digital\n",
      "============================================================\n",
      "Loaded 185 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 340 nodes, 250 edges\n",
      "  Artists: 167, Labels: 173\n",
      "  Building 18 clusters...\n",
      "  Assigned 18 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_digital/electro_funk_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_digital/electro_funk_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_digital/electro_funk_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_electro_funk_digital/electro_funk_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: electro_funk_dubplate\n",
      "============================================================\n",
      "Loaded 40 releases\n",
      "Skipping electro_funk_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_vinyl\n",
      "============================================================\n",
      "Loaded 4782 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 5036 nodes, 188635 edges\n",
      "  Artists: 2623, Labels: 2413\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vinyl/gabber_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vinyl/gabber_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vinyl/gabber_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vinyl/gabber_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_cd\n",
      "============================================================\n",
      "Loaded 5475 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 6076 nodes, 29662 edges\n",
      "  Artists: 1814, Labels: 4262\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cd/gabber_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cd/gabber_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cd/gabber_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cd/gabber_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_vhs\n",
      "============================================================\n",
      "Loaded 115 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 95 nodes, 95 edges\n",
      "  Artists: 8, Labels: 87\n",
      "  Building 11 clusters...\n",
      "  Assigned 11 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vhs/gabber_vhs_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vhs/gabber_vhs_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vhs/gabber_vhs_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_vhs/gabber_vhs_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_other\n",
      "============================================================\n",
      "Loaded 172 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 219 nodes, 209 edges\n",
      "  Artists: 71, Labels: 148\n",
      "  Building 17 clusters...\n",
      "  Assigned 17 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_other/gabber_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_other/gabber_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_other/gabber_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_other/gabber_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_cassette\n",
      "============================================================\n",
      "Loaded 1805 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1848 nodes, 4332 edges\n",
      "  Artists: 797, Labels: 1051\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cassette/gabber_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cassette/gabber_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cassette/gabber_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_cassette/gabber_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_digital\n",
      "============================================================\n",
      "Loaded 8349 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 5861 nodes, 51563 edges\n",
      "  Artists: 3851, Labels: 2010\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_digital/gabber_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_digital/gabber_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_digital/gabber_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_gabber_digital/gabber_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: gabber_dubplate\n",
      "============================================================\n",
      "Loaded 7 releases\n",
      "Skipping gabber_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: britcore_breakbeat_hardcore_vinyl\n",
      "============================================================\n",
      "Loaded 4 releases\n",
      "Skipping britcore_breakbeat_hardcore_vinyl - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: britcore_breakbeat_hardcore_digital\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping britcore_breakbeat_hardcore_digital - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dub_jungle_vinyl\n",
      "============================================================\n",
      "Loaded 284 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 654 nodes, 1144 edges\n",
      "  Artists: 259, Labels: 395\n",
      "  Building 28 clusters...\n",
      "  Assigned 28 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_vinyl/dub_jungle_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_vinyl/dub_jungle_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_vinyl/dub_jungle_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_vinyl/dub_jungle_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dub_jungle_cd\n",
      "============================================================\n",
      "Loaded 336 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 979 nodes, 1266 edges\n",
      "  Artists: 207, Labels: 772\n",
      "  Building 33 clusters...\n",
      "  Assigned 33 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_cd/dub_jungle_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_cd/dub_jungle_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_cd/dub_jungle_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_cd/dub_jungle_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dub_jungle_other\n",
      "============================================================\n",
      "Loaded 7 releases\n",
      "Skipping dub_jungle_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dub_jungle_cassette\n",
      "============================================================\n",
      "Loaded 79 releases\n",
      "Skipping dub_jungle_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dub_jungle_digital\n",
      "============================================================\n",
      "Loaded 300 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 497 nodes, 465 edges\n",
      "  Artists: 267, Labels: 230\n",
      "  Building 30 clusters...\n",
      "  Assigned 30 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_digital/dub_jungle_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_digital/dub_jungle_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_digital/dub_jungle_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dub_jungle_digital/dub_jungle_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_vinyl\n",
      "============================================================\n",
      "Loaded 2281 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2530 nodes, 54768 edges\n",
      "  Artists: 1316, Labels: 1214\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_vinyl/hardcore_jungle_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_vinyl/hardcore_jungle_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_vinyl/hardcore_jungle_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_vinyl/hardcore_jungle_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_cd\n",
      "============================================================\n",
      "Loaded 645 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1209 nodes, 1653 edges\n",
      "  Artists: 279, Labels: 930\n",
      "  Building 64 clusters...\n",
      "  Assigned 64 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cd/hardcore_jungle_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cd/hardcore_jungle_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cd/hardcore_jungle_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cd/hardcore_jungle_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_vhs\n",
      "============================================================\n",
      "Loaded 30 releases\n",
      "Skipping hardcore_jungle_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_other\n",
      "============================================================\n",
      "Loaded 94 releases\n",
      "Skipping hardcore_jungle_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_cassette\n",
      "============================================================\n",
      "Loaded 1009 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 787 nodes, 4568 edges\n",
      "  Artists: 424, Labels: 363\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cassette/hardcore_jungle_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cassette/hardcore_jungle_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cassette/hardcore_jungle_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_cassette/hardcore_jungle_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_digital\n",
      "============================================================\n",
      "Loaded 1381 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1419 nodes, 3614 edges\n",
      "  Artists: 803, Labels: 616\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_digital/hardcore_jungle_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_digital/hardcore_jungle_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_digital/hardcore_jungle_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_digital/hardcore_jungle_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_jungle_dubplate\n",
      "============================================================\n",
      "Loaded 121 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 144 nodes, 1149 edges\n",
      "  Artists: 128, Labels: 16\n",
      "  Building 12 clusters...\n",
      "  Assigned 12 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_dubplate/hardcore_jungle_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_dubplate/hardcore_jungle_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_dubplate/hardcore_jungle_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_jungle_dubplate/hardcore_jungle_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_vinyl\n",
      "============================================================\n",
      "Loaded 590 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1683 nodes, 2650 edges\n",
      "  Artists: 490, Labels: 1193\n",
      "  Building 59 clusters...\n",
      "  Assigned 59 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_vinyl/hiphop_breakbeat_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_vinyl/hiphop_breakbeat_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_vinyl/hiphop_breakbeat_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_vinyl/hiphop_breakbeat_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_cd\n",
      "============================================================\n",
      "Loaded 1110 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2991 nodes, 4321 edges\n",
      "  Artists: 593, Labels: 2398\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cd/hiphop_breakbeat_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cd/hiphop_breakbeat_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cd/hiphop_breakbeat_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cd/hiphop_breakbeat_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_vhs\n",
      "============================================================\n",
      "Loaded 13 releases\n",
      "Skipping hiphop_breakbeat_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_other\n",
      "============================================================\n",
      "Loaded 34 releases\n",
      "Skipping hiphop_breakbeat_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_cassette\n",
      "============================================================\n",
      "Loaded 152 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 489 nodes, 622 edges\n",
      "  Artists: 110, Labels: 379\n",
      "  Building 15 clusters...\n",
      "  Assigned 15 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cassette/hiphop_breakbeat_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cassette/hiphop_breakbeat_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cassette/hiphop_breakbeat_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_cassette/hiphop_breakbeat_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_digital\n",
      "============================================================\n",
      "Loaded 896 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1130 nodes, 1324 edges\n",
      "  Artists: 611, Labels: 519\n",
      "  Building 89 clusters...\n",
      "  Assigned 89 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_digital/hiphop_breakbeat_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_digital/hiphop_breakbeat_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_digital/hiphop_breakbeat_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hiphop_breakbeat_digital/hiphop_breakbeat_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hiphop_breakbeat_dubplate\n",
      "============================================================\n",
      "Loaded 2 releases\n",
      "Skipping hiphop_breakbeat_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_vinyl\n",
      "============================================================\n",
      "Loaded 881 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1126 nodes, 12583 edges\n",
      "  Artists: 616, Labels: 510\n",
      "  Building 88 clusters...\n",
      "  Assigned 88 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_vinyl/breakbeat_happy_hardcore_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_vinyl/breakbeat_happy_hardcore_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_vinyl/breakbeat_happy_hardcore_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_vinyl/breakbeat_happy_hardcore_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_cd\n",
      "============================================================\n",
      "Loaded 802 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1615 nodes, 3172 edges\n",
      "  Artists: 318, Labels: 1297\n",
      "  Building 80 clusters...\n",
      "  Assigned 80 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cd/breakbeat_happy_hardcore_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cd/breakbeat_happy_hardcore_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cd/breakbeat_happy_hardcore_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cd/breakbeat_happy_hardcore_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_vhs\n",
      "============================================================\n",
      "Loaded 19 releases\n",
      "Skipping breakbeat_happy_hardcore_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_other\n",
      "============================================================\n",
      "Loaded 32 releases\n",
      "Skipping breakbeat_happy_hardcore_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_cassette\n",
      "============================================================\n",
      "Loaded 344 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 324 nodes, 738 edges\n",
      "  Artists: 130, Labels: 194\n",
      "  Building 34 clusters...\n",
      "  Assigned 34 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cassette/breakbeat_happy_hardcore_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cassette/breakbeat_happy_hardcore_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cassette/breakbeat_happy_hardcore_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_cassette/breakbeat_happy_hardcore_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_digital\n",
      "============================================================\n",
      "Loaded 685 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 695 nodes, 2882 edges\n",
      "  Artists: 403, Labels: 292\n",
      "  Building 68 clusters...\n",
      "  Assigned 68 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_digital/breakbeat_happy_hardcore_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_digital/breakbeat_happy_hardcore_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_digital/breakbeat_happy_hardcore_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_happy_hardcore_digital/breakbeat_happy_hardcore_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_happy_hardcore_dubplate\n",
      "============================================================\n",
      "Loaded 31 releases\n",
      "Skipping breakbeat_happy_hardcore_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_vinyl\n",
      "============================================================\n",
      "Loaded 5212 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 6159 nodes, 142721 edges\n",
      "  Artists: 3296, Labels: 2863\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_vinyl/happy_hardcore_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_vinyl/happy_hardcore_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_vinyl/happy_hardcore_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_vinyl/happy_hardcore_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_cd\n",
      "============================================================\n",
      "Loaded 9173 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 11000 nodes, 85314 edges\n",
      "  Artists: 2955, Labels: 8045\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cd/happy_hardcore_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cd/happy_hardcore_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cd/happy_hardcore_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cd/happy_hardcore_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_vhs\n",
      "============================================================\n",
      "Loaded 71 releases\n",
      "Skipping happy_hardcore_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_other\n",
      "============================================================\n",
      "Loaded 143 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 297 nodes, 273 edges\n",
      "  Artists: 67, Labels: 230\n",
      "  Building 14 clusters...\n",
      "  Assigned 14 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_other/happy_hardcore_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_other/happy_hardcore_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_other/happy_hardcore_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_other/happy_hardcore_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_cassette\n",
      "============================================================\n",
      "Loaded 2665 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2583 nodes, 10878 edges\n",
      "  Artists: 792, Labels: 1791\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cassette/happy_hardcore_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cassette/happy_hardcore_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cassette/happy_hardcore_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_cassette/happy_hardcore_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_digital\n",
      "============================================================\n",
      "Loaded 13201 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8018 nodes, 794039 edges\n",
      "  Artists: 5669, Labels: 2349\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_digital/happy_hardcore_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_digital/happy_hardcore_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_digital/happy_hardcore_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_digital/happy_hardcore_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: happy_hardcore_dubplate\n",
      "============================================================\n",
      "Loaded 176 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 209 nodes, 978 edges\n",
      "  Artists: 193, Labels: 16\n",
      "  Building 17 clusters...\n",
      "  Assigned 17 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_dubplate/happy_hardcore_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_dubplate/happy_hardcore_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_dubplate/happy_hardcore_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_happy_hardcore_dubplate/happy_hardcore_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_vinyl\n",
      "============================================================\n",
      "Loaded 7853 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 8263 nodes, 246120 edges\n",
      "  Artists: 4461, Labels: 3802\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_vinyl/dubstep_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_vinyl/dubstep_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_vinyl/dubstep_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_vinyl/dubstep_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_cd\n",
      "============================================================\n",
      "Loaded 6275 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 9623 nodes, 56714 edges\n",
      "  Artists: 3553, Labels: 6070\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cd/dubstep_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cd/dubstep_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cd/dubstep_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cd/dubstep_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping dubstep_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_other\n",
      "============================================================\n",
      "Loaded 480 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 747 nodes, 2641 edges\n",
      "  Artists: 393, Labels: 354\n",
      "  Building 48 clusters...\n",
      "  Assigned 48 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_other/dubstep_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_other/dubstep_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_other/dubstep_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_other/dubstep_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_cassette\n",
      "============================================================\n",
      "Loaded 308 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 522 nodes, 437 edges\n",
      "  Artists: 249, Labels: 273\n",
      "  Building 30 clusters...\n",
      "  Assigned 30 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cassette/dubstep_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cassette/dubstep_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cassette/dubstep_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_cassette/dubstep_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_digital\n",
      "============================================================\n",
      "Loaded 31106 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 23927 nodes, 609062 edges\n",
      "  Artists: 16222, Labels: 7705\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_digital/dubstep_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_digital/dubstep_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_digital/dubstep_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dubstep_digital/dubstep_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dubstep_dubplate\n",
      "============================================================\n",
      "Loaded 23 releases\n",
      "Skipping dubstep_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_vinyl\n",
      "============================================================\n",
      "Loaded 579 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1129 nodes, 7030 edges\n",
      "  Artists: 368, Labels: 761\n",
      "  Building 57 clusters...\n",
      "  Assigned 57 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_vinyl/miami_bass_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_vinyl/miami_bass_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_vinyl/miami_bass_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_vinyl/miami_bass_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_cd\n",
      "============================================================\n",
      "Loaded 649 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1549 nodes, 4419 edges\n",
      "  Artists: 358, Labels: 1191\n",
      "  Building 64 clusters...\n",
      "  Assigned 64 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cd/miami_bass_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cd/miami_bass_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cd/miami_bass_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cd/miami_bass_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping miami_bass_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_other\n",
      "============================================================\n",
      "Loaded 20 releases\n",
      "Skipping miami_bass_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_cassette\n",
      "============================================================\n",
      "Loaded 218 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 434 nodes, 808 edges\n",
      "  Artists: 136, Labels: 298\n",
      "  Building 21 clusters...\n",
      "  Assigned 21 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cassette/miami_bass_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cassette/miami_bass_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cassette/miami_bass_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_cassette/miami_bass_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_digital\n",
      "============================================================\n",
      "Loaded 215 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 333 nodes, 392 edges\n",
      "  Artists: 158, Labels: 175\n",
      "  Building 21 clusters...\n",
      "  Assigned 21 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_digital/miami_bass_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_digital/miami_bass_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_digital/miami_bass_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_miami_bass_digital/miami_bass_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: miami_bass_dubplate\n",
      "============================================================\n",
      "Loaded 2 releases\n",
      "Skipping miami_bass_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_vinyl\n",
      "============================================================\n",
      "Loaded 1443 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2266 nodes, 16057 edges\n",
      "  Artists: 1038, Labels: 1228\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_vinyl/ghetto_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_vinyl/ghetto_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_vinyl/ghetto_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_vinyl/ghetto_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_cd\n",
      "============================================================\n",
      "Loaded 746 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1681 nodes, 2620 edges\n",
      "  Artists: 506, Labels: 1175\n",
      "  Building 74 clusters...\n",
      "  Assigned 74 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cd/ghetto_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cd/ghetto_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cd/ghetto_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cd/ghetto_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_vhs\n",
      "============================================================\n",
      "Loaded 4 releases\n",
      "Skipping ghetto_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_other\n",
      "============================================================\n",
      "Loaded 46 releases\n",
      "Skipping ghetto_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_cassette\n",
      "============================================================\n",
      "Loaded 348 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 536 nodes, 731 edges\n",
      "  Artists: 273, Labels: 263\n",
      "  Building 34 clusters...\n",
      "  Assigned 34 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cassette/ghetto_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cassette/ghetto_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cassette/ghetto_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_cassette/ghetto_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_digital\n",
      "============================================================\n",
      "Loaded 2274 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2680 nodes, 6355 edges\n",
      "  Artists: 1548, Labels: 1132\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_digital/ghetto_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_digital/ghetto_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_digital/ghetto_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_ghetto_digital/ghetto_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: ghetto_dubplate\n",
      "============================================================\n",
      "Loaded 2 releases\n",
      "Skipping ghetto_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_deep_acid_vinyl\n",
      "============================================================\n",
      "Loaded 35 releases\n",
      "Skipping techno_deep_acid_vinyl - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_deep_acid_cd\n",
      "============================================================\n",
      "Loaded 12 releases\n",
      "Skipping techno_deep_acid_cd - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_deep_acid_other\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping techno_deep_acid_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_deep_acid_cassette\n",
      "============================================================\n",
      "Loaded 5 releases\n",
      "Skipping techno_deep_acid_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_deep_acid_digital\n",
      "============================================================\n",
      "Loaded 54 releases\n",
      "Skipping techno_deep_acid_digital - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_vinyl\n",
      "============================================================\n",
      "Loaded 12601 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 19920 nodes, 349408 edges\n",
      "  Artists: 8348, Labels: 11572\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_vinyl/acid_house_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_vinyl/acid_house_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_vinyl/acid_house_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_vinyl/acid_house_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_cd\n",
      "============================================================\n",
      "Loaded 6701 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 14558 nodes, 70180 edges\n",
      "  Artists: 2912, Labels: 11646\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cd/acid_house_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cd/acid_house_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cd/acid_house_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cd/acid_house_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_vhs\n",
      "============================================================\n",
      "Loaded 98 releases\n",
      "Skipping acid_house_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_other\n",
      "============================================================\n",
      "Loaded 253 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 440 nodes, 612 edges\n",
      "  Artists: 153, Labels: 287\n",
      "  Building 25 clusters...\n",
      "  Assigned 25 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_other/acid_house_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_other/acid_house_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_other/acid_house_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_other/acid_house_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_cassette\n",
      "============================================================\n",
      "Loaded 2079 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 3580 nodes, 5303 edges\n",
      "  Artists: 1081, Labels: 2499\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cassette/acid_house_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cassette/acid_house_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cassette/acid_house_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_cassette/acid_house_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_digital\n",
      "============================================================\n",
      "Loaded 8220 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 9349 nodes, 24861 edges\n",
      "  Artists: 5141, Labels: 4208\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_digital/acid_house_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_digital/acid_house_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_digital/acid_house_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_acid_house_digital/acid_house_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: acid_house_dubplate\n",
      "============================================================\n",
      "Loaded 43 releases\n",
      "Skipping acid_house_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_vinyl\n",
      "============================================================\n",
      "Loaded 134 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 287 nodes, 569 edges\n",
      "  Artists: 109, Labels: 178\n",
      "  Building 13 clusters...\n",
      "  Assigned 13 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_vinyl/hardcore_techno_jungle_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_vinyl/hardcore_techno_jungle_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_vinyl/hardcore_techno_jungle_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_vinyl/hardcore_techno_jungle_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_cd\n",
      "============================================================\n",
      "Loaded 130 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 409 nodes, 412 edges\n",
      "  Artists: 56, Labels: 353\n",
      "  Building 13 clusters...\n",
      "  Assigned 13 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cd/hardcore_techno_jungle_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cd/hardcore_techno_jungle_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cd/hardcore_techno_jungle_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cd/hardcore_techno_jungle_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_vhs\n",
      "============================================================\n",
      "Loaded 21 releases\n",
      "Skipping hardcore_techno_jungle_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_other\n",
      "============================================================\n",
      "Loaded 13 releases\n",
      "Skipping hardcore_techno_jungle_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_cassette\n",
      "============================================================\n",
      "Loaded 138 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 178 nodes, 542 edges\n",
      "  Artists: 90, Labels: 88\n",
      "  Building 13 clusters...\n",
      "  Assigned 13 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cassette/hardcore_techno_jungle_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cassette/hardcore_techno_jungle_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cassette/hardcore_techno_jungle_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_cassette/hardcore_techno_jungle_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: hardcore_techno_jungle_digital\n",
      "============================================================\n",
      "Loaded 187 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 265 nodes, 225 edges\n",
      "  Artists: 119, Labels: 146\n",
      "  Building 18 clusters...\n",
      "  Assigned 18 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_digital/hardcore_techno_jungle_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_digital/hardcore_techno_jungle_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_digital/hardcore_techno_jungle_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_hardcore_techno_jungle_digital/hardcore_techno_jungle_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_vinyl\n",
      "============================================================\n",
      "Loaded 27081 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 33430 nodes, 1448675 edges\n",
      "  Artists: 16705, Labels: 16725\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vinyl/breakbeat_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vinyl/breakbeat_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vinyl/breakbeat_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vinyl/breakbeat_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_cd\n",
      "============================================================\n",
      "Loaded 17267 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 29224 nodes, 301279 edges\n",
      "  Artists: 8295, Labels: 20929\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cd/breakbeat_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cd/breakbeat_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cd/breakbeat_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cd/breakbeat_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_vhs\n",
      "============================================================\n",
      "Loaded 161 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 252 nodes, 270 edges\n",
      "  Artists: 71, Labels: 181\n",
      "  Building 16 clusters...\n",
      "  Assigned 16 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vhs/breakbeat_vhs_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vhs/breakbeat_vhs_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vhs/breakbeat_vhs_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_vhs/breakbeat_vhs_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_other\n",
      "============================================================\n",
      "Loaded 807 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1254 nodes, 2170 edges\n",
      "  Artists: 519, Labels: 735\n",
      "  Building 80 clusters...\n",
      "  Assigned 80 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_other/breakbeat_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_other/breakbeat_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_other/breakbeat_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_other/breakbeat_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_cassette\n",
      "============================================================\n",
      "Loaded 4495 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 6625 nodes, 20028 edges\n",
      "  Artists: 2382, Labels: 4243\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cassette/breakbeat_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cassette/breakbeat_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cassette/breakbeat_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_cassette/breakbeat_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_digital\n",
      "============================================================\n",
      "Loaded 20699 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 20178 nodes, 86876 edges\n",
      "  Artists: 12343, Labels: 7835\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_digital/breakbeat_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_digital/breakbeat_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_digital/breakbeat_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_digital/breakbeat_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: breakbeat_dubplate\n",
      "============================================================\n",
      "Loaded 354 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 495 nodes, 5622 edges\n",
      "  Artists: 370, Labels: 125\n",
      "  Building 35 clusters...\n",
      "  Assigned 35 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_dubplate/breakbeat_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_dubplate/breakbeat_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_dubplate/breakbeat_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_breakbeat_dubplate/breakbeat_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_vinyl\n",
      "============================================================\n",
      "Loaded 2275 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1809 nodes, 21470 edges\n",
      "  Artists: 940, Labels: 869\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_vinyl/tribal_freetekno_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_vinyl/tribal_freetekno_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_vinyl/tribal_freetekno_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_vinyl/tribal_freetekno_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_cd\n",
      "============================================================\n",
      "Loaded 80 releases\n",
      "Skipping tribal_freetekno_cd - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_other\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping tribal_freetekno_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_cassette\n",
      "============================================================\n",
      "Loaded 141 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 143 nodes, 177 edges\n",
      "  Artists: 97, Labels: 46\n",
      "  Building 14 clusters...\n",
      "  Assigned 14 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_cassette/tribal_freetekno_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_cassette/tribal_freetekno_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_cassette/tribal_freetekno_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_cassette/tribal_freetekno_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_digital\n",
      "============================================================\n",
      "Loaded 895 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 645 nodes, 2333 edges\n",
      "  Artists: 357, Labels: 288\n",
      "  Building 89 clusters...\n",
      "  Assigned 89 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_digital/tribal_freetekno_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_digital/tribal_freetekno_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_digital/tribal_freetekno_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_tribal_freetekno_digital/tribal_freetekno_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: tribal_freetekno_dubplate\n",
      "============================================================\n",
      "Loaded 3 releases\n",
      "Skipping tribal_freetekno_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_vinyl\n",
      "============================================================\n",
      "Loaded 30624 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 25915 nodes, 2679809 edges\n",
      "  Artists: 14807, Labels: 11108\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_vinyl/dnb_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_vinyl/dnb_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_vinyl/dnb_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_vinyl/dnb_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_cd\n",
      "============================================================\n",
      "Loaded 18498 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 27302 nodes, 233199 edges\n",
      "  Artists: 8512, Labels: 18790\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cd/dnb_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cd/dnb_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cd/dnb_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cd/dnb_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_vhs\n",
      "============================================================\n",
      "Loaded 68 releases\n",
      "Skipping dnb_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_other\n",
      "============================================================\n",
      "Loaded 715 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1121 nodes, 1572 edges\n",
      "  Artists: 491, Labels: 630\n",
      "  Building 71 clusters...\n",
      "  Assigned 71 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_other/dnb_other_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_other/dnb_other_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_other/dnb_other_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_other/dnb_other_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_cassette\n",
      "============================================================\n",
      "Loaded 3675 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 4499 nodes, 12391 edges\n",
      "  Artists: 1907, Labels: 2592\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cassette/dnb_cassette_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cassette/dnb_cassette_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cassette/dnb_cassette_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_cassette/dnb_cassette_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_digital\n",
      "============================================================\n",
      "Loaded 56570 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 33510 nodes, 1255491 edges\n",
      "  Artists: 23276, Labels: 10234\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_digital/dnb_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_digital/dnb_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_digital/dnb_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_digital/dnb_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dnb_dubplate\n",
      "============================================================\n",
      "Loaded 1478 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1529 nodes, 255366 edges\n",
      "  Artists: 1381, Labels: 148\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_dubplate/dnb_dubplate_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_dubplate/dnb_dubplate_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_dubplate/dnb_dubplate_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dnb_dubplate/dnb_dubplate_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_vinyl\n",
      "============================================================\n",
      "Loaded 2102 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2508 nodes, 6700 edges\n",
      "  Artists: 1154, Labels: 1354\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_vinyl/dj_battle_tool_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_vinyl/dj_battle_tool_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_vinyl/dj_battle_tool_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_vinyl/dj_battle_tool_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_cd\n",
      "============================================================\n",
      "Loaded 237 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 451 nodes, 479 edges\n",
      "  Artists: 159, Labels: 292\n",
      "  Building 23 clusters...\n",
      "  Assigned 23 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_cd/dj_battle_tool_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_cd/dj_battle_tool_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_cd/dj_battle_tool_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_cd/dj_battle_tool_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_vhs\n",
      "============================================================\n",
      "Loaded 16 releases\n",
      "Skipping dj_battle_tool_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_other\n",
      "============================================================\n",
      "Loaded 34 releases\n",
      "Skipping dj_battle_tool_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_cassette\n",
      "============================================================\n",
      "Loaded 36 releases\n",
      "Skipping dj_battle_tool_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_digital\n",
      "============================================================\n",
      "Loaded 130 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 190 nodes, 152 edges\n",
      "  Artists: 114, Labels: 76\n",
      "  Building 13 clusters...\n",
      "  Assigned 13 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_digital/dj_battle_tool_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_digital/dj_battle_tool_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_digital/dj_battle_tool_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_dj_battle_tool_digital/dj_battle_tool_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: dj_battle_tool_dubplate\n",
      "============================================================\n",
      "Loaded 2 releases\n",
      "Skipping dj_battle_tool_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_vinyl\n",
      "============================================================\n",
      "Loaded 425 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 606 nodes, 2044 edges\n",
      "  Artists: 267, Labels: 339\n",
      "  Building 42 clusters...\n",
      "  Assigned 42 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_vinyl/baltimore_club_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_vinyl/baltimore_club_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_vinyl/baltimore_club_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_vinyl/baltimore_club_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_cd\n",
      "============================================================\n",
      "Loaded 114 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 262 nodes, 369 edges\n",
      "  Artists: 96, Labels: 166\n",
      "  Building 11 clusters...\n",
      "  Assigned 11 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_cd/baltimore_club_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_cd/baltimore_club_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_cd/baltimore_club_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_cd/baltimore_club_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_vhs\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping baltimore_club_vhs - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_other\n",
      "============================================================\n",
      "Loaded 6 releases\n",
      "Skipping baltimore_club_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_cassette\n",
      "============================================================\n",
      "Loaded 42 releases\n",
      "Skipping baltimore_club_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_digital\n",
      "============================================================\n",
      "Loaded 347 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 462 nodes, 506 edges\n",
      "  Artists: 259, Labels: 203\n",
      "  Building 34 clusters...\n",
      "  Assigned 34 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_digital/baltimore_club_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_digital/baltimore_club_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_digital/baltimore_club_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_baltimore_club_digital/baltimore_club_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: baltimore_club_dubplate\n",
      "============================================================\n",
      "Loaded 1 releases\n",
      "Skipping baltimore_club_dubplate - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: donk_vinyl\n",
      "============================================================\n",
      "Loaded 1204 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 1359 nodes, 21132 edges\n",
      "  Artists: 773, Labels: 586\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_vinyl/donk_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_vinyl/donk_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_vinyl/donk_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_vinyl/donk_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: donk_cd\n",
      "============================================================\n",
      "Loaded 619 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 964 nodes, 5502 edges\n",
      "  Artists: 362, Labels: 602\n",
      "  Building 61 clusters...\n",
      "  Assigned 61 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_cd/donk_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_cd/donk_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_cd/donk_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_cd/donk_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: donk_other\n",
      "============================================================\n",
      "Loaded 19 releases\n",
      "Skipping donk_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: donk_cassette\n",
      "============================================================\n",
      "Loaded 46 releases\n",
      "Skipping donk_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: donk_digital\n",
      "============================================================\n",
      "Loaded 3274 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 2121 nodes, 121929 edges\n",
      "  Artists: 1593, Labels: 528\n",
      "  Building 100 clusters...\n",
      "  Assigned 100 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_digital/donk_digital_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_digital/donk_digital_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_digital/donk_digital_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_donk_digital/donk_digital_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: techno_future_jazz_vinyl\n",
      "============================================================\n",
      "Loaded 146 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 386 nodes, 639 edges\n",
      "  Artists: 109, Labels: 277\n",
      "  Building 14 clusters...\n",
      "  Assigned 14 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_vinyl/techno_future_jazz_vinyl_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_vinyl/techno_future_jazz_vinyl_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_vinyl/techno_future_jazz_vinyl_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_vinyl/techno_future_jazz_vinyl_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: techno_future_jazz_cd\n",
      "============================================================\n",
      "Loaded 185 releases\n",
      "Parsing artists...\n",
      "Computing oldest years...\n",
      "Building knowledge graph...\n",
      "  Building nodes and edges...\n",
      "  Adding same-label artist connections...\n",
      "  Graph: 662 nodes, 845 edges\n",
      "  Artists: 94, Labels: 568\n",
      "  Building 18 clusters...\n",
      "  Assigned 18 clusters\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_cd/techno_future_jazz_cd_processed.csv\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_cd/techno_future_jazz_cd_graph.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_cd/techno_future_jazz_cd_kmeans.pkl\n",
      "Saved: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/processed_techno_future_jazz_cd/techno_future_jazz_cd_stats.txt\n",
      "\n",
      "============================================================\n",
      "Processing: techno_future_jazz_other\n",
      "============================================================\n",
      "Loaded 15 releases\n",
      "Skipping techno_future_jazz_other - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_future_jazz_cassette\n",
      "============================================================\n",
      "Loaded 8 releases\n",
      "Skipping techno_future_jazz_cassette - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "Processing: techno_future_jazz_digital\n",
      "============================================================\n",
      "Loaded 8 releases\n",
      "Skipping techno_future_jazz_digital - too few releases for meaningful clustering\n",
      "\n",
      "============================================================\n",
      "COMPLETE: Processed 289 CSV files\n",
      "Output folder: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DISCOGS PRE-PROCESSING SCRIPT\n",
    "Builds knowledge graph and clusters for recommendation engine.\n",
    "\n",
    "OUTPUTS:\n",
    "- Knowledge graph (NetworkX pickle file)\n",
    "- CSV with cluster assignments\n",
    "- Artist lookup dictionary\n",
    "\n",
    "USAGE:\n",
    "1. Set INPUT_FOLDER and OUTPUT_FOLDER paths\n",
    "2. Run script - processes all CSVs in split folders\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === CONFIGURABLE PARAMETERS ===\n",
    "INPUT_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/CLEANED_CSVS\"\n",
    "OUTPUT_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS\"\n",
    "N_CLUSTERS = 100  # Number of clusters for K-Means\n",
    "# For reproducibility you will need to update these paths for your system\n",
    "\n",
    "# ============================================================\n",
    "# ARTIST PARSING\n",
    "# ============================================================\n",
    "\n",
    "def parse_artists(artist_string):\n",
    "    \"\"\"\n",
    "    Parse multiple artists from artist string.\n",
    "    Splits on &, feat., ft., vs, x, and, comma.\n",
    "    Fuzzy matching handles variations later.\n",
    "    \"\"\"\n",
    "    if pd.isna(artist_string) or artist_string == '':\n",
    "        return []\n",
    "    \n",
    "    artist_string = str(artist_string).strip()\n",
    "    \n",
    "    # Split on separators (order matters - longer patterns first)\n",
    "    separators = [\n",
    "        ' feat. ', ' Feat. ', ' ft. ', ' Ft. ',\n",
    "        ' vs ', ' vs. ', ' Vs ', ' Vs. ',\n",
    "        ' x ', ' X ',\n",
    "        ' & ',\n",
    "        ' and ',\n",
    "        ', ',\n",
    "        ','\n",
    "    ]\n",
    "    \n",
    "    artists = [artist_string]\n",
    "    for sep in separators:\n",
    "        new_artists = []\n",
    "        for a in artists:\n",
    "            new_artists.extend(a.split(sep))\n",
    "        artists = new_artists\n",
    "    \n",
    "    # Clean up\n",
    "    cleaned = []\n",
    "    for a in artists:\n",
    "        a = a.strip()\n",
    "        if a and len(a) > 1:  # Ignore single characters\n",
    "            cleaned.append(a)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def get_oldest_year(row):\n",
    "    \"\"\"\n",
    "    Get the oldest year from year and alt_versions_year columns.\n",
    "    \"\"\"\n",
    "    years = []\n",
    "    \n",
    "    # Main year\n",
    "    if pd.notna(row['year']) and row['year'] > 0:\n",
    "        years.append(int(row['year']))\n",
    "    \n",
    "    # Alt versions years\n",
    "    if 'alt_versions_year' in row and pd.notna(row['alt_versions_year']) and row['alt_versions_year'] != '':\n",
    "        alt_years_str = str(row['alt_versions_year'])\n",
    "        for y in alt_years_str.split(' | '):\n",
    "            try:\n",
    "                year_val = int(float(y.strip()))\n",
    "                if year_val > 0:\n",
    "                    years.append(year_val)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return min(years) if years else 0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# KNOWLEDGE GRAPH BUILDING\n",
    "# ============================================================\n",
    "\n",
    "def build_knowledge_graph(df):\n",
    "    \"\"\"\n",
    "    Build knowledge graph from release data.\n",
    "    \n",
    "    Nodes:\n",
    "    - Artists (node_type='artist')\n",
    "    - Labels (node_type='label')\n",
    "    \n",
    "    Edges:\n",
    "    - artist -> label (released_on)\n",
    "    - artist <-> artist (collaborated_with) - from same release\n",
    "    - artist <-> artist (same_label) - released on same label\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track artist-label relationships for same_label edges\n",
    "    label_artists = {}  # label -> set of artists\n",
    "    \n",
    "    print(\"  Building nodes and edges...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse artists\n",
    "        artists = parse_artists(row['parsed_artist'])\n",
    "        \n",
    "        # Parse labels (split on comma, ignore \"Not On Label\")\n",
    "        labels = []\n",
    "        if pd.notna(row['label']):\n",
    "            for label in str(row['label']).split(','):\n",
    "                label = label.strip()\n",
    "                if label and 'not on label' not in label.lower():\n",
    "                    labels.append(label)\n",
    "        \n",
    "        # Add artist nodes\n",
    "        for artist in artists:\n",
    "            if artist not in G:\n",
    "                G.add_node(artist, node_type='artist')\n",
    "        \n",
    "        # Add label nodes\n",
    "        for label in labels:\n",
    "            if label not in G:\n",
    "                G.add_node(label, node_type='label')\n",
    "            \n",
    "            # Track artists per label\n",
    "            if label not in label_artists:\n",
    "                label_artists[label] = set()\n",
    "        \n",
    "        # Add artist -> label edges\n",
    "        for artist in artists:\n",
    "            for label in labels:\n",
    "                if not G.has_edge(artist, label):\n",
    "                    G.add_edge(artist, label, relationship='released_on')\n",
    "                label_artists[label].add(artist)\n",
    "        \n",
    "        # Add collaboration edges (artists on same release)\n",
    "        if len(artists) > 1:\n",
    "            for i, artist1 in enumerate(artists):\n",
    "                for artist2 in artists[i+1:]:\n",
    "                    if not G.has_edge(artist1, artist2):\n",
    "                        G.add_edge(artist1, artist2, relationship='collaborated_with')\n",
    "                    elif G[artist1][artist2].get('relationship') != 'collaborated_with':\n",
    "                        # Upgrade to collaboration if they actually collaborated\n",
    "                        G[artist1][artist2]['relationship'] = 'collaborated_with'\n",
    "    \n",
    "    # Add same_label edges between artists\n",
    "    print(\"  Adding same-label artist connections...\")\n",
    "    for label, artists_set in label_artists.items():\n",
    "        artists_list = list(artists_set)\n",
    "        for i, artist1 in enumerate(artists_list):\n",
    "            for artist2 in artists_list[i+1:]:\n",
    "                if not G.has_edge(artist1, artist2):\n",
    "                    G.add_edge(artist1, artist2, relationship='same_label', via_label=label)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CLUSTERING\n",
    "# ============================================================\n",
    "\n",
    "def build_clusters(df, n_clusters=100):\n",
    "    \"\"\"\n",
    "    Build K-Means clusters based on year, styles, and country.\n",
    "    \n",
    "    Returns dataframe with 'cluster' column added.\n",
    "    \"\"\"\n",
    "    print(f\"  Building {n_clusters} clusters...\")\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Year (normalised)\n",
    "    feature_df['year_norm'] = df['year'].fillna(0) / 2025\n",
    "    \n",
    "    # Country (one-hot encoded, top 20 countries only to limit columns)\n",
    "    top_countries = df['country'].value_counts().head(20).index.tolist()\n",
    "    for country in top_countries:\n",
    "        feature_df[f'country_{country}'] = (df['country'] == country).astype(int)\n",
    "    \n",
    "    # Styles (one-hot encoded)\n",
    "    # Explode styles and create binary columns\n",
    "    all_styles = set()\n",
    "    for styles_str in df['style'].dropna():\n",
    "        for style in str(styles_str).split(','):\n",
    "            all_styles.add(style.strip())\n",
    "    \n",
    "    # Limit to top 50 styles to manage dimensionality\n",
    "    style_counts = {}\n",
    "    for styles_str in df['style'].dropna():\n",
    "        for style in str(styles_str).split(','):\n",
    "            style = style.strip()\n",
    "            style_counts[style] = style_counts.get(style, 0) + 1\n",
    "    \n",
    "    top_styles = sorted(style_counts.keys(), key=lambda x: style_counts[x], reverse=True)[:50]\n",
    "    \n",
    "    for style in top_styles:\n",
    "        feature_df[f'style_{style}'] = df['style'].fillna('').str.contains(style, case=False, regex=False).astype(int)\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    feature_df = feature_df.fillna(0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(feature_df)\n",
    "    \n",
    "    # Run K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df = df.copy()\n",
    "    df['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    return df, kmeans\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def process_csv(input_path, output_folder, style_name):\n",
    "    \"\"\"\n",
    "    Process a single CSV file:\n",
    "    1. Parse artists\n",
    "    2. Get oldest year\n",
    "    3. Build knowledge graph\n",
    "    4. Build clusters\n",
    "    5. Save outputs\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to CSV\n",
    "        output_folder: Base output folder\n",
    "        style_name: Style/genre name (e.g., 'happy_hardcore')\n",
    "    \"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    \n",
    "    # Create unique name combining style and format\n",
    "    unique_name = f\"{style_name}_{filename}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {unique_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(input_path)\n",
    "    print(f\"Loaded {len(df)} releases\")\n",
    "    \n",
    "    # Skip if too few releases\n",
    "    if len(df) < 100:\n",
    "        print(f\"Skipping {unique_name} - too few releases for meaningful clustering\")\n",
    "        return\n",
    "    \n",
    "    # Parse artists into list column\n",
    "    print(\"Parsing artists...\")\n",
    "    df['artists_list'] = df['parsed_artist'].apply(parse_artists)\n",
    "    \n",
    "    # Get oldest year\n",
    "    print(\"Computing oldest years...\")\n",
    "    df['oldest_year'] = df.apply(get_oldest_year, axis=1)\n",
    "    \n",
    "    # Build knowledge graph\n",
    "    print(\"Building knowledge graph...\")\n",
    "    G = build_knowledge_graph(df)\n",
    "    print(f\"  Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Count node types\n",
    "    artists = [n for n, d in G.nodes(data=True) if d.get('node_type') == 'artist']\n",
    "    labels = [n for n, d in G.nodes(data=True) if d.get('node_type') == 'label']\n",
    "    print(f\"  Artists: {len(artists)}, Labels: {len(labels)}\")\n",
    "    \n",
    "    # Build clusters\n",
    "    n_clusters = min(N_CLUSTERS, len(df) // 10)  # At least 10 per cluster\n",
    "    df, kmeans = build_clusters(df, n_clusters=n_clusters)\n",
    "    print(f\"  Assigned {n_clusters} clusters\")\n",
    "    \n",
    "    # Create output subfolder with unique name\n",
    "    output_subfolder = os.path.join(output_folder, f\"processed_{unique_name}\")\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "    \n",
    "    # Save processed CSV\n",
    "    csv_path = os.path.join(output_subfolder, f\"{unique_name}_processed.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "    \n",
    "    # Save knowledge graph\n",
    "    graph_path = os.path.join(output_subfolder, f\"{unique_name}_graph.pkl\")\n",
    "    with open(graph_path, 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"Saved: {graph_path}\")\n",
    "    \n",
    "    # Save kmeans model\n",
    "    kmeans_path = os.path.join(output_subfolder, f\"{unique_name}_kmeans.pkl\")\n",
    "    with open(kmeans_path, 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    print(f\"Saved: {kmeans_path}\")\n",
    "    \n",
    "    # Save summary stats\n",
    "    stats = {\n",
    "        'total_releases': len(df),\n",
    "        'unique_artists': len(artists),\n",
    "        'unique_labels': len(labels),\n",
    "        'graph_nodes': G.number_of_nodes(),\n",
    "        'graph_edges': G.number_of_edges(),\n",
    "        'n_clusters': n_clusters\n",
    "    }\n",
    "    stats_path = os.path.join(output_subfolder, f\"{unique_name}_stats.txt\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        for k, v in stats.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "    print(f\"Saved: {stats_path}\")\n",
    "    \n",
    "    return df, G\n",
    "\n",
    "\n",
    "def process_all_csvs(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all CSVs in split subfolders.\n",
    "    Extracts style name from parent folder (e.g., split_discogs_happy_hardcore -> happy_hardcore)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    processed = 0\n",
    "    \n",
    "    # Walk through all subfolders\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            # Process vinyl.csv, cd.csv, etc. but not cleaned_full.csv\n",
    "            if file.endswith('.csv') and file != 'cleaned_full.csv':\n",
    "                input_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract style name from parent folder\n",
    "                # e.g., \"split_discogs_happy_hardcore\" -> \"happy_hardcore\"\n",
    "                parent_folder = os.path.basename(root)\n",
    "                if parent_folder.startswith('split_discogs_'):\n",
    "                    style_name = parent_folder.replace('split_discogs_', '')\n",
    "                elif parent_folder.startswith('split_'):\n",
    "                    style_name = parent_folder.replace('split_', '')\n",
    "                else:\n",
    "                    style_name = parent_folder\n",
    "                \n",
    "                try:\n",
    "                    process_csv(input_path, output_folder, style_name)\n",
    "                    processed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE: Processed {processed} CSV files\")\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    print('='*60)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_csvs(INPUT_FOLDER, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb2c35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Reorganise folders by style\n",
      "============================================================\n",
      "Found 184 folders to reorganise\n",
      "\n",
      "Preview of reorganisation:\n",
      "------------------------------------------------------------\n",
      "  acid_house/: cd, other, cassette, vinyl, digital\n",
      "  baltimore_club/: cd, digital, vinyl\n",
      "  bassline/: digital, cassette, cd, vinyl, other\n",
      "  bleep/: cd, digital, vinyl\n",
      "  breakbeat/: other, vhs, digital, cd, cassette, vinyl, dubplate\n",
      "  breakbeat_acid/: vinyl, cd, digital, cassette\n",
      "  breakbeat_happy_hardcore/: digital, vinyl, cassette, cd\n",
      "  breakbeat_hardcore/: digital, vinyl, other, cassette, cd, dubplate\n",
      "  breakbeat_hardcore_happy_hardcore/: digital, cassette, cd, vinyl\n",
      "  breakbeat_hardcore_jungle/: vinyl, digital, cassette, cd\n",
      "  ... and 32 more styles\n",
      "  Moved: happy_hardcore/processed_vinyl/\n",
      "  Moved: electro/processed_cd/\n",
      "  Moved: jungle_techno/processed_vinyl/\n",
      "  Moved: electro/processed_vhs/\n",
      "  Moved: breakbeat_house/processed_cassette/\n",
      "  Moved: bleep/processed_cd/\n",
      "  Moved: breakbeat_house/processed_digital/\n",
      "  Moved: happy_hardcore/processed_cassette/\n",
      "  Moved: hiphop_breakbeat/processed_cd/\n",
      "  Moved: breakbeat_hardcore_jungle/processed_vinyl/\n",
      "  Moved: dnb/processed_dubplate/\n",
      "  Moved: footwork/processed_digital/\n",
      "  Moved: italo_house/processed_vinyl/\n",
      "  Moved: hardcore_acid/processed_digital/\n",
      "  Moved: hardcore_acid/processed_vinyl/\n",
      "  Moved: dub_jungle/processed_digital/\n",
      "  Moved: gabber/processed_other/\n",
      "  Moved: breakbeat_house/processed_vinyl/\n",
      "  Moved: acid_house/processed_cd/\n",
      "  Moved: dnb/processed_cd/\n",
      "  Moved: jungle/processed_digital/\n",
      "  Moved: ghetto/processed_cassette/\n",
      "  Moved: hardcore_acid/processed_cd/\n",
      "  Moved: breakbeat_acid/processed_vinyl/\n",
      "  Moved: speed_garage/processed_digital/\n",
      "  Moved: bassline/processed_digital/\n",
      "  Moved: uk_garage/processed_digital/\n",
      "  Moved: tribal/processed_digital/\n",
      "  Moved: gabber/processed_digital/\n",
      "  Moved: bleep/processed_digital/\n",
      "  Moved: uk_funky/processed_vinyl/\n",
      "  Moved: dnb/processed_cassette/\n",
      "  Moved: breakbeat_hardcore_techno/processed_cassette/\n",
      "  Moved: breakbeat_hardcore_happy_hardcore/processed_digital/\n",
      "  Moved: ghetto_house/processed_cassette/\n",
      "  Moved: hiphop_breakbeat/processed_vinyl/\n",
      "  Moved: acid_house/processed_other/\n",
      "  Moved: breakbeat_hardcore/processed_digital/\n",
      "  Moved: happy_hardcore/processed_dubplate/\n",
      "  Moved: makina/processed_vinyl/\n",
      "  Moved: bassline/processed_cassette/\n",
      "  Moved: uk_garage/processed_other/\n",
      "  Moved: electro/processed_other/\n",
      "  Moved: ghettotech/processed_vinyl/\n",
      "  Moved: juke/processed_cassette/\n",
      "  Moved: breakbeat_hardcore/processed_vinyl/\n",
      "  Moved: tribal/processed_other/\n",
      "  Moved: freetekno/processed_digital/\n",
      "  Moved: jungle_techno/processed_digital/\n",
      "  Moved: bassline/processed_cd/\n",
      "  Moved: dnb/processed_vinyl/\n",
      "  Moved: ghettotech/processed_cd/\n",
      "  Moved: jungle/processed_dubplate/\n",
      "  Moved: ghetto_house/processed_digital/\n",
      "  Moved: ghetto_house/processed_cd/\n",
      "  Moved: italo_house/processed_digital/\n",
      "  Moved: jungle/processed_vinyl/\n",
      "  Moved: hardcore_jungle/processed_dubplate/\n",
      "  Moved: gabber/processed_cassette/\n",
      "  Moved: dubstep/processed_digital/\n",
      "  Moved: footwork/processed_cd/\n",
      "  Moved: hardcore_techno_jungle/processed_vinyl/\n",
      "  Moved: ghetto/processed_digital/\n",
      "  Moved: breakbeat/processed_other/\n",
      "  Moved: dubstep/processed_cd/\n",
      "  Moved: makina/processed_digital/\n",
      "  Moved: hardcore_jungle/processed_cd/\n",
      "  Moved: speed_garage/processed_cassette/\n",
      "  Moved: electro/processed_digital/\n",
      "  Moved: hardcore_jungle/processed_cassette/\n",
      "  Moved: bassline/processed_vinyl/\n",
      "  Moved: breakbeat_happy_hardcore/processed_digital/\n",
      "  Moved: speed_garage/processed_other/\n",
      "  Moved: hardcore_techno_jungle/processed_cassette/\n",
      "  Moved: jungle/processed_cassette/\n",
      "  Moved: breakbeat/processed_vhs/\n",
      "  Moved: dub_jungle/processed_cd/\n",
      "  Moved: breakbeat_hardcore_techno/processed_digital/\n",
      "  Moved: uk_funky/processed_cd/\n",
      "  Moved: freetekno/processed_vinyl/\n",
      "  Moved: hardcore_jungle/processed_vinyl/\n",
      "  Moved: dubstep/processed_vinyl/\n",
      "  Moved: techno_future_jazz/processed_vinyl/\n",
      "  Moved: speed_garage/processed_cd/\n",
      "  Moved: dj_battle_tool/processed_vinyl/\n",
      "  Moved: ghettotech/processed_cassette/\n",
      "  Moved: breakbeat_hardcore_jungle/processed_digital/\n",
      "  Moved: hiphop_breakbeat/processed_digital/\n",
      "  Moved: electro_funk/processed_digital/\n",
      "  Moved: breakbeat_hardcore_jungle/processed_cassette/\n",
      "  Moved: makina/processed_cassette/\n",
      "  Moved: breakbeat_house/processed_cd/\n",
      "  Moved: breakbeat/processed_digital/\n",
      "  Moved: acid_house/processed_cassette/\n",
      "  Moved: ghettotech/processed_digital/\n",
      "  Moved: electro/processed_dubplate/\n",
      "  Moved: happy_hardcore/processed_other/\n",
      "  Moved: donk/processed_digital/\n",
      "  Moved: hardcore_techno_jungle/processed_cd/\n",
      "  Moved: breakbeat_acid/processed_cd/\n",
      "  Moved: uk_garage/processed_cassette/\n",
      "  Moved: electro_funk/processed_cassette/\n",
      "  Moved: breakbeat_acid/processed_digital/\n",
      "  Moved: freetekno/processed_cassette/\n",
      "  Moved: electro_funk/processed_cd/\n",
      "  Moved: jungle_techno/processed_cd/\n",
      "  Moved: uk_funky/processed_digital/\n",
      "  Moved: breakbeat_house/processed_other/\n",
      "  Moved: electro_funk/processed_vinyl/\n",
      "  Moved: tribal/processed_cassette/\n",
      "  Moved: gabber/processed_vinyl/\n",
      "  Moved: tribal_freetekno/processed_cassette/\n",
      "  Moved: breakbeat_acid/processed_cassette/\n",
      "  Moved: dnb/processed_digital/\n",
      "  Moved: ghetto/processed_cd/\n",
      "  Moved: dj_battle_tool/processed_digital/\n",
      "  Moved: hardcore_jungle/processed_digital/\n",
      "  Moved: uk_garage/processed_dubplate/\n",
      "  Moved: footwork/processed_vinyl/\n",
      "  Moved: dub_jungle/processed_vinyl/\n",
      "  Moved: techno_future_jazz/processed_cd/\n",
      "  Moved: tribal_freetekno/processed_digital/\n",
      "  Moved: breakbeat_happy_hardcore/processed_vinyl/\n",
      "  Moved: uk_garage/processed_vinyl/\n",
      "  Moved: electro/processed_cassette/\n",
      "  Moved: juke/processed_cd/\n",
      "  Moved: miami_bass/processed_digital/\n",
      "  Moved: acid_house/processed_vinyl/\n",
      "  Moved: gabber/processed_vhs/\n",
      "  Moved: hardcore_acid/processed_cassette/\n",
      "  Moved: breakbeat_hardcore_happy_hardcore/processed_cassette/\n",
      "  Moved: miami_bass/processed_cassette/\n",
      "  Moved: electro/processed_vinyl/\n",
      "  Moved: happy_hardcore/processed_digital/\n",
      "  Moved: juke/processed_vinyl/\n",
      "  Moved: miami_bass/processed_cd/\n",
      "  Moved: breakbeat_hardcore_techno/processed_cd/\n",
      "  Moved: juke/processed_digital/\n",
      "  Moved: tribal/processed_vinyl/\n",
      "  Moved: breakbeat_hardcore_jungle/processed_cd/\n",
      "  Moved: happy_hardcore/processed_cd/\n",
      "  Moved: breakbeat_hardcore/processed_other/\n",
      "  Moved: breakbeat_hardcore_happy_hardcore/processed_cd/\n",
      "  Moved: donk/processed_cd/\n",
      "  Moved: breakbeat_hardcore_happy_hardcore/processed_vinyl/\n",
      "  Moved: dnb/processed_other/\n",
      "  Moved: breakbeat_hardcore/processed_cassette/\n",
      "  Moved: breakbeat_hardcore_techno/processed_vinyl/\n",
      "  Moved: breakbeat/processed_cd/\n",
      "  Moved: baltimore_club/processed_cd/\n",
      "  Moved: jungle/processed_other/\n",
      "  Moved: miami_bass/processed_vinyl/\n",
      "  Moved: jungle/processed_cd/\n",
      "  Moved: bleep/processed_vinyl/\n",
      "  Moved: dubstep/processed_cassette/\n",
      "  Moved: ghetto/processed_vinyl/\n",
      "  Moved: breakbeat/processed_cassette/\n",
      "  Moved: donk/processed_vinyl/\n",
      "  Moved: italo_house/processed_cassette/\n",
      "  Moved: uk_garage/processed_cd/\n",
      "  Moved: dj_battle_tool/processed_cd/\n",
      "  Moved: tribal_freetekno/processed_vinyl/\n",
      "  Moved: breakbeat_hardcore/processed_cd/\n",
      "  Moved: breakbeat_happy_hardcore/processed_cassette/\n",
      "  Moved: ghetto_house/processed_vinyl/\n",
      "  Moved: breakbeat/processed_vinyl/\n",
      "  Moved: hiphop_breakbeat/processed_cassette/\n",
      "  Moved: speed_garage/processed_vinyl/\n",
      "  Moved: jungle_techno/processed_cassette/\n",
      "  Moved: hardcore_techno_jungle/processed_digital/\n",
      "  Moved: makina/processed_cd/\n",
      "  Moved: bassline/processed_other/\n",
      "  Moved: breakbeat_happy_hardcore/processed_cd/\n",
      "  Moved: breakbeat_hardcore/processed_dubplate/\n",
      "  Moved: baltimore_club/processed_digital/\n",
      "  Moved: footwork/processed_cassette/\n",
      "  Moved: gabber/processed_cd/\n",
      "  Moved: italo_house/processed_cd/\n",
      "  Moved: acid_house/processed_digital/\n",
      "  Moved: tribal/processed_cd/\n",
      "  Moved: dubstep/processed_other/\n",
      "  Moved: baltimore_club/processed_vinyl/\n",
      "  Moved: freetekno/processed_cd/\n",
      "  Moved: breakbeat/processed_dubplate/\n",
      "\n",
      "Reorganised into 42 style folders\n",
      "\n",
      "============================================================\n",
      "STEP 2: Create combined CSVs\n",
      "============================================================\n",
      "\n",
      "Scanning for processed CSVs...\n",
      "  Found: tribal_freetekno/cassette (tribal_freetekno_cassette_processed.csv)\n",
      "  Found: tribal_freetekno/vinyl (tribal_freetekno_vinyl_processed.csv)\n",
      "  Found: tribal_freetekno/digital (tribal_freetekno_digital_processed.csv)\n",
      "  Found: uk_funky/vinyl (uk_funky_vinyl_processed.csv)\n",
      "  Found: uk_funky/cd (uk_funky_cd_processed.csv)\n",
      "  Found: uk_funky/digital (uk_funky_digital_processed.csv)\n",
      "  Found: electro/vhs (electro_vhs_processed.csv)\n",
      "  Found: electro/cassette (electro_cassette_processed.csv)\n",
      "  Found: electro/vinyl (electro_vinyl_processed.csv)\n",
      "  Found: electro/dubplate (electro_dubplate_processed.csv)\n",
      "  Found: electro/cd (electro_cd_processed.csv)\n",
      "  Found: electro/other (electro_other_processed.csv)\n",
      "  Found: electro/digital (electro_digital_processed.csv)\n",
      "  Found: dj_battle_tool/vinyl (dj_battle_tool_vinyl_processed.csv)\n",
      "  Found: dj_battle_tool/cd (dj_battle_tool_cd_processed.csv)\n",
      "  Found: dj_battle_tool/digital (dj_battle_tool_digital_processed.csv)\n",
      "  Found: baltimore_club/vinyl (baltimore_club_vinyl_processed.csv)\n",
      "  Found: baltimore_club/cd (baltimore_club_cd_processed.csv)\n",
      "  Found: baltimore_club/digital (baltimore_club_digital_processed.csv)\n",
      "  Found: electro_funk/cassette (electro_funk_cassette_processed.csv)\n",
      "  Found: electro_funk/vinyl (electro_funk_vinyl_processed.csv)\n",
      "  Found: electro_funk/cd (electro_funk_cd_processed.csv)\n",
      "  Found: electro_funk/digital (electro_funk_digital_processed.csv)\n",
      "  Found: freetekno/cassette (freetekno_cassette_processed.csv)\n",
      "  Found: freetekno/vinyl (freetekno_vinyl_processed.csv)\n",
      "  Found: freetekno/cd (freetekno_cd_processed.csv)\n",
      "  Found: freetekno/digital (freetekno_digital_processed.csv)\n",
      "  Found: footwork/cassette (footwork_cassette_processed.csv)\n",
      "  Found: footwork/vinyl (footwork_vinyl_processed.csv)\n",
      "  Found: footwork/cd (footwork_cd_processed.csv)\n",
      "  Found: footwork/digital (footwork_digital_processed.csv)\n",
      "  Found: gabber/vhs (gabber_vhs_processed.csv)\n",
      "  Found: gabber/cassette (gabber_cassette_processed.csv)\n",
      "  Found: gabber/vinyl (gabber_vinyl_processed.csv)\n",
      "  Found: gabber/cd (gabber_cd_processed.csv)\n",
      "  Found: gabber/other (gabber_other_processed.csv)\n",
      "  Found: gabber/digital (gabber_digital_processed.csv)\n",
      "  Found: hardcore_jungle/cassette (hardcore_jungle_cassette_processed.csv)\n",
      "  Found: hardcore_jungle/vinyl (hardcore_jungle_vinyl_processed.csv)\n",
      "  Found: hardcore_jungle/dubplate (hardcore_jungle_dubplate_processed.csv)\n",
      "  Found: hardcore_jungle/cd (hardcore_jungle_cd_processed.csv)\n",
      "  Found: hardcore_jungle/digital (hardcore_jungle_digital_processed.csv)\n",
      "  Found: dnb/cassette (dnb_cassette_processed.csv)\n",
      "  Found: dnb/vinyl (dnb_vinyl_processed.csv)\n",
      "  Found: dnb/dubplate (dnb_dubplate_processed.csv)\n",
      "  Found: dnb/cd (dnb_cd_processed.csv)\n",
      "  Found: dnb/other (dnb_other_processed.csv)\n",
      "  Found: dnb/digital (dnb_digital_processed.csv)\n",
      "  Found: techno_future_jazz/vinyl (techno_future_jazz_vinyl_processed.csv)\n",
      "  Found: techno_future_jazz/cd (techno_future_jazz_cd_processed.csv)\n",
      "  Found: hiphop_breakbeat/cassette (hiphop_breakbeat_cassette_processed.csv)\n",
      "  Found: hiphop_breakbeat/vinyl (hiphop_breakbeat_vinyl_processed.csv)\n",
      "  Found: hiphop_breakbeat/cd (hiphop_breakbeat_cd_processed.csv)\n",
      "  Found: hiphop_breakbeat/digital (hiphop_breakbeat_digital_processed.csv)\n",
      "  Found: bleep/vinyl (bleep_vinyl_processed.csv)\n",
      "  Found: bleep/cd (bleep_cd_processed.csv)\n",
      "  Found: bleep/digital (bleep_digital_processed.csv)\n",
      "  Found: bassline/cassette (bassline_cassette_processed.csv)\n",
      "  Found: bassline/vinyl (bassline_vinyl_processed.csv)\n",
      "  Found: bassline/cd (bassline_cd_processed.csv)\n",
      "  Found: bassline/other (bassline_other_processed.csv)\n",
      "  Found: bassline/digital (bassline_digital_processed.csv)\n",
      "  Found: happy_hardcore/cassette (happy_hardcore_cassette_processed.csv)\n",
      "  Found: happy_hardcore/vinyl (happy_hardcore_vinyl_processed.csv)\n",
      "  Found: happy_hardcore/dubplate (happy_hardcore_dubplate_processed.csv)\n",
      "  Found: happy_hardcore/cd (happy_hardcore_cd_processed.csv)\n",
      "  Found: happy_hardcore/other (happy_hardcore_other_processed.csv)\n",
      "  Found: happy_hardcore/digital (happy_hardcore_digital_processed.csv)\n",
      "  Found: uk_garage/cassette (uk_garage_cassette_processed.csv)\n",
      "  Found: uk_garage/vinyl (uk_garage_vinyl_processed.csv)\n",
      "  Found: uk_garage/dubplate (uk_garage_dubplate_processed.csv)\n",
      "  Found: uk_garage/cd (uk_garage_cd_processed.csv)\n",
      "  Found: uk_garage/other (uk_garage_other_processed.csv)\n",
      "  Found: uk_garage/digital (uk_garage_digital_processed.csv)\n",
      "  Found: ghetto/cassette (ghetto_cassette_processed.csv)\n",
      "  Found: ghetto/vinyl (ghetto_vinyl_processed.csv)\n",
      "  Found: ghetto/cd (ghetto_cd_processed.csv)\n",
      "  Found: ghetto/digital (ghetto_digital_processed.csv)\n",
      "  Found: breakbeat_hardcore_jungle/cassette (breakbeat_hardcore_jungle_cassette_processed.csv)\n",
      "  Found: breakbeat_hardcore_jungle/vinyl (breakbeat_hardcore_jungle_vinyl_processed.csv)\n",
      "  Found: breakbeat_hardcore_jungle/cd (breakbeat_hardcore_jungle_cd_processed.csv)\n",
      "  Found: breakbeat_hardcore_jungle/digital (breakbeat_hardcore_jungle_digital_processed.csv)\n",
      "  Found: makina/cassette (makina_cassette_processed.csv)\n",
      "  Found: makina/vinyl (makina_vinyl_processed.csv)\n",
      "  Found: makina/cd (makina_cd_processed.csv)\n",
      "  Found: makina/digital (makina_digital_processed.csv)\n",
      "  Found: dub_jungle/vinyl (dub_jungle_vinyl_processed.csv)\n",
      "  Found: dub_jungle/cd (dub_jungle_cd_processed.csv)\n",
      "  Found: dub_jungle/digital (dub_jungle_digital_processed.csv)\n",
      "  Found: hardcore_acid/cassette (hardcore_acid_cassette_processed.csv)\n",
      "  Found: hardcore_acid/vinyl (hardcore_acid_vinyl_processed.csv)\n",
      "  Found: hardcore_acid/cd (hardcore_acid_cd_processed.csv)\n",
      "  Found: hardcore_acid/digital (hardcore_acid_digital_processed.csv)\n",
      "  Found: italo_house/cassette (italo_house_cassette_processed.csv)\n",
      "  Found: italo_house/vinyl (italo_house_vinyl_processed.csv)\n",
      "  Found: italo_house/cd (italo_house_cd_processed.csv)\n",
      "  Found: italo_house/digital (italo_house_digital_processed.csv)\n",
      "  Found: hardcore_techno_jungle/cassette (hardcore_techno_jungle_cassette_processed.csv)\n",
      "  Found: hardcore_techno_jungle/vinyl (hardcore_techno_jungle_vinyl_processed.csv)\n",
      "  Found: hardcore_techno_jungle/cd (hardcore_techno_jungle_cd_processed.csv)\n",
      "  Found: hardcore_techno_jungle/digital (hardcore_techno_jungle_digital_processed.csv)\n",
      "  Found: ghettotech/cassette (ghettotech_cassette_processed.csv)\n",
      "  Found: ghettotech/vinyl (ghettotech_vinyl_processed.csv)\n",
      "  Found: ghettotech/cd (ghettotech_cd_processed.csv)\n",
      "  Found: ghettotech/digital (ghettotech_digital_processed.csv)\n",
      "  Found: ghetto_house/cassette (ghetto_house_cassette_processed.csv)\n",
      "  Found: ghetto_house/vinyl (ghetto_house_vinyl_processed.csv)\n",
      "  Found: ghetto_house/cd (ghetto_house_cd_processed.csv)\n",
      "  Found: ghetto_house/digital (ghetto_house_digital_processed.csv)\n",
      "  Found: breakbeat_happy_hardcore/cassette (breakbeat_happy_hardcore_cassette_processed.csv)\n",
      "  Found: breakbeat_happy_hardcore/vinyl (breakbeat_happy_hardcore_vinyl_processed.csv)\n",
      "  Found: breakbeat_happy_hardcore/cd (breakbeat_happy_hardcore_cd_processed.csv)\n",
      "  Found: breakbeat_happy_hardcore/digital (breakbeat_happy_hardcore_digital_processed.csv)\n",
      "  Found: breakbeat_house/cassette (breakbeat_house_cassette_processed.csv)\n",
      "  Found: breakbeat_house/vinyl (breakbeat_house_vinyl_processed.csv)\n",
      "  Found: breakbeat_house/cd (breakbeat_house_cd_processed.csv)\n",
      "  Found: breakbeat_house/other (breakbeat_house_other_processed.csv)\n",
      "  Found: breakbeat_house/digital (breakbeat_house_digital_processed.csv)\n",
      "  Found: donk/vinyl (donk_vinyl_processed.csv)\n",
      "  Found: donk/cd (donk_cd_processed.csv)\n",
      "  Found: donk/digital (donk_digital_processed.csv)\n",
      "  Found: miami_bass/cassette (miami_bass_cassette_processed.csv)\n",
      "  Found: miami_bass/vinyl (miami_bass_vinyl_processed.csv)\n",
      "  Found: miami_bass/cd (miami_bass_cd_processed.csv)\n",
      "  Found: miami_bass/digital (miami_bass_digital_processed.csv)\n",
      "  Found: acid_house/cassette (acid_house_cassette_processed.csv)\n",
      "  Found: acid_house/vinyl (acid_house_vinyl_processed.csv)\n",
      "  Found: acid_house/cd (acid_house_cd_processed.csv)\n",
      "  Found: acid_house/other (acid_house_other_processed.csv)\n",
      "  Found: acid_house/digital (acid_house_digital_processed.csv)\n",
      "  Found: jungle/cassette (jungle_cassette_processed.csv)\n",
      "  Found: jungle/vinyl (jungle_vinyl_processed.csv)\n",
      "  Found: jungle/dubplate (jungle_dubplate_processed.csv)\n",
      "  Found: jungle/cd (jungle_cd_processed.csv)\n",
      "  Found: jungle/other (jungle_other_processed.csv)\n",
      "  Found: jungle/digital (jungle_digital_processed.csv)\n",
      "  Found: breakbeat_hardcore_happy_hardcore/cassette (breakbeat_hardcore_happy_hardcore_cassette_processed.csv)\n",
      "  Found: breakbeat_hardcore_happy_hardcore/vinyl (breakbeat_hardcore_happy_hardcore_vinyl_processed.csv)\n",
      "  Found: breakbeat_hardcore_happy_hardcore/cd (breakbeat_hardcore_happy_hardcore_cd_processed.csv)\n",
      "  Found: breakbeat_hardcore_happy_hardcore/digital (breakbeat_hardcore_happy_hardcore_digital_processed.csv)\n",
      "  Found: tribal/cassette (tribal_cassette_processed.csv)\n",
      "  Found: tribal/vinyl (tribal_vinyl_processed.csv)\n",
      "  Found: tribal/cd (tribal_cd_processed.csv)\n",
      "  Found: tribal/other (tribal_other_processed.csv)\n",
      "  Found: tribal/digital (tribal_digital_processed.csv)\n",
      "  Found: jungle_techno/cassette (jungle_techno_cassette_processed.csv)\n",
      "  Found: jungle_techno/vinyl (jungle_techno_vinyl_processed.csv)\n",
      "  Found: jungle_techno/cd (jungle_techno_cd_processed.csv)\n",
      "  Found: jungle_techno/digital (jungle_techno_digital_processed.csv)\n",
      "  Found: breakbeat_hardcore/cassette (breakbeat_hardcore_cassette_processed.csv)\n",
      "  Found: breakbeat_hardcore/vinyl (breakbeat_hardcore_vinyl_processed.csv)\n",
      "  Found: breakbeat_hardcore/dubplate (breakbeat_hardcore_dubplate_processed.csv)\n",
      "  Found: breakbeat_hardcore/cd (breakbeat_hardcore_cd_processed.csv)\n",
      "  Found: breakbeat_hardcore/other (breakbeat_hardcore_other_processed.csv)\n",
      "  Found: breakbeat_hardcore/digital (breakbeat_hardcore_digital_processed.csv)\n",
      "  Found: breakbeat/vhs (breakbeat_vhs_processed.csv)\n",
      "  Found: breakbeat/cassette (breakbeat_cassette_processed.csv)\n",
      "  Found: breakbeat/vinyl (breakbeat_vinyl_processed.csv)\n",
      "  Found: breakbeat/dubplate (breakbeat_dubplate_processed.csv)\n",
      "  Found: breakbeat/cd (breakbeat_cd_processed.csv)\n",
      "  Found: breakbeat/other (breakbeat_other_processed.csv)\n",
      "  Found: breakbeat/digital (breakbeat_digital_processed.csv)\n",
      "  Found: breakbeat_acid/cassette (breakbeat_acid_cassette_processed.csv)\n",
      "  Found: breakbeat_acid/vinyl (breakbeat_acid_vinyl_processed.csv)\n",
      "  Found: breakbeat_acid/cd (breakbeat_acid_cd_processed.csv)\n",
      "  Found: breakbeat_acid/digital (breakbeat_acid_digital_processed.csv)\n",
      "  Found: breakbeat_hardcore_techno/cassette (breakbeat_hardcore_techno_cassette_processed.csv)\n",
      "  Found: breakbeat_hardcore_techno/vinyl (breakbeat_hardcore_techno_vinyl_processed.csv)\n",
      "  Found: breakbeat_hardcore_techno/cd (breakbeat_hardcore_techno_cd_processed.csv)\n",
      "  Found: breakbeat_hardcore_techno/digital (breakbeat_hardcore_techno_digital_processed.csv)\n",
      "  Found: speed_garage/cassette (speed_garage_cassette_processed.csv)\n",
      "  Found: speed_garage/vinyl (speed_garage_vinyl_processed.csv)\n",
      "  Found: speed_garage/cd (speed_garage_cd_processed.csv)\n",
      "  Found: speed_garage/other (speed_garage_other_processed.csv)\n",
      "  Found: speed_garage/digital (speed_garage_digital_processed.csv)\n",
      "  Found: dubstep/cassette (dubstep_cassette_processed.csv)\n",
      "  Found: dubstep/vinyl (dubstep_vinyl_processed.csv)\n",
      "  Found: dubstep/cd (dubstep_cd_processed.csv)\n",
      "  Found: dubstep/other (dubstep_other_processed.csv)\n",
      "  Found: dubstep/digital (dubstep_digital_processed.csv)\n",
      "  Found: juke/cassette (juke_cassette_processed.csv)\n",
      "  Found: juke/vinyl (juke_vinyl_processed.csv)\n",
      "  Found: juke/cd (juke_cd_processed.csv)\n",
      "  Found: juke/digital (juke_digital_processed.csv)\n",
      "\n",
      "Creating combined CSVs...\n",
      "  all_cassette_processed.csv: 38463 releases\n",
      "  all_vinyl_processed.csv: 226746 releases\n",
      "  all_digital_processed.csv: 283173 releases\n",
      "  all_cd_processed.csv: 157835 releases\n",
      "  all_vhs_processed.csv: 622 releases\n",
      "  all_dubplate_processed.csv: 3477 releases\n",
      "  all_other_processed.csv: 6498 releases\n",
      "  all_formats_processed.csv: 716814 releases\n",
      "\n",
      "Combined CSVs saved to: /Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/_combined\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reorganise processed folders into style-based structure AND create combined CSVs.\n",
    "\n",
    "FROM:\n",
    "processed_acid_house_vinyl/\n",
    "processed_acid_house_cd/\n",
    "processed_jungle_vinyl/\n",
    "\n",
    "TO:\n",
    "_combined/\n",
    "    all_vinyl_processed.csv      (all styles, vinyl only)\n",
    "    all_cd_processed.csv         (all styles, cd only)\n",
    "    all_formats_processed.csv    (everything)\n",
    "acid_house/\n",
    "    processed_vinyl/\n",
    "    processed_cd/\n",
    "jungle/\n",
    "    processed_vinyl/\n",
    "    processed_cd/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# === EDIT THIS PATH ===\n",
    "PROCESSING_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS\"\n",
    "\n",
    "\n",
    "def reorganise_folders(base_folder):\n",
    "    \"\"\"Reorganise flat structure into nested style folders.\"\"\"\n",
    "    \n",
    "    # Get all processed_ folders\n",
    "    folders = [f for f in os.listdir(base_folder) \n",
    "               if f.startswith('processed_') and os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    print(f\"Found {len(folders)} folders to reorganise\")\n",
    "    \n",
    "    moves = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        # e.g., \"processed_acid_house_vinyl\" -> style=\"acid_house\", format=\"vinyl\"\n",
    "        parts = folder.replace('processed_', '').rsplit('_', 1)\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            style = parts[0]\n",
    "            fmt = parts[1]\n",
    "        else:\n",
    "            style = parts[0]\n",
    "            fmt = 'unknown'\n",
    "        \n",
    "        moves.append({\n",
    "            'old_path': os.path.join(base_folder, folder),\n",
    "            'style': style,\n",
    "            'format': fmt,\n",
    "            'new_style_folder': os.path.join(base_folder, style),\n",
    "            'new_path': os.path.join(base_folder, style, f\"processed_{fmt}\")\n",
    "        })\n",
    "    \n",
    "    # Preview moves\n",
    "    print(\"\\nPreview of reorganisation:\")\n",
    "    print(\"-\" * 60)\n",
    "    styles = sorted(set(m['style'] for m in moves))\n",
    "    for style in styles[:10]:\n",
    "        formats = [m['format'] for m in moves if m['style'] == style]\n",
    "        print(f\"  {style}/: {', '.join(formats)}\")\n",
    "    if len(styles) > 10:\n",
    "        print(f\"  ... and {len(styles) - 10} more styles\")\n",
    "    \n",
    "    # Confirm\n",
    "    response = input(\"\\nProceed with reorganisation? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Cancelled.\")\n",
    "        return None\n",
    "    \n",
    "    # Execute moves\n",
    "    for m in moves:\n",
    "        os.makedirs(m['new_style_folder'], exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(m['new_path']):\n",
    "            print(f\"  Skipping {m['old_path']} - destination exists\")\n",
    "        else:\n",
    "            shutil.move(m['old_path'], m['new_path'])\n",
    "            print(f\"  Moved: {m['style']}/processed_{m['format']}/\")\n",
    "    \n",
    "    print(f\"\\nReorganised into {len(styles)} style folders\")\n",
    "    return moves\n",
    "\n",
    "\n",
    "def create_combined_csvs(base_folder):\n",
    "    \"\"\"Create combined CSVs per format and one with all formats.\"\"\"\n",
    "    \n",
    "    combined_folder = os.path.join(base_folder, \"_combined\")\n",
    "    os.makedirs(combined_folder, exist_ok=True)\n",
    "    \n",
    "    # Collect all CSVs by format\n",
    "    format_dfs = {}  # format -> list of dataframes\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(\"\\nScanning for processed CSVs...\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        # Skip the _combined folder itself\n",
    "        if '_combined' in root:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith('_processed.csv'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                \n",
    "                # Determine format from folder name\n",
    "                parent = os.path.basename(root)\n",
    "                if parent.startswith('processed_'):\n",
    "                    fmt = parent.replace('processed_', '')\n",
    "                else:\n",
    "                    fmt = 'unknown'\n",
    "                \n",
    "                # Determine style from grandparent folder\n",
    "                grandparent = os.path.basename(os.path.dirname(root))\n",
    "                style = grandparent if grandparent != os.path.basename(base_folder) else 'unknown'\n",
    "                \n",
    "                print(f\"  Found: {style}/{fmt} ({file})\")\n",
    "                \n",
    "                # Load CSV\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    df['_source_style'] = style\n",
    "                    df['_source_format'] = fmt\n",
    "                    \n",
    "                    if fmt not in format_dfs:\n",
    "                        format_dfs[fmt] = []\n",
    "                    format_dfs[fmt].append(df)\n",
    "                    all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error loading {filepath}: {e}\")\n",
    "    \n",
    "    # Create combined CSVs per format\n",
    "    print(\"\\nCreating combined CSVs...\")\n",
    "    \n",
    "    for fmt, dfs in format_dfs.items():\n",
    "        if dfs:\n",
    "            combined = pd.concat(dfs, ignore_index=True)\n",
    "            output_path = os.path.join(combined_folder, f\"all_{fmt}_processed.csv\")\n",
    "            combined.to_csv(output_path, index=False)\n",
    "            print(f\"  all_{fmt}_processed.csv: {len(combined)} releases\")\n",
    "    \n",
    "    # Create combined CSV with all formats\n",
    "    if all_dfs:\n",
    "        combined_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        output_path = os.path.join(combined_folder, \"all_formats_processed.csv\")\n",
    "        combined_all.to_csv(output_path, index=False)\n",
    "        print(f\"  all_formats_processed.csv: {len(combined_all)} releases\")\n",
    "    \n",
    "    print(f\"\\nCombined CSVs saved to: {combined_folder}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Reorganise folders by style\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    moves = reorganise_folders(PROCESSING_FOLDER)\n",
    "    \n",
    "    if moves is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Create combined CSVs\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    response = input(\"\\nCreate combined CSVs? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        create_combined_csvs(PROCESSING_FOLDER)\n",
    "    else:\n",
    "        print(\"Skipped combined CSV creation.\")\n",
    "    \n",
    "    print(\"\\nAll done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9509773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Deduplicating: all_formats_processed.csv\n",
      "============================================================\n",
      "Original: 716814 releases\n",
      "After dedup: 573239 releases\n",
      "Removed: 143575 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_vhs_processed.csv\n",
      "============================================================\n",
      "Original: 622 releases\n",
      "After dedup: 596 releases\n",
      "Removed: 26 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_digital_processed.csv\n",
      "============================================================\n",
      "Original: 283173 releases\n",
      "After dedup: 237325 releases\n",
      "Removed: 45848 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_cd_processed.csv\n",
      "============================================================\n",
      "Original: 157835 releases\n",
      "After dedup: 124142 releases\n",
      "Removed: 33693 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_vinyl_processed.csv\n",
      "============================================================\n",
      "Original: 226746 releases\n",
      "After dedup: 176726 releases\n",
      "Removed: 50020 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_other_processed.csv\n",
      "============================================================\n",
      "Original: 6498 releases\n",
      "After dedup: 5101 releases\n",
      "Removed: 1397 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_dubplate_processed.csv\n",
      "============================================================\n",
      "Original: 3477 releases\n",
      "After dedup: 2698 releases\n",
      "Removed: 779 duplicates\n",
      "\n",
      "============================================================\n",
      "Deduplicating: all_cassette_processed.csv\n",
      "============================================================\n",
      "Original: 38463 releases\n",
      "After dedup: 26651 releases\n",
      "Removed: 11812 duplicates\n",
      "\n",
      "============================================================\n",
      "COMPLETE\n",
      "Files processed: 8\n",
      "Total duplicates removed: 287150\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deduplicate combined CSVs.\n",
    "\n",
    "Removes duplicates based on same title + same media type.\n",
    "Does NOT modify alt_versions columns (already populated).\n",
    "\n",
    "Run this AFTER reorganise_and_combine.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === EDIT THIS PATH ===\n",
    "COMBINED_FOLDER = \"/Users/benhasy/Documents/UNI/Foundations of AI/api/PROCESSING_CSVS/_combined\"\n",
    "\n",
    "\n",
    "def is_white_label(format_str):\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'white label' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def is_promo(format_str):\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'promo' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def is_test_pressing(format_str):\n",
    "    if pd.isna(format_str):\n",
    "        return False\n",
    "    return 'test pressing' in str(format_str).lower()\n",
    "\n",
    "\n",
    "def get_priority_score(row):\n",
    "    \"\"\"Higher score = better candidate to keep.\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Prefer non-white-label\n",
    "    if not is_white_label(row.get('format', '')):\n",
    "        score += 100\n",
    "    \n",
    "    # Prefer non-promo\n",
    "    if not is_promo(row.get('format', '')):\n",
    "        score += 50\n",
    "    \n",
    "    # Prefer non-test-pressing\n",
    "    if not is_test_pressing(row.get('format', '')):\n",
    "        score += 50\n",
    "    \n",
    "    # Has real label bonus\n",
    "    label = row.get('label', '')\n",
    "    if pd.notna(label) and 'not on label' not in str(label).lower():\n",
    "        score += 80\n",
    "    \n",
    "    # Earlier year bonus\n",
    "    year = row.get('oldest_year', row.get('year', 0))\n",
    "    if pd.notna(year) and year > 0:\n",
    "        score += (2030 - int(year))\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def deduplicate_csv(filepath):\n",
    "    \"\"\"\n",
    "    Remove duplicates from a combined CSV.\n",
    "    Same title + same primary_format = duplicate.\n",
    "    Keep the best version based on priority score.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Deduplicating: {filename}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(filepath)\n",
    "    original_count = len(df)\n",
    "    print(f\"Original: {original_count} releases\")\n",
    "    \n",
    "    # Create matching key\n",
    "    df['title_lower'] = df['title'].str.lower().str.strip()\n",
    "    \n",
    "    # Calculate priority scores\n",
    "    df['_priority'] = df.apply(get_priority_score, axis=1)\n",
    "    \n",
    "    # For format-specific files (all_vinyl, all_cd, etc.), just dedupe by title\n",
    "    # For all_formats, dedupe by title + primary_format\n",
    "    if 'all_formats' in filename:\n",
    "        # Group by title AND format\n",
    "        df = df.sort_values('_priority', ascending=False)\n",
    "        df = df.drop_duplicates(subset=['title_lower', 'primary_format'], keep='first')\n",
    "    else:\n",
    "        # Group by title only (all same format anyway)\n",
    "        df = df.sort_values('_priority', ascending=False)\n",
    "        df = df.drop_duplicates(subset=['title_lower'], keep='first')\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    df = df.drop(columns=['title_lower', '_priority'])\n",
    "    \n",
    "    # Save back\n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    removed = original_count - len(df)\n",
    "    print(f\"After dedup: {len(df)} releases\")\n",
    "    print(f\"Removed: {removed} duplicates\")\n",
    "    \n",
    "    return removed\n",
    "\n",
    "\n",
    "def deduplicate_all(combined_folder):\n",
    "    \"\"\"Deduplicate all combined CSVs.\"\"\"\n",
    "    \n",
    "    total_removed = 0\n",
    "    files_processed = 0\n",
    "    \n",
    "    for file in os.listdir(combined_folder):\n",
    "        if file.endswith('_processed.csv'):\n",
    "            filepath = os.path.join(combined_folder, file)\n",
    "            removed = deduplicate_csv(filepath)\n",
    "            total_removed += removed\n",
    "            files_processed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE\")\n",
    "    print(f\"Files processed: {files_processed}\")\n",
    "    print(f\"Total duplicates removed: {total_removed}\")\n",
    "    print('='*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deduplicate_all(COMBINED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9351b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DISCOGS RECOMMENDATION ENGINE v2 - NOTEBOOK VERSION\n",
    "Combines 4 methods for release recommendations:\n",
    "1. Rule-Based (0.35) - Artist, label, year, country, catno prefix\n",
    "2. Knowledge Graph (0.30) - Hop distance in artist-label graph\n",
    "3. Clustering (0.20) - Same cluster bonus\n",
    "4. Jaccard (0.15) - Style overlap\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Optional: fuzzy matching\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    FUZZY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FUZZY_AVAILABLE = False\n",
    "    print(\"Warning: rapidfuzz not installed. Using exact matching only.\")\n",
    "    print(\"Install with: pip install rapidfuzz\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURABLE PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Method weights (must sum to 1.0)\n",
    "WEIGHT_RULE_BASED = 0.35\n",
    "WEIGHT_KNOWLEDGE_GRAPH = 0.30\n",
    "WEIGHT_CLUSTERING = 0.20\n",
    "WEIGHT_JACCARD = 0.15\n",
    "\n",
    "# Rule-based parameters\n",
    "YEAR_TOLERANCE_HIGH = 2      # 2 years = high score\n",
    "YEAR_TOLERANCE_LOW = 4       # 4 years = low score\n",
    "ARTIST_EXACT_MATCH = 1.0     # Exact artist match\n",
    "ARTIST_CLOSE_MATCH = 0.7     # 90-99% fuzzy match\n",
    "ARTIST_PARTIAL_MATCH = 0.4   # 70-89% fuzzy match\n",
    "ARTIST_LOOSE_MATCH = 0.1     # 50-69% fuzzy match\n",
    "LABEL_MATCH_SCORE = 0.8      # Same label\n",
    "YEAR_HIGH_SCORE = 0.6        # Within 2 years\n",
    "YEAR_LOW_SCORE = 0.3         # Within 4 years\n",
    "YEAR_NO_MATCH_SCORE = 0.0    # Outside 4 years\n",
    "COUNTRY_MATCH_SCORE = 0.2    # Same country\n",
    "CATNO_PREFIX_SCORE = 0.5     # Same catalogue number prefix\n",
    "\n",
    "# Knowledge graph parameters\n",
    "HOP_1_COLLAB_SCORE = 1.0     # Direct collaboration\n",
    "HOP_1_LABEL_SCORE = 0.8      # Same label (1 hop)\n",
    "HOP_2_SCORE = 0.4            # 2 hops away\n",
    "\n",
    "# Clustering parameters\n",
    "SAME_CLUSTER_SCORE = 1.0     # Same cluster\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ARTIST PARSING\n",
    "# ============================================================\n",
    "\n",
    "def parse_artists(artist_string):\n",
    "    \"\"\"\n",
    "    Parse multiple artists from artist string.\n",
    "    Splits on &, feat., ft., vs, x, and, comma.\n",
    "    Fuzzy matching handles variations later.\n",
    "    \"\"\"\n",
    "    if pd.isna(artist_string) or artist_string == '':\n",
    "        return []\n",
    "    \n",
    "    artist_string = str(artist_string).strip()\n",
    "    \n",
    "    separators = [\n",
    "        ' feat. ', ' Feat. ', ' ft. ', ' Ft. ',\n",
    "        ' vs ', ' vs. ', ' Vs ', ' Vs. ',\n",
    "        ' x ', ' X ',\n",
    "        ' & ',\n",
    "        ' and ',\n",
    "        ', ',\n",
    "        ','\n",
    "    ]\n",
    "    \n",
    "    artists = [artist_string]\n",
    "    for sep in separators:\n",
    "        new_artists = []\n",
    "        for a in artists:\n",
    "            new_artists.extend(a.split(sep))\n",
    "        artists = new_artists\n",
    "    \n",
    "    cleaned = []\n",
    "    for a in artists:\n",
    "        a = a.strip()\n",
    "        if a and len(a) > 1:\n",
    "            cleaned.append(a)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def get_catno_prefix(catno):\n",
    "    \"\"\"Extract prefix from catalogue number (letters only).\"\"\"\n",
    "    if pd.isna(catno) or catno == '' or catno == 'N0N3 - 000':\n",
    "        return None\n",
    "    catno = str(catno).strip()\n",
    "    match = re.match(r'^([A-Za-z]+)', catno)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RECOMMENDATION ENGINE CLASS\n",
    "# ============================================================\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"\n",
    "    Combined recommendation engine using 4 methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, graph_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the engine.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to processed CSV (with cluster column)\n",
    "            graph_path: Path to knowledge graph pickle file (optional)\n",
    "        \"\"\"\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(self.df)} releases\")\n",
    "        \n",
    "        if graph_path:\n",
    "            print(f\"Loading knowledge graph from {graph_path}...\")\n",
    "            with open(graph_path, 'rb') as f:\n",
    "                self.graph = pickle.load(f)\n",
    "            print(f\"Graph: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\")\n",
    "        else:\n",
    "            self.graph = None\n",
    "            print(\"No knowledge graph provided - graph-based scoring disabled\")\n",
    "        \n",
    "        if 'artists_list' not in self.df.columns:\n",
    "            print(\"Parsing artists...\")\n",
    "            self.df['artists_list'] = self.df['parsed_artist'].apply(parse_artists)\n",
    "        else:\n",
    "            if len(self.df) > 0 and isinstance(self.df['artists_list'].iloc[0], str):\n",
    "                self.df['artists_list'] = self.df['artists_list'].apply(\n",
    "                    lambda x: eval(x) if pd.notna(x) and x != '' else []\n",
    "                )\n",
    "        \n",
    "        if 'oldest_year' not in self.df.columns:\n",
    "            self.df['oldest_year'] = self.df['year']\n",
    "        \n",
    "        self._build_artist_index()\n",
    "        print(\"Engine ready!\")\n",
    "    \n",
    "    def _build_artist_index(self):\n",
    "        \"\"\"Build index for fast artist lookup.\"\"\"\n",
    "        self.artist_to_releases = defaultdict(list)\n",
    "        for idx, row in self.df.iterrows():\n",
    "            artists = row['artists_list'] if isinstance(row['artists_list'], list) else []\n",
    "            for artist in artists:\n",
    "                self.artist_to_releases[artist.lower()].append(idx)\n",
    "    \n",
    "    # ========================================\n",
    "    # METHOD 1: RULE-BASED SCORING\n",
    "    # ========================================\n",
    "    \n",
    "    def _artist_similarity(self, artists1, artists2):\n",
    "        if not artists1 or not artists2:\n",
    "            return 0, []\n",
    "        \n",
    "        best_score = 0\n",
    "        matched_artists = []\n",
    "        \n",
    "        for a1 in artists1:\n",
    "            for a2 in artists2:\n",
    "                a1_lower = a1.lower().strip()\n",
    "                a2_lower = a2.lower().strip()\n",
    "                \n",
    "                if a1_lower == a2_lower:\n",
    "                    score = ARTIST_EXACT_MATCH\n",
    "                elif FUZZY_AVAILABLE:\n",
    "                    ratio = fuzz.ratio(a1_lower, a2_lower)\n",
    "                    if ratio >= 90:\n",
    "                        score = ARTIST_CLOSE_MATCH\n",
    "                    elif ratio >= 70:\n",
    "                        score = ARTIST_PARTIAL_MATCH\n",
    "                    elif ratio >= 50:\n",
    "                        score = ARTIST_LOOSE_MATCH\n",
    "                    else:\n",
    "                        score = 0\n",
    "                else:\n",
    "                    if a1_lower in a2_lower or a2_lower in a1_lower:\n",
    "                        score = ARTIST_PARTIAL_MATCH\n",
    "                    else:\n",
    "                        score = 0\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    matched_artists = [(a1, a2, score)]\n",
    "        \n",
    "        return best_score, matched_artists\n",
    "    \n",
    "    def _label_similarity(self, labels1, labels2):\n",
    "        if pd.isna(labels1) or pd.isna(labels2):\n",
    "            return 0, []\n",
    "        \n",
    "        set1 = set(l.strip().lower() for l in str(labels1).split(',') \n",
    "                   if 'not on label' not in l.lower())\n",
    "        set2 = set(l.strip().lower() for l in str(labels2).split(',') \n",
    "                   if 'not on label' not in l.lower())\n",
    "        \n",
    "        matches = set1 & set2\n",
    "        if matches:\n",
    "            return LABEL_MATCH_SCORE, list(matches)\n",
    "        return 0, []\n",
    "    \n",
    "    def _year_similarity(self, year1, year2):\n",
    "        if pd.isna(year1) or pd.isna(year2) or year1 == 0 or year2 == 0:\n",
    "            return YEAR_NO_MATCH_SCORE\n",
    "        \n",
    "        diff = abs(int(year1) - int(year2))\n",
    "        if diff <= YEAR_TOLERANCE_HIGH:\n",
    "            return YEAR_HIGH_SCORE\n",
    "        elif diff <= YEAR_TOLERANCE_LOW:\n",
    "            return YEAR_LOW_SCORE\n",
    "        return YEAR_NO_MATCH_SCORE\n",
    "    \n",
    "    def _catno_similarity(self, catno1, catno2):\n",
    "        prefix1 = get_catno_prefix(catno1)\n",
    "        prefix2 = get_catno_prefix(catno2)\n",
    "        \n",
    "        if prefix1 and prefix2 and prefix1 == prefix2:\n",
    "            return CATNO_PREFIX_SCORE\n",
    "        return 0\n",
    "    \n",
    "    def _rule_based_score(self, seed_row, candidate_row):\n",
    "        score = 0\n",
    "        reasons = []\n",
    "        \n",
    "        seed_artists = seed_row['artists_list'] if isinstance(seed_row['artists_list'], list) else []\n",
    "        cand_artists = candidate_row['artists_list'] if isinstance(candidate_row['artists_list'], list) else []\n",
    "        \n",
    "        artist_score, artist_matches = self._artist_similarity(seed_artists, cand_artists)\n",
    "        if artist_score > 0:\n",
    "            score += artist_score\n",
    "            if artist_score == ARTIST_EXACT_MATCH:\n",
    "                reasons.append(f\"same artist\")\n",
    "            elif artist_score >= ARTIST_CLOSE_MATCH:\n",
    "                reasons.append(f\"similar artist (close match)\")\n",
    "            elif artist_score >= ARTIST_PARTIAL_MATCH:\n",
    "                reasons.append(f\"similar artist (partial match)\")\n",
    "            else:\n",
    "                reasons.append(f\"similar artist (loose match)\")\n",
    "        \n",
    "        label_score, label_matches = self._label_similarity(seed_row['label'], candidate_row['label'])\n",
    "        if label_score > 0:\n",
    "            score += label_score\n",
    "            label_name = label_matches[0][:30] + '...' if len(label_matches[0]) > 30 else label_matches[0]\n",
    "            reasons.append(f\"same label ({label_name})\")\n",
    "        \n",
    "        seed_year = seed_row.get('oldest_year', seed_row['year'])\n",
    "        cand_year = candidate_row.get('oldest_year', candidate_row['year'])\n",
    "        year_score = self._year_similarity(seed_year, cand_year)\n",
    "        if year_score > 0:\n",
    "            score += year_score\n",
    "            if year_score == YEAR_HIGH_SCORE:\n",
    "                reasons.append(f\"same era ({int(cand_year)})\")\n",
    "            else:\n",
    "                reasons.append(f\"similar era ({int(cand_year)})\")\n",
    "        \n",
    "        if pd.notna(seed_row['country']) and pd.notna(candidate_row['country']):\n",
    "            if seed_row['country'] == candidate_row['country']:\n",
    "                score += COUNTRY_MATCH_SCORE\n",
    "                reasons.append(f\"same country ({candidate_row['country']})\")\n",
    "        \n",
    "        catno_score = self._catno_similarity(\n",
    "            seed_row.get('catno_cleaned', seed_row.get('catno', '')),\n",
    "            candidate_row.get('catno_cleaned', candidate_row.get('catno', ''))\n",
    "        )\n",
    "        if catno_score > 0:\n",
    "            score += catno_score\n",
    "            reasons.append(\"same catalogue series\")\n",
    "        \n",
    "        max_possible = ARTIST_EXACT_MATCH + LABEL_MATCH_SCORE + YEAR_HIGH_SCORE + COUNTRY_MATCH_SCORE + CATNO_PREFIX_SCORE\n",
    "        normalised = score / max_possible\n",
    "        \n",
    "        return normalised, reasons\n",
    "    \n",
    "    # ========================================\n",
    "    # METHOD 2: KNOWLEDGE GRAPH SCORING\n",
    "    # ========================================\n",
    "    \n",
    "    def _knowledge_graph_score(self, seed_row, candidate_row):\n",
    "        if self.graph is None:\n",
    "            return 0, []\n",
    "        \n",
    "        seed_artists = seed_row['artists_list'] if isinstance(seed_row['artists_list'], list) else []\n",
    "        cand_artists = candidate_row['artists_list'] if isinstance(candidate_row['artists_list'], list) else []\n",
    "        \n",
    "        if not seed_artists or not cand_artists:\n",
    "            return 0, []\n",
    "        \n",
    "        best_score = 0\n",
    "        best_reason = []\n",
    "        \n",
    "        for s_artist in seed_artists:\n",
    "            if s_artist not in self.graph:\n",
    "                continue\n",
    "            \n",
    "            for c_artist in cand_artists:\n",
    "                if c_artist not in self.graph:\n",
    "                    continue\n",
    "                \n",
    "                if s_artist.lower() == c_artist.lower():\n",
    "                    continue\n",
    "                \n",
    "                if self.graph.has_edge(s_artist, c_artist):\n",
    "                    edge_data = self.graph[s_artist][c_artist]\n",
    "                    relationship = edge_data.get('relationship', 'connected')\n",
    "                    \n",
    "                    if relationship == 'collaborated_with':\n",
    "                        if HOP_1_COLLAB_SCORE > best_score:\n",
    "                            best_score = HOP_1_COLLAB_SCORE\n",
    "                            best_reason = [f\"collaborated ({s_artist}  {c_artist})\"]\n",
    "                    elif relationship == 'same_label':\n",
    "                        if HOP_1_LABEL_SCORE > best_score:\n",
    "                            best_score = HOP_1_LABEL_SCORE\n",
    "                            via_label = edge_data.get('via_label', 'shared label')\n",
    "                            via_label = via_label[:25] + '...' if len(via_label) > 25 else via_label\n",
    "                            best_reason = [f\"same label network ({via_label})\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(self.graph, s_artist, c_artist)\n",
    "                        if len(path) == 3:\n",
    "                            if HOP_2_SCORE > best_score:\n",
    "                                best_score = HOP_2_SCORE\n",
    "                                middle = path[1][:20] + '...' if len(path[1]) > 20 else path[1]\n",
    "                                best_reason = [f\"connected via {middle}\"]\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        pass\n",
    "        \n",
    "        return best_score, best_reason\n",
    "    \n",
    "    # ========================================\n",
    "    # METHOD 3: CLUSTERING SCORE\n",
    "    # ========================================\n",
    "    \n",
    "    def _clustering_score(self, seed_row, candidate_row):\n",
    "        if 'cluster' not in seed_row or 'cluster' not in candidate_row:\n",
    "            return 0, []\n",
    "        \n",
    "        if pd.isna(seed_row['cluster']) or pd.isna(candidate_row['cluster']):\n",
    "            return 0, []\n",
    "        \n",
    "        if int(seed_row['cluster']) == int(candidate_row['cluster']):\n",
    "            return SAME_CLUSTER_SCORE, [f\"same cluster (#{int(candidate_row['cluster'])})\"]\n",
    "        \n",
    "        return 0, []\n",
    "    \n",
    "    # ========================================\n",
    "    # METHOD 4: JACCARD SIMILARITY\n",
    "    # ========================================\n",
    "    \n",
    "    def _jaccard_score(self, seed_row, candidate_row):\n",
    "        seed_styles = set()\n",
    "        cand_styles = set()\n",
    "        \n",
    "        if pd.notna(seed_row['style']):\n",
    "            seed_styles = set(s.strip().lower() for s in str(seed_row['style']).split(','))\n",
    "        if pd.notna(candidate_row['style']):\n",
    "            cand_styles = set(s.strip().lower() for s in str(candidate_row['style']).split(','))\n",
    "        \n",
    "        if not seed_styles or not cand_styles:\n",
    "            return 0, []\n",
    "        \n",
    "        intersection = seed_styles & cand_styles\n",
    "        union = seed_styles | cand_styles\n",
    "        \n",
    "        jaccard = len(intersection) / len(union) if union else 0\n",
    "        \n",
    "        if jaccard > 0:\n",
    "            return jaccard, [f\"style match ({jaccard:.0%})\"]\n",
    "        \n",
    "        return 0, []\n",
    "    \n",
    "    # ========================================\n",
    "    # COMBINED SCORING\n",
    "    # ========================================\n",
    "    \n",
    "    def _combined_score(self, seed_row, candidate_row):\n",
    "        scores = {}\n",
    "        all_reasons = []\n",
    "        \n",
    "        scores['rule_based'], reasons = self._rule_based_score(seed_row, candidate_row)\n",
    "        all_reasons.extend(reasons)\n",
    "        \n",
    "        scores['knowledge_graph'], reasons = self._knowledge_graph_score(seed_row, candidate_row)\n",
    "        all_reasons.extend(reasons)\n",
    "        \n",
    "        scores['clustering'], reasons = self._clustering_score(seed_row, candidate_row)\n",
    "        all_reasons.extend(reasons)\n",
    "        \n",
    "        scores['jaccard'], reasons = self._jaccard_score(seed_row, candidate_row)\n",
    "        all_reasons.extend(reasons)\n",
    "        \n",
    "        final_score = (\n",
    "            scores['rule_based'] * WEIGHT_RULE_BASED +\n",
    "            scores['knowledge_graph'] * WEIGHT_KNOWLEDGE_GRAPH +\n",
    "            scores['clustering'] * WEIGHT_CLUSTERING +\n",
    "            scores['jaccard'] * WEIGHT_JACCARD\n",
    "        )\n",
    "        \n",
    "        return final_score, scores, all_reasons\n",
    "    \n",
    "    # ========================================\n",
    "    # MAIN RECOMMENDATION FUNCTION\n",
    "    # ========================================\n",
    "    \n",
    "    def get_recommendations(self, seed_id, n=50, exclude_same_artist=False):\n",
    "        \"\"\"\n",
    "        Get recommendations for a release.\n",
    "        \n",
    "        Args:\n",
    "            seed_id: Release ID (from 'id' column) or DataFrame index\n",
    "            n: Number of recommendations (default 50)\n",
    "            exclude_same_artist: If True, exclude releases by same artist\n",
    "        \n",
    "        Returns:\n",
    "            List of dicts with 'row', 'score', 'breakdown', 'reasons'\n",
    "            Top 3 by score, remaining shuffled randomly\n",
    "        \"\"\"\n",
    "        if isinstance(seed_id, (int, float)) and seed_id in self.df['id'].values:\n",
    "            seed_row = self.df[self.df['id'] == seed_id].iloc[0]\n",
    "        elif isinstance(seed_id, (int, float)) and seed_id < len(self.df):\n",
    "            seed_row = self.df.iloc[int(seed_id)]\n",
    "        else:\n",
    "            matches = self.df[self.df['id'] == seed_id]\n",
    "            if len(matches) > 0:\n",
    "                seed_row = matches.iloc[0]\n",
    "            else:\n",
    "                print(f\"Could not find release with id: {seed_id}\")\n",
    "                return []\n",
    "        \n",
    "        seed_idx = seed_row.name\n",
    "        seed_artists = set(a.lower() for a in (seed_row['artists_list'] if isinstance(seed_row['artists_list'], list) else []))\n",
    "        \n",
    "        print(f\"\\nFinding recommendations for: {seed_row['title']}\")\n",
    "        print(f\"Artists: {seed_row['parsed_artist']}\")\n",
    "        print(f\"Label: {str(seed_row['label'])[:50]}...\" if pd.notna(seed_row['label']) and len(str(seed_row['label'])) > 50 else f\"Label: {seed_row['label']}\")\n",
    "        print(f\"Year: {seed_row.get('oldest_year', seed_row['year'])}\")\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        for idx, candidate_row in self.df.iterrows():\n",
    "            if idx == seed_idx:\n",
    "                continue\n",
    "            \n",
    "            if exclude_same_artist:\n",
    "                cand_artists = set(a.lower() for a in (candidate_row['artists_list'] if isinstance(candidate_row['artists_list'], list) else []))\n",
    "                if seed_artists & cand_artists:\n",
    "                    continue\n",
    "            \n",
    "            final_score, breakdown, reasons = self._combined_score(seed_row, candidate_row)\n",
    "            \n",
    "            if final_score > 0:\n",
    "                candidates.append({\n",
    "                    'row': candidate_row,\n",
    "                    'score': final_score,\n",
    "                    'breakdown': breakdown,\n",
    "                    'reasons': reasons\n",
    "                })\n",
    "        \n",
    "        candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "        top_candidates = candidates[:max(n, 50)]\n",
    "        \n",
    "        if len(top_candidates) == 0:\n",
    "            print(\"No recommendations found!\")\n",
    "            return []\n",
    "        \n",
    "        top_3 = top_candidates[:3]\n",
    "        rest = top_candidates[3:n]\n",
    "        random.shuffle(rest)\n",
    "        \n",
    "        final_recommendations = top_3 + rest\n",
    "        print(f\"Found {len(final_recommendations)} recommendations\")\n",
    "        \n",
    "        return final_recommendations\n",
    "    \n",
    "    def search_releases(self, query):\n",
    "        \"\"\"Search for releases by title or artist. Case-insensitive.\"\"\"\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        matches = self.df[\n",
    "            self.df['title'].str.lower().str.contains(query_lower, na=False) |\n",
    "            self.df['parsed_artist'].str.lower().str.contains(query_lower, na=False)\n",
    "        ]\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def filter_releases(self, year_min=None, year_max=None, style=None, country=None, label=None):\n",
    "        \"\"\"Filter releases by various criteria.\"\"\"\n",
    "        filtered = self.df.copy()\n",
    "        \n",
    "        if year_min is not None:\n",
    "            filtered = filtered[filtered['oldest_year'] >= year_min]\n",
    "        \n",
    "        if year_max is not None:\n",
    "            filtered = filtered[filtered['oldest_year'] <= year_max]\n",
    "        \n",
    "        if style is not None:\n",
    "            filtered = filtered[filtered['style'].str.lower().str.contains(style.lower(), na=False)]\n",
    "        \n",
    "        if country is not None:\n",
    "            filtered = filtered[filtered['country'].str.lower() == country.lower()]\n",
    "        \n",
    "        if label is not None:\n",
    "            filtered = filtered[filtered['label'].str.lower().str.contains(label.lower(), na=False)]\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def print_recommendations(self, recommendations, n=10):\n",
    "        \"\"\"Pretty print recommendations.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOP {min(n, len(recommendations))} RECOMMENDATIONS\")\n",
    "        print('='*60)\n",
    "        \n",
    "        for i, rec in enumerate(recommendations[:n], 1):\n",
    "            row = rec['row']\n",
    "            print(f\"\\n{i}. {row['title']}\")\n",
    "            label_display = str(row['label'])[:50] + '...' if pd.notna(row['label']) and len(str(row['label'])) > 50 else row['label']\n",
    "            print(f\"   Year: {row.get('oldest_year', row['year'])} | Label: {label_display}\")\n",
    "            print(f\"   Score: {rec['score']:.3f}\")\n",
    "            print(f\"   Breakdown: RB={rec['breakdown']['rule_based']:.2f}, KG={rec['breakdown']['knowledge_graph']:.2f}, CL={rec['breakdown']['clustering']:.2f}, JC={rec['breakdown']['jaccard']:.2f}\")\n",
    "            print(f\"   Why: {', '.join(rec['reasons'][:4])}\")\n",
    "            print(f\"   URL: {row['resource_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ac3f3",
   "metadata": {},
   "source": [
    "# Streamlit app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
